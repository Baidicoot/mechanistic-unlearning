{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models for edge or weight masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow:\n",
    "- Load model\n",
    "- Use Task with clean and corrupt data, use ACDCPP and get the ACDCPP-style edges\n",
    "- Convert ACDCPP-style edges to edge mask, get either edge superset of node superset\n",
    "- Apply these masks to the mask training, either by limiting edge mask to only edge superset, node superset, or by limiting weight mask to node superset\n",
    "\n",
    "- Also need to test other baselines, like regular finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('acdcpp/Automatic-Circuit-Discovery/')\n",
    "sys.path.append('acdcpp/')\n",
    "from acdc import TLACDCExperiment\n",
    "from acdcpp.ACDCPPExperiment import ACDCPPExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# import acdc\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# set up pipeline from acdcpp to edge mask\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "gpt2_tokenizer = model.tokenizer\n",
    "\n",
    "reference_gpt2 = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DemoTransformer Implementations Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "tensor(7.2482e-06, device='cuda:0')\n",
      "tensor(7.2237e-06, device='cuda:0')\n",
      "tensor(6.9957e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.transformers.gpt2.edge_masked_transformer import DemoTransformer as GPT2EdgeDemoTransformer, Config as GPT2Config\n",
    "\n",
    "from cb_utils.models import tl_config_to_demo_config\n",
    "with open(\"models/gpt2_weights.pkl\", \"rb\") as f:\n",
    "    gpt2_weights = pickle.load(f)\n",
    "demo_edge_gpt2 = GPT2EdgeDemoTransformer(GPT2Config(debug=False, n_layers=12, n_heads=12), means=False)\n",
    "demo_edge_gpt2.load_state_dict(gpt2_weights, strict=False)\n",
    "demo_edge_gpt2.cuda()\n",
    "\n",
    "\n",
    "from cb_utils.transformers.gpt2.weight_masked_transformer import DemoTransformer as GPT2WeightDemoTransformer, Config as GPT2Config\n",
    "demo_weight_gpt2 = GPT2WeightDemoTransformer(GPT2Config(debug=False, n_layers=12, n_heads=12))\n",
    "demo_weight_gpt2.load_state_dict(gpt2_weights, strict=False)\n",
    "demo_weight_gpt2.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_input = t.tensor(gpt2_tokenizer.encode(\"The quick brown fox jumps over the lazy\")).unsqueeze(0).cuda()\n",
    "    print((demo_edge_gpt2(test_input)[0][0, -1] - reference_gpt2(test_input)[0, -1]).std())\n",
    "    print((demo_weight_gpt2(test_input)[0][0, -1] - reference_gpt2(test_input)[0, -1]).std())\n",
    "    print((demo_edge_gpt2(test_input)[0][0, -1] - demo_weight_gpt2(test_input)[0][0, -1]).std())\n",
    "    # print((model(test_input)[0][0, -1] - edge_masked_model(test_input)[0][0, -1]).std())\n",
    "    # print((reference_pythia(test_input)[0, -1] - edge_masked_model(test_input)[0][0, -1]).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ACDCPP edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found, will not be able to run evaluations on HPSAQ Task\n",
      "OpenAI API key not found, will not be able to run evaluations on HPSAQ Task\n",
      "Clean logit diff: 3.040117025375366, Corrupted logit diff: 1.2651995420455933\n",
      "Clean logit diff: 3.040, Corrupt logit diff: 1.265\n"
     ]
    }
   ],
   "source": [
    "from tasks.ioi.IOITask import IOITask_old, IOITask\n",
    "# ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25)\n",
    "ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"ABBA\")\n",
    "ioi_task.set_logit_diffs(model)\n",
    "\n",
    "ioi_metric = ioi_task.get_acdcpp_metric()\n",
    "def negative_abs_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ioi_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ioi_task.clean_data.toks)\n",
    "    corrupt_logits = model(ioi_task.corr_data.toks)\n",
    "    clean_logit_diff = ioi_task.ave_logit_diff(clean_logits, ioi_task.clean_data).item()\n",
    "    corrupt_logit_diff = ioi_task.ave_logit_diff(corrupt_logits, ioi_task.corr_data).item()\n",
    "    print(f'Clean logit diff: {clean_logit_diff:.3f}, Corrupt logit diff: {corrupt_logit_diff:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_metric(clean_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_metric(corrupt_logits, ioi_dataset=ioi_task.corr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.040117025375366, 1.2651995420455933)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logit_diff, corrupt_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PLACE]': ['station',\n",
       "  'restaurant',\n",
       "  'restaurant',\n",
       "  'restaurant',\n",
       "  'restaurant'],\n",
       " '[OBJECT]': ['ring', 'computer', 'necklace', 'bone', 'computer'],\n",
       " 'text': ['Then, William and Richard went to the station. William gave a ring to',\n",
       "  'Then, Charles and Jeremy went to the restaurant. Charles gave a computer to',\n",
       "  'Then, Simon and Clark went to the restaurant. Simon gave a necklace to',\n",
       "  'Then, Jacob and Scott went to the restaurant. Jacob gave a bone to',\n",
       "  'Then, Steven and Sullivan went to the restaurant. Steven gave a computer to'],\n",
       " 'IO': ['Richard', 'Jeremy', 'Clark', 'Scott', 'Sullivan'],\n",
       " 'S': ['William', 'Charles', 'Simon', 'Jacob', 'Steven'],\n",
       " 'TEMPLATE_IDX': tensor([0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_task_2 = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"BABA\", template_start_idx=0)\n",
    "ioi_task_2.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15299.74it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 258.14it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 281818.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:08<00:08,  8.11s/it]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15224.87it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 253.97it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 306343.88it/s]\n",
      "100%|██████████| 2/2 [00:15<00:00,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.08, .15]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ioi_task.clean_data.toks,\n",
    "    corr_data=ioi_task.corr_data.toks,\n",
    "    acdc_metric=negative_abs_ioi_metric,\n",
    "    acdcpp_metric=ioi_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 'a0.10'), (-1, 'embed')),\n",
       " ((0, 'a0.9'), (-1, 'embed')),\n",
       " ((0, 'm0'), (-1, 'embed')),\n",
       " ((0, 'm0'), (0, 'a0.10')),\n",
       " ((1, 'm1'), (0, 'm0')),\n",
       " ((3, 'a3.0'), (0, 'm0')),\n",
       " ((3, 'm3'), (0, 'm0')),\n",
       " ((3, 'm3'), (1, 'm1')),\n",
       " ((3, 'm3'), (3, 'a3.0')),\n",
       " ((4, 'm4'), (0, 'm0')),\n",
       " ((4, 'm4'), (3, 'a3.0')),\n",
       " ((5, 'a5.9'), (0, 'm0')),\n",
       " ((5, 'a5.9'), (1, 'm1')),\n",
       " ((5, 'a5.9'), (3, 'a3.0')),\n",
       " ((5, 'a5.9'), (3, 'm3')),\n",
       " ((5, 'm5'), (3, 'a3.0')),\n",
       " ((5, 'm5'), (5, 'a5.5')),\n",
       " ((6, 'm6'), (3, 'a3.0')),\n",
       " ((6, 'm6'), (5, 'm5')),\n",
       " ((7, 'a7.9'), (0, 'm0')),\n",
       " ((7, 'm7'), (6, 'm6')),\n",
       " ((8, 'a8.10'), (4, 'm4')),\n",
       " ((8, 'a8.10'), (5, 'a5.5')),\n",
       " ((8, 'a8.10'), (5, 'a5.9')),\n",
       " ((8, 'a8.6'), (5, 'a5.5')),\n",
       " ((9, 'a9.9'), (8, 'a8.10')),\n",
       " ((10, 'a10.0'), (8, 'a8.10')),\n",
       " ((10, 'a10.7'), (0, 'm0')),\n",
       " ((10, 'a10.7'), (6, 'm6')),\n",
       " ((10, 'a10.7'), (9, 'a9.6')),\n",
       " ((10, 'a10.7'), (9, 'a9.9')),\n",
       " ((11, 'a11.10'), (0, 'm0')),\n",
       " ((11, 'a11.10'), (6, 'm6')),\n",
       " ((11, 'a11.10'), (8, 'a8.10')),\n",
       " ((11, 'a11.10'), (9, 'a9.6')),\n",
       " ((11, 'a11.10'), (9, 'a9.9')),\n",
       " ((11, 'a11.10'), (10, 'a10.0')),\n",
       " ((11, 'a11.10'), (10, 'a10.1')),\n",
       " ((11, 'a11.10'), (10, 'a10.10')),\n",
       " ((11, 'a11.10'), (10, 'a10.7')),\n",
       " ((11, 'a11.2'), (0, 'm0')),\n",
       " ((11, 'a11.2'), (8, 'a8.10')),\n",
       " ((11, 'a11.2'), (9, 'a9.8')),\n",
       " ((11, 'a11.2'), (9, 'a9.9')),\n",
       " ((11, 'a11.2'), (9, 'm9')),\n",
       " ((12, 'output'), (7, 'a7.9')),\n",
       " ((12, 'output'), (8, 'a8.10')),\n",
       " ((12, 'output'), (9, 'a9.6')),\n",
       " ((12, 'output'), (9, 'a9.8')),\n",
       " ((12, 'output'), (9, 'a9.9')),\n",
       " ((12, 'output'), (10, 'a10.0')),\n",
       " ((12, 'output'), (10, 'a10.1')),\n",
       " ((12, 'output'), (10, 'a10.10')),\n",
       " ((12, 'output'), (10, 'a10.2')),\n",
       " ((12, 'output'), (10, 'a10.6')),\n",
       " ((12, 'output'), (10, 'a10.7')),\n",
       " ((12, 'output'), (11, 'a11.10')),\n",
       " ((12, 'output'), (11, 'a11.2'))}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdcpp_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('head.0.2', 'mlp.0', 0.009443754330277443), ('head.0.14', 'mlp.0', 0.006188404746353626), ('mlp.6', 'mlp.8', 0.00573932658880949), ('mlp.0', 'mlp.2', 0.005653967149555683), ('head.16.20', 'mlp.16', -0.005345507059246302), ('head.16.20', 'head.17.30.v', 0.005315450485795736), ('mlp.0', 'head.1.16.k', -0.005109565332531929), ('head.14.14', 'mlp.15', -0.004921694286167622), ('mlp.15', 'head.16.20.k', -0.004847021773457527), ('mlp.6', 'mlp.15', -0.004780464340001345), ('mlp.10', 'head.16.20.k', 0.004766407422721386), ('mlp.6', 'head.16.20.k', 0.004757486749440432), ('mlp.0', 'head.1.15.k', -0.004494336899369955), ('mlp.0', 'mlp.5', -0.004315529949963093), ('mlp.0', 'mlp.4', -0.004094669129699469), ('head.16.20', 'mlp.18', 0.004019735846668482), ('head.0.30', 'mlp.0', 0.0039080469869077206), ('mlp.0', 'mlp.6', -0.0038432518485933542), ('mlp.8', 'mlp.9', 0.00376768596470356), ('head.16.20', 'head.21.9.v', 0.003543522208929062), ('mlp.9', 'mlp.11', 0.003415714716538787), ('mlp.6', 'head.21.9.k', 0.0033131211530417204), ('head.15.4', 'mlp.15', -0.003284941893070936), ('head.15.20', 'head.16.20.q', -0.0032149143517017365), ('mlp.5', 'mlp.6', 0.003205507528036833), ('mlp.9', 'mlp.13', 0.003195255296304822), ('mlp.3', 'mlp.6', 0.0031151920557022095), ('mlp.5', 'mlp.8', 0.003063748823478818), ('mlp.12', 'head.16.20.k', -0.0030587592627853155), ('mlp.0', 'head.1.17.k', -0.0030355819035321474), ('mlp.11', 'head.16.20.v', 0.0030052291695028543), ('head.9.15', 'head.16.20.k', 0.002988740336149931), ('head.15.20', 'mlp.15', 0.002843316178768873), ('mlp.4', 'mlp.8', 0.002790694823488593), ('head.15.4', 'mlp.16', -0.002768619218841195), ('mlp.13', 'head.16.20.v', 0.002720218151807785), ('mlp.11', 'mlp.14', 0.0025788003113120794), ('mlp.11', 'head.16.20.k', -0.0025367813650518656), ('mlp.0', 'mlp.7', -0.002536351792514324), ('mlp.11', 'mlp.15', 0.0024518363643437624), ('mlp.8', 'head.16.20.v', 0.0023244901094585657), ('head.0.27', 'mlp.0', -0.0023238908033818007), ('mlp.6', 'mlp.11', 0.002282592235133052), ('head.17.30', 'mlp.18', 0.0022805416956543922), ('mlp.0', 'head.21.9.k', 0.0022331280633807182), ('head.0.21', 'mlp.0', 0.0022144378162920475), ('mlp.3', 'mlp.15', -0.00221416843123734), ('head.15.20', 'head.17.30.q', -0.002205037511885166), ('mlp.10', 'mlp.15', -0.0022013422567397356), ('mlp.4', 'head.16.20.k', 0.0021811285987496376), ('mlp.2', 'mlp.3', 0.0021742351818829775), ('mlp.30', 'mlp.31', -0.002144202124327421), ('mlp.5', 'head.16.20.k', 0.0021362670231610537), ('head.16.17', 'head.17.30.q', 0.002123921876773238), ('mlp.29', 'mlp.31', -0.00211007590405643), ('mlp.14', 'head.16.20.v', 0.0021046672482043505), ('mlp.10', 'mlp.11', -0.0020922652911394835), ('head.12.17', 'mlp.15', -0.0020885460544377565), ('mlp.0', 'mlp.13', -0.0020814668387174606), ('mlp.5', 'head.16.20.v', 0.0020730250980705023), ('mlp.28', 'mlp.31', -0.0020570242777466774), ('mlp.7', 'mlp.8', 0.00204459554515779), ('head.9.15', 'head.11.21.q', -0.002041686326265335), ('mlp.0', 'mlp.14', -0.002014364581555128), ('mlp.0', 'head.15.5.k', -0.002007126808166504), ('mlp.15', 'mlp.16', 0.001995496451854706), ('head.14.14', 'mlp.16', -0.001992548583075404), ('mlp.6', 'mlp.14', -0.001976989908143878), ('mlp.15', 'head.17.30.k', -0.0019326637266203761), ('head.16.20', 'head.22.15.v', 0.001931019127368927), ('mlp.6', 'mlp.12', 0.0019294897792860866), ('head.13.8', 'head.21.9.q', 0.0019150624284520745), ('mlp.0', 'mlp.11', -0.0019050340633839369), ('head.0.21', 'mlp.1', 0.0018889722414314747), ('head.7.8', 'mlp.13', -0.0018879767740145326), ('head.1.25', 'mlp.2', 0.0018849290208891034), ('head.15.4', 'head.16.20.q', 0.0018740722443908453), ('mlp.12', 'mlp.13', 0.001870404346846044), ('head.16.20', 'mlp.17', -0.0018696865299716592), ('head.10.1', 'mlp.15', -0.0018582759657874703), ('mlp.1', 'mlp.2', 0.0018570123938843608), ('mlp.2', 'mlp.5', 0.0018493117531761527), ('mlp.0', 'mlp.1', -0.001831269939430058), ('head.0.6', 'mlp.0', 0.00180960597936064), ('mlp.10', 'head.21.9.k', 0.001803320599719882), ('head.0.31', 'mlp.0', -0.0017993211513385177), ('mlp.12', 'head.15.20.k', 0.0017812863225117326), ('mlp.14', 'head.21.9.k', 0.0017715865978971124), ('head.7.8', 'head.16.20.k', 0.0017514253268018365), ('mlp.10', 'mlp.12', -0.001751400763168931), ('head.16.20', 'head.21.9.k', 0.0017360210185870528), ('mlp.5', 'mlp.15', -0.0017260626191273332), ('head.9.15', 'head.17.30.k', 0.0017186870099976659), ('head.15.4', 'head.21.9.k', 0.00171701202634722), ('mlp.5', 'mlp.9', 0.0017162136500701308), ('mlp.6', 'mlp.9', 0.001710148761048913), ('mlp.3', 'head.16.20.k', 0.0016996695194393396), ('mlp.3', 'mlp.8', 0.001693469937890768), ('mlp.0', 'head.1.11.v', 0.0016771022928878665), ('mlp.0', 'head.1.18.k', 0.001676780404523015), ('head.15.20', 'mlp.16', 0.0016647606389597058), ('mlp.2', 'mlp.4', 0.0016523718368262053), ('head.15.4', 'head.16.10.v', 0.0016409446252509952), ('mlp.4', 'mlp.5', 0.0015915663680061698), ('head.7.8', 'mlp.15', 0.001587998354807496), ('head.10.11', 'mlp.11', -0.0015877102268859744), ('head.16.20', 'head.25.1.k', -0.0015855319797992706), ('mlp.8', 'mlp.14', 0.0015662244986742735), ('head.14.14', 'head.16.20.q', 0.0015645340317860246), ('mlp.18', 'head.21.9.v', 0.0015612227143719792), ('head.17.30', 'mlp.17', -0.0015610615955665708), ('mlp.7', 'mlp.10', -0.0015545483911409974), ('head.7.14', 'mlp.15', 0.0015438116388395429), ('mlp.3', 'mlp.5', 0.0015396539820358157), ('head.9.15', 'mlp.12', -0.001538568758405745), ('head.9.15', 'mlp.14', -0.0015276921913027763), ('head.17.30', 'head.21.9.v', 0.0015151547268033028), ('head.16.17', 'mlp.16', -0.0015022397274151444), ('mlp.24', 'head.25.1.k', -0.0014950997428968549), ('mlp.8', 'mlp.13', 0.0014932940248399973), ('mlp.4', 'head.21.9.k', 0.0014929039170965552), ('mlp.6', 'mlp.13', 0.0014775244053453207), ('head.9.15', 'mlp.13', -0.0014682551845908165), ('mlp.7', 'head.16.20.v', 0.0014557429822161794), ('mlp.0', 'head.21.31.k', -0.0014490863541141152), ('head.12.17', 'head.15.4.v', 0.0014430660521611571), ('mlp.6', 'head.7.20.v', 0.0014389472780749202), ('mlp.0', 'head.4.13.q', 0.0014347839169204235), ('mlp.0', 'head.1.16.q', 0.0014134803786873817), ('mlp.8', 'mlp.10', 0.0014106555609032512), ('mlp.0', 'head.21.19.k', -0.001405258197337389), ('head.17.30', 'head.21.9.k', 0.0014030528254806995), ('mlp.9', 'mlp.10', 0.0013922336511313915), ('mlp.0', 'mlp.3', -0.0013902803184464574), ('mlp.6', 'head.16.20.v', 0.0013750517973676324), ('head.9.3', 'mlp.9', -0.0013734701788052917), ('head.13.8', 'head.17.30.q', 0.0013706028694286942), ('head.16.20', 'head.22.17.v', 0.0013674175133928657), ('head.13.8', 'head.15.20.q', -0.0013636015355587006), ('mlp.8', 'head.14.14.q', -0.001362730166874826), ('mlp.0', 'head.16.21.k', 0.0013574105687439442), ('head.7.8', 'head.17.30.k', 0.0013550223084166646), ('head.19.24', 'head.21.9.v', 0.00135287013836205), ('head.0.28', 'mlp.0', -0.001351020997390151), ('head.14.14', 'head.15.20.q', -0.0013473035069182515), ('mlp.0', 'head.1.10.k', 0.0013392082182690501), ('mlp.10', 'head.15.20.q', 0.0013274125522002578), ('mlp.7', 'mlp.14', -0.0013219445245340466), ('head.12.17', 'head.15.20.v', -0.0013092361623421311), ('mlp.11', 'head.17.30.k', -0.0013053971342742443), ('mlp.7', 'head.21.9.k', 0.001304464996792376), ('mlp.0', 'head.1.4.v', 0.0012995852157473564), ('head.13.8', 'head.14.31.q', -0.0012961990432813764), ('mlp.21', 'mlp.31', 0.0012819045223295689), ('mlp.15', 'head.16.20.v', 0.001273989793844521), ('head.13.8', 'head.16.20.q', 0.001270874636247754), ('mlp.0', 'head.1.25.k', -0.001269094762392342), ('mlp.20', 'mlp.31', 0.0012679877690970898), ('mlp.7', 'mlp.11', 0.0012615789892151952), ('mlp.9', 'head.11.21.q', 0.0012575825676321983), ('head.0.30', 'mlp.3', 0.001251908834092319), ('mlp.0', 'head.24.21.k', 0.0012514435220509768), ('mlp.8', 'head.16.20.k', 0.0012460232246667147), ('head.0.16', 'mlp.0', 0.0012409038608893752), ('mlp.0', 'head.15.11.k', -0.001235301955603063), ('head.0.17', 'mlp.0', 0.0012327723670750856), ('mlp.23', 'head.25.1.k', -0.0012299207737669349), ('head.12.17', 'head.16.20.v', 0.0012290324084460735), ('mlp.12', 'mlp.15', 0.0012218646006658673), ('head.15.4', 'head.16.20.v', 0.001221155864186585), ('head.0.4', 'mlp.0', 0.001221106736920774), ('head.12.17', 'head.21.9.k', 0.001220965525135398), ('mlp.12', 'head.16.20.v', 0.0012155117001384497), ('head.16.20', 'head.21.19.k', -0.001211330178193748), ('mlp.3', 'head.21.9.k', 0.0011980809504166245), ('head.16.20', 'head.17.30.q', -0.0011938184034079313), ('mlp.15', 'head.16.17.k', -0.001190812443383038), ('head.13.30', 'mlp.14', 0.0011839853832498193), ('head.8.11', 'mlp.15', -0.0011744748335331678), ('mlp.8', 'head.15.20.v', -0.0011725453659892082), ('head.14.14', 'head.21.9.k', 0.0011706198565661907), ('mlp.6', 'head.15.20.k', -0.0011685850331559777), ('mlp.25', 'mlp.26', -0.0011683834018185735), ('mlp.4', 'head.16.20.v', 0.0011622239835560322), ('mlp.8', 'mlp.11', 0.0011603647144511342), ('mlp.2', 'mlp.15', -0.001156176207587123), ('head.9.3', 'mlp.14', -0.001154008787125349), ('mlp.15', 'head.16.20.q', -0.001148901996202767), ('head.17.30', 'head.22.15.v', 0.0011446698335930705), ('head.13.30', 'mlp.15', 0.0011397490743547678), ('mlp.18', 'head.25.1.k', -0.0011324080405756831), ('head.13.20', 'head.15.4.v', 0.001122756046243012), ('mlp.21', 'head.25.1.k', -0.0011200009612366557), ('head.0.7', 'mlp.0', 0.0011196527630090714), ('mlp.13', 'head.15.20.k', 0.0011141891591250896), ('head.7.9', 'mlp.15', -0.001111720921471715), ('mlp.2', 'head.21.9.k', 0.001110269222408533), ('head.1.25', 'mlp.3', 0.0011059733806177974), ('head.9.15', 'head.21.9.k', 0.0011041940888389945), ('mlp.0', 'head.1.4.k', -0.001102518173865974), ('mlp.23', 'head.26.25.k', 0.001102395704947412), ('mlp.15', 'head.16.17.v', -0.0011007385328412056), ('head.13.30', 'head.16.20.q', -0.001094624400138855), ('mlp.18', 'head.21.9.k', 0.0010888677788898349), ('head.8.14', 'mlp.9', -0.0010858307359740138), ('head.15.4', 'head.17.30.q', 0.0010818063747137785), ('mlp.6', 'head.15.20.v', -0.001078712404705584), ('head.9.15', 'head.10.26.v', -0.0010778617579489946), ('mlp.0', 'mlp.8', -0.0010757920099422336), ('head.15.27', 'head.16.20.q', -0.0010755263501778245), ('mlp.7', 'head.14.14.v', -0.001074428204447031), ('head.15.4', 'head.21.9.q', 0.0010719193378463387), ('head.14.14', 'head.16.17.v', 0.0010705209570005536), ('mlp.4', 'mlp.12', -0.0010661009000614285), ('head.7.8', 'head.15.20.k', 0.0010634062346071005), ('mlp.23', 'mlp.28', -0.001054505119100213), ('head.7.8', 'head.14.8.v', 0.0010530278086662292), ('mlp.13', 'head.15.4.v', 0.0010519307106733322), ('mlp.11', 'head.16.17.k', -0.0010491471039131284), ('mlp.9', 'mlp.18', -0.001046185614541173), ('head.5.17', 'mlp.7', 0.001044679433107376), ('mlp.8', 'head.13.8.v', 0.0010418110759928823), ('head.7.15', 'mlp.9', -0.0010394830023869872), ('mlp.26', 'mlp.30', -0.0010386754292994738), ('mlp.6', 'head.16.21.k', 0.0010373503901064396), ('head.15.20', 'head.16.17.v', -0.0010349199874326587), ('mlp.0', 'head.1.12.v', 0.0010330087970942259), ('head.12.26', 'mlp.14', -0.0010282696457579732), ('mlp.0', 'head.1.6.v', 0.0010257066460326314), ('head.13.30', 'mlp.13', 0.001021881471388042), ('mlp.6', 'head.20.2.k', -0.0010205552680417895), ('head.13.30', 'head.16.20.k', -0.0010205348953604698), ('head.15.4', 'head.21.19.k', -0.0010198857635259628), ('mlp.8', 'head.13.30.v', -0.0010180402314290404), ('head.15.4', 'head.25.1.k', -0.0010171744506806135), ('mlp.0', 'head.1.17.v', 0.0010166249703615904), ('head.14.14', 'head.17.30.q', 0.0010091098956763744), ('head.14.14', 'head.15.20.k', -0.001008716062642634), ('mlp.0', 'head.22.17.k', 0.0010071411961689591), ('head.17.30', 'head.25.1.k', -0.0010067331604659557), ('head.9.3', 'head.15.4.v', 0.001002408331260085), ('mlp.13', 'head.16.20.k', 0.0010006306692957878)]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.001\n",
    "import pickle\n",
    "with open('localizations/eap/eap_sports/1000_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "\n",
    "eap_edges = graph.top_edges(n=1000, threshold=threshold)\n",
    "# eap_edges = set()\n",
    "# for i in range(eap_scores.shape[0]):\n",
    "#     for j in range(eap_scores.shape[1]):\n",
    "#         if eap_scores[i, j] > threshold:\n",
    "            # eap_edges.add((get_node_name(graph.node_names[i], show_full_index=False), get_node_name(graph.node_names[j, show_full_index=False))))\n",
    "print(eap_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.eap_scores.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert format:\n",
    "want: {((3, 'm3'), (3, 'a3.0')),\n",
    " ((4, 'm4'), (0, 'm0')),\n",
    " ((4, 'm4'), (3, 'a3.0')),\n",
    " ((5, 'a5.9'), (0, 'm0')),}\n",
    "\n",
    "have:\n",
    "[('mlp.0', 'mlp.2', 0.005653967149555683),\n",
    "('head.0.14', 'mlp.0', 0.006188404746353626),]\n",
    "...\n",
    "\"\"\"\n",
    "from cb_utils.mask_utils import get_formatted_edges_from_eap, get_masks_from_eap_exp\n",
    "# formatted_eap_edges = get_formatted_edges_from_eap(eap_edges)\n",
    "# formatted_eap_edges\n",
    "with open('localizations/eap/eap_sports/1000_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_eap_exp(graph, threshold=0.001, num_layers=32, num_heads=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acdcpp_mask_dict['m31'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.292320251464844 1.4027974605560303\n"
     ]
    }
   ],
   "source": [
    "from tasks import InductionTask\n",
    "ind_task = InductionTask(batch_size=16, tokenizer=model.tokenizer, prep_acdcpp=True, seq_len=10, acdcpp_metric=\"ave_logit_diff\")\n",
    "ind_task.set_logit_diffs(model)\n",
    "print(ind_task.clean_logit_diff, ind_task.corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_metric = ind_task.get_acdcpp_metric()\n",
    "def negative_abs_ind_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ind_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ind_task.clean_data.cuda())\n",
    "    corrupt_logits = model(ind_task.corr_data.cuda())\n",
    "    clean_logit_diff = ind_task.ave_logit_diff(clean_logits, ind_task.clean_data).item()\n",
    "    corrupt_logit_diff = ind_task.ave_logit_diff(corrupt_logits, ind_task.corr_data).item()\n",
    "    \n",
    "print(ind_metric(clean_logits))\n",
    "print(ind_metric(corrupt_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15171.34it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 252.85it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 304067.19it/s]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 5, 0, 1, 2, 3, 4, 6, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.05]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ind_task.clean_data,\n",
    "    corr_data=ind_task.corr_data,\n",
    "    acdc_metric=negative_abs_ind_metric,\n",
    "    acdcpp_metric=ind_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=THRESHOLDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded edge-masked transformer\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.models import load_demo_pythia, load_demo_gpt2\n",
    "threshold = 0.05\n",
    "with open(f\"localizations/eap/ioi/gpt2_{threshold=}.pkl\", \"rb\") as f:\n",
    "    acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = pickle.load(f)\n",
    "\n",
    "# model = load_demo_pythia(means=False, model_name=\"pythia-2.8b\", edge_masks=True, mask_dict_superset=acdcpp_mask_dict)\n",
    "\n",
    "model = load_demo_gpt2(means=False, edge_mask=True, weight_mask=False, mask_dict_superset=acdcpp_mask_dict)\n",
    "\n",
    "# model = load_demo_gpt2(means=False, edge_mask=False, weight_mask=True, weight_mask_attn_dict=acdcpp_weight_mask_attn_dict, weight_mask_mlp_dict=acdcpp_weight_mask_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt = HookedTransformer.from_pretrained(\n",
    "        'gpt2-small',\n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        # default_padding_side=\"left\",\n",
    "        # device='cuda'\n",
    "        device='cuda'\n",
    "    )\n",
    "tokenizer = reference_gpt.tokenizer\n",
    "# pythia_tokenizer = reference_pythia.tokenizer\n",
    "# reference_pythia.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8494e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# compare the two models\n",
    "# compare model outputs\n",
    "with torch.no_grad():\n",
    "    test_input = t.tensor(tokenizer.encode(\"The quick brown fox jumps over the lazy\")).unsqueeze(0).cuda()\n",
    "    print((model(test_input)[0][0, -1] - reference_gpt(test_input)[0, -1]).std())\n",
    "    # print((model(test_input)[0][0, -1] - edge_masked_model(test_input)[0][0, -1]).std())\n",
    "    # print((reference_pythia(test_input)[0, -1] - edge_masked_model(test_input)[0][0, -1]).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.96875\n",
      "1.0\n",
      "tensor(15.3725, device='cuda:0')\n",
      "tensor(3.5195, device='cuda:0')\n",
      "tensor(3.1784, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from tasks import SportsTask, InductionTask, IOITask, InductionTask_Uniform, OWTTask#, SportsTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask_Uniform\n",
    "sports_task = SportsTask(batch_size=32, tokenizer=tokenizer, prep_acdcpp=False)\n",
    "ioi_task = IOITask(batch_size=32, tokenizer=tokenizer, prep_acdcpp=False)\n",
    "ind_task = InductionTask(batch_size=32, tokenizer=tokenizer, prep_acdcpp=False)\n",
    "ind_uniform_task = InductionTask_Uniform(batch_size=16, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, uniform_over=\"rep_tokens\")\n",
    "owt_task = OWTTask(batch_size=32, tokenizer=tokenizer, device=device, ctx_length=30)\n",
    "sports_uniform_task = SportsTask_Uniform(batch_size=32, tokenizer=tokenizer, uniform_over=\"sports_tokens\")\n",
    "print(sports_task.get_test_accuracy(model))#, sports_task.get_test_accuracy(reference_pythia))\n",
    "print(ioi_task.get_test_accuracy(model))#, ioi_task.get_test_accuracy(reference_pythia))\n",
    "print(ind_task.get_test_accuracy(model))#, ind_task.get_test_accuracy(reference_pythia))\n",
    "print(ind_uniform_task.get_test_loss(model))#, ind_uniform_task.get_test_loss(reference_pythia))\n",
    "print(owt_task.get_test_loss(model))#, owt_task.get_test_accuracy(reference_pythia))\n",
    "print(sports_uniform_task.get_test_loss(model))#, sports_uniform_task.get_test_accuracy(reference_pythia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.2142, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sports_uniform_task = SportsTask_Uniform(batch_size=32, tokenizer=tokenizer, uniform_over=\"all_tokens\")\n",
    "print(sports_uniform_task.get_test_loss(model))#, sports_uniform_task.get_test_accuracy(reference_pythia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tasks import LimitedSportsTask\n",
    "forget_task = LimitedSportsTask(batch_size=32, tokenizer=tokenizer, start_index=0, stop_index=64, make_complementary_task=True)\n",
    "remember_task = forget_task.complementary_task\n",
    "forget_task.get_test_accuracy(model), remember_task.get_test_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.828438528"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# test max batch size for sports, owt\n",
    "owt_task = OWTTask(batch_size=1, tokenizer=tokenizer, ctx_length=30)\n",
    "sports_task = SportsTask(batch_size=1, tokenizer=tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.304587776\n",
      "41.535725568\n",
      "51.745850368\n",
      "61.98750208\n",
      "72.183995392\n",
      "31.304588288\n"
     ]
    }
   ],
   "source": [
    "sports_task = SportsTask(batch_size=1, tokenizer=tokenizer, shuffle=False)\n",
    "tot_loss = 0\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "for i in range(4):\n",
    "    loss = sports_task.get_train_loss(model)\n",
    "    tot_loss += loss\n",
    "    print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "tot_loss.backward()\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.808385536\n",
      "19.528289792\n",
      "None\n",
      "19.150077952\n",
      "tensor([-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.1926,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0430,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "        -0.0000,  0.0000, -0.0000, -0.0041,  0.0266,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0028], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.1994,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0397,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0101,  0.0224,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0029], device='cuda:0')\n",
      "19.916095488\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.2187,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0516,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0415,  0.0118,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0107], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3756,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0774,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0243, -0.0222,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0840], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3745,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0777,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0035, -0.0313,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0760], device='cuda:0')\n",
      "20.301221888\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.2625,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0764,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.3730, -0.0606,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2777], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.2575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0730,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.4224, -0.0521,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1660], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3422,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0573,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.4138, -0.0537,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1833], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3454,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0576,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.4142, -0.0529,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1860], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.6088,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0573,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.2618, -0.0234,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1402], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.8482,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0413,  0.0014,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2160], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9013,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0280,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0584,  0.0114,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2556], device='cuda:0')\n",
      "18.75236864\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9123,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0311,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0663,  0.0107,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2653], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9955,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0243,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.1122,  0.0201,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2994], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9868,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0802,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0145, -0.0399,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2010], device='cuda:0')\n",
      "19.916095488\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9972,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0819,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0032, -0.0410,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1897], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -1.3423,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0843,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0399, -0.0116,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2129], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -1.4041,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0863,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0732, -0.0122,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2291], device='cuda:0')\n",
      "20.684939264\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -1.4221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0859,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0132, -0.0242,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1758], device='cuda:0')\n",
      "11.808385536\n"
     ]
    }
   ],
   "source": [
    "tot_loss = 0\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "model.zero_grad()\n",
    "for i in range(20):\n",
    "    loss = sports_task.get_train_loss(model)\n",
    "    print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "    print(model.blocks[2].edge_mask_mlp.grad)\n",
    "    loss.backward()\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000, -0.4634,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.1553,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.2797,  0.0654,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.3760], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[2].edge_mask_mlp.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that gradients flow correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check reg term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(95., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(95., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "weight_reg_term, tot_weight_params = model.get_edge_reg()\n",
    "print(weight_reg_term)\n",
    "print(tot_weight_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1, 6, 9, 10], 1: [7, 11], 2: [], 3: [0, 4], 4: [11], 5: [5, 8, 9], 6: [0], 7: [3, 9], 8: [6, 10], 9: [6, 7, 8, 9], 10: [0, 1, 2, 6, 7, 10], 11: [2, 3, 10]}\n",
      "{0: True, 1: True, 2: True, 3: True, 4: True, 5: True, 6: True, 7: True, 8: True, 9: True, 10: True, 11: False}\n"
     ]
    }
   ],
   "source": [
    "print(acdcpp_weight_mask_attn_dict)\n",
    "print(acdcpp_weight_mask_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2682, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2701, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2711, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2721, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2741, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2750, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2770, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2789, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2799, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2809, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2818, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2828, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2838, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2847, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2857, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2867, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2876, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2886, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2895, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2905, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2914, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2924, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2933, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2942, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2952, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2961, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2989, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2999, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3017, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3026, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3036, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3045, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3054, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3072, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3090, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3099, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3108, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3117, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3126, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3135, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3144, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3153, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3162, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3171, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3180, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3189, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3197, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3206, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3215, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3224, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3232, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3241, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3250, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3258, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3267, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3276, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3284, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3293, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3301, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3310, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3318, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3326, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3335, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3343, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3352, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3360, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3634, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3684, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3691, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3698, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3705, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3712, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3719, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3726, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3739, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3746, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3753, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3766, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3773, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3786, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3793, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3799, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3806, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3812, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3819, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3825, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3831, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3838, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3844, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3850, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3856, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3863, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3869, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3875, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3881, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3887, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3893, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3899, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.3905, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m optim \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     weight_reg_term, tot_weight_params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_edge_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m weight_reg_term \u001b[38;5;241m/\u001b[39m tot_weight_params\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/mechanistic-unlearning/cb_utils/transformers/gpt2/edge_masked_transformer.py:390\u001b[0m, in \u001b[0;36mDemoTransformer.get_edge_reg\u001b[0;34m(self, norm)\u001b[0m\n\u001b[1;32m    388\u001b[0m tot_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 390\u001b[0m     block_reg, block_params \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mask_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     edge_reg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m block_reg\n\u001b[1;32m    392\u001b[0m     tot_params \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m block_params\n",
      "File \u001b[0;32m~/mechanistic-unlearning/cb_utils/transformers/gpt2/edge_masked_transformer.py:286\u001b[0m, in \u001b[0;36mTransformerBlock.get_mask_reg\u001b[0;34m(self, norm)\u001b[0m\n\u001b[1;32m    282\u001b[0m     tot_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask_attentions\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask_mlp\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# only add edge_mask_attentions that aren't masked by the frozen mask\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# edges with 0s in frozen mask are not added to reg\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     edge_reg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask_attentions_frozen \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask_attentions)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_mask_mlp_frozen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_mask_mlp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# only count 1s in the frozen mask for total trainable parameters\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     tot_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask_attentions_frozen\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask_mlp_frozen\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# step\n",
    "optim = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for i in range(1000):\n",
    "    weight_reg_term, tot_weight_params = model.get_edge_reg()\n",
    "    loss = weight_reg_term / tot_weight_params\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "        [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "        [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "        ...,\n",
       "        [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "        [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "        [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[10].mlp.weight_mask_W_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         ...,\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994]],\n",
       "\n",
       "        [[0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         ...,\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994]],\n",
       "\n",
       "        [[0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         ...,\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         ...,\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994],\n",
       "         [0.9994, 0.9994, 0.9994,  ..., 0.9994, 0.9994, 0.9994]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[10].attn.weight_mask_W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.0.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.0.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.0.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.0.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.1.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.1.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.1.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.1.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.1.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.2.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.2.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.2.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.2.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.2.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.3.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.3.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.3.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.3.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.3.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.4.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.4.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.4.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.4.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.4.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.5.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.5.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.5.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.5.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.5.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.6.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.7.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.7.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.7.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.7.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.7.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.8.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.8.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.8.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.8.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.8.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.9.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.9.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.9.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.9.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.9.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.10.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.10.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.10.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.10.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.10.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.11.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "    # print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "batch_size = 2\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False)\n",
    "loss = ioi.get_train_loss(model)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.17700352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0216, device='cuda:0')\n",
      "blocks.0.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.5815, device='cuda:0')\n",
      "blocks.0.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.3946, device='cuda:0')\n",
      "blocks.0.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.7858, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.0081, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(5.3979, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.5360, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.6562, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0523, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(2.6793, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(3.4024, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(4.1716, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.0001, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.7603, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1013, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.4754, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0153, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4646, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6357, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.5915, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(6.0486, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(5.4989, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1658, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.4407, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0287, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.1237, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.1898, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.3442, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(6.7269, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(4.4302, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1361, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.3720, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0340, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.2193, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.1874, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.5638, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(5.7872, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(3.8785, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1223, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.2525, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0247, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.5181, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.1584, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.9633, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(5.6964, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(3.4137, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1316, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.2880, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0326, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.3960, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.2927, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.3720, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(4.1122, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.8020, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1218, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.2403, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0707, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.0487, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.0041, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.3239, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.1489, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.4518, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0938, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1907, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0243, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.6422, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4250, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6018, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.1996, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.1894, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1043, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1480, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1076, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.6959, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6134, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.0054, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(2.0523, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.9276, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0588, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1275, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1095, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4939, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6077, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.8943, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.7116, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.7489, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0526, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1074, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1050, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.7140, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4779, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.7883, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(2.0600, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.6167, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0715, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1083, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0721, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4778, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.7437, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6520, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.4094, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.4171, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0439, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1021, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1011, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.5514, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4197, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6264, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.4831, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.3279, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0461, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1136, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1698, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4920, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4761, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.5347, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.3809, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.2714, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0472, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1006, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1117, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.8832, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.7689, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6868, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.2130, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.1867, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0375, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1102, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0881, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.6491, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.4239, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.5172, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.9002, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.8568, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0284, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0876, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0202, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.5240, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6827, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.4406, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8107, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7710, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0251, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0848, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0177, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.1040, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1383, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.1011, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8756, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7267, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0321, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0764, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0253, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.8453, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.0762, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0645, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7466, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.6848, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0295, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0766, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1612, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.5107, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1835, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.1371, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8223, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7292, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0507, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0725, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0411, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.6082, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4147, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.2566, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8589, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7254, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0369, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0550, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0118, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(2.9092, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4474, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.3173, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7197, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.6838, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0204, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0511, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0093, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(2.3585, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1862, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0740, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8991, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.8119, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0500, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1179, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0508, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.1172, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1264, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0462, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7543, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7233, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0206, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0740, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0253, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.7412, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1592, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0607, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8118, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7977, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0196, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0570, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0965, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.4008, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.3048, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.1360, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7854, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7327, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0208, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0462, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0133, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.1217, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1298, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0378, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8495, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7477, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0203, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0382, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0673, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.2781, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.0908, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0264, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8755, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7465, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0207, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0370, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0097, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.0849, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1065, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0473, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8123, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7129, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0166, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0395, device='cuda:0')\n",
      "blocks.30.attn.weight_mask_W_Q grad is all zeros\n",
      "blocks.30.attn.weight_mask_W_K grad is all zeros\n",
      "blocks.30.attn.weight_mask_W_V grad is all zeros\n",
      "blocks.30.attn.weight_mask_W_O grad is all zeros\n",
      "blocks.30.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.2086, device='cuda:0')\n",
      "blocks.30.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.9111, device='cuda:0')\n",
      "blocks.30.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0195, device='cuda:0')\n",
      "blocks.30.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0239, device='cuda:0')\n",
      "blocks.31.attn.weight_mask_W_Q grad is all zeros\n",
      "blocks.31.attn.weight_mask_W_K grad is all zeros\n",
      "blocks.31.attn.weight_mask_W_V grad is all zeros\n",
      "blocks.31.attn.weight_mask_W_O grad is all zeros\n",
      "blocks.31.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8112, device='cuda:0')\n",
      "blocks.31.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.8142, device='cuda:0')\n",
      "blocks.31.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0111, device='cuda:0')\n",
      "blocks.31.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0247, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad: # and \"edge\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'a0.21'),\n",
       " (0, 'a0.30'),\n",
       " (0, 'm0'),\n",
       " (1, 'a1.0'),\n",
       " (1, 'a1.10'),\n",
       " (1, 'a1.11'),\n",
       " (1, 'a1.12'),\n",
       " (1, 'a1.14'),\n",
       " (1, 'a1.15'),\n",
       " (1, 'a1.16'),\n",
       " (1, 'a1.17'),\n",
       " (1, 'a1.18'),\n",
       " (1, 'a1.25'),\n",
       " (1, 'a1.26'),\n",
       " (1, 'a1.29'),\n",
       " (1, 'a1.30'),\n",
       " (1, 'a1.4'),\n",
       " (1, 'a1.6'),\n",
       " (1, 'm1'),\n",
       " (2, 'a2.11'),\n",
       " (2, 'a2.12'),\n",
       " (2, 'm2'),\n",
       " (3, 'a3.23'),\n",
       " (3, 'a3.29'),\n",
       " (3, 'm3'),\n",
       " (4, 'a4.10'),\n",
       " (4, 'a4.13'),\n",
       " (4, 'a4.16'),\n",
       " (4, 'a4.20'),\n",
       " (4, 'a4.25'),\n",
       " (4, 'a4.4'),\n",
       " (4, 'm4'),\n",
       " (5, 'a5.0'),\n",
       " (5, 'a5.17'),\n",
       " (5, 'a5.20'),\n",
       " (5, 'm5'),\n",
       " (6, 'a6.19'),\n",
       " (6, 'a6.6'),\n",
       " (6, 'm6'),\n",
       " (7, 'a7.14'),\n",
       " (7, 'a7.15'),\n",
       " (7, 'a7.20'),\n",
       " (7, 'a7.8'),\n",
       " (7, 'a7.9'),\n",
       " (7, 'm7'),\n",
       " (8, 'a8.11'),\n",
       " (8, 'a8.14'),\n",
       " (8, 'a8.26'),\n",
       " (8, 'a8.6'),\n",
       " (8, 'm8'),\n",
       " (9, 'a9.0'),\n",
       " (9, 'a9.15'),\n",
       " (9, 'a9.19'),\n",
       " (9, 'a9.3'),\n",
       " (9, 'a9.8'),\n",
       " (9, 'a9.9'),\n",
       " (9, 'm9'),\n",
       " (10, 'a10.1'),\n",
       " (10, 'a10.10'),\n",
       " (10, 'a10.11'),\n",
       " (10, 'a10.14'),\n",
       " (10, 'a10.21'),\n",
       " (10, 'a10.26'),\n",
       " (10, 'a10.29'),\n",
       " (10, 'm10'),\n",
       " (11, 'a11.10'),\n",
       " (11, 'a11.12'),\n",
       " (11, 'a11.14'),\n",
       " (11, 'a11.21'),\n",
       " (11, 'a11.24'),\n",
       " (11, 'a11.25'),\n",
       " (11, 'a11.31'),\n",
       " (11, 'a11.8'),\n",
       " (11, 'm11'),\n",
       " (12, 'a12.1'),\n",
       " (12, 'a12.12'),\n",
       " (12, 'a12.15'),\n",
       " (12, 'a12.17'),\n",
       " (12, 'a12.2'),\n",
       " (12, 'a12.23'),\n",
       " (12, 'a12.26'),\n",
       " (12, 'm12'),\n",
       " (13, 'a13.0'),\n",
       " (13, 'a13.12'),\n",
       " (13, 'a13.16'),\n",
       " (13, 'a13.20'),\n",
       " (13, 'a13.25'),\n",
       " (13, 'a13.30'),\n",
       " (13, 'a13.8'),\n",
       " (13, 'm13'),\n",
       " (14, 'a14.12'),\n",
       " (14, 'a14.14'),\n",
       " (14, 'a14.22'),\n",
       " (14, 'a14.23'),\n",
       " (14, 'a14.31'),\n",
       " (14, 'a14.4'),\n",
       " (14, 'a14.8'),\n",
       " (14, 'm14'),\n",
       " (15, 'a15.11'),\n",
       " (15, 'a15.18'),\n",
       " (15, 'a15.20'),\n",
       " (15, 'a15.27'),\n",
       " (15, 'a15.28'),\n",
       " (15, 'a15.4'),\n",
       " (15, 'a15.5'),\n",
       " (15, 'a15.6'),\n",
       " (15, 'm15'),\n",
       " (16, 'a16.10'),\n",
       " (16, 'a16.17'),\n",
       " (16, 'a16.20'),\n",
       " (16, 'a16.21'),\n",
       " (16, 'a16.29'),\n",
       " (16, 'm16'),\n",
       " (17, 'a17.11'),\n",
       " (17, 'a17.21'),\n",
       " (17, 'a17.28'),\n",
       " (17, 'a17.30'),\n",
       " (17, 'a17.6'),\n",
       " (17, 'm17'),\n",
       " (18, 'a18.31'),\n",
       " (18, 'm18'),\n",
       " (19, 'a19.24'),\n",
       " (19, 'a19.30'),\n",
       " (19, 'm19'),\n",
       " (20, 'a20.17'),\n",
       " (20, 'a20.2'),\n",
       " (20, 'a20.8'),\n",
       " (20, 'm20'),\n",
       " (21, 'a21.19'),\n",
       " (21, 'a21.23'),\n",
       " (21, 'a21.31'),\n",
       " (21, 'a21.9'),\n",
       " (21, 'm21'),\n",
       " (22, 'a22.15'),\n",
       " (22, 'a22.17'),\n",
       " (22, 'a22.24'),\n",
       " (22, 'm22'),\n",
       " (23, 'a23.26'),\n",
       " (23, 'm23'),\n",
       " (24, 'a24.21'),\n",
       " (24, 'm24'),\n",
       " (25, 'a25.1'),\n",
       " (25, 'a25.17'),\n",
       " (25, 'm25'),\n",
       " (26, 'a26.0'),\n",
       " (26, 'a26.25'),\n",
       " (26, 'a26.4'),\n",
       " (26, 'm26'),\n",
       " (27, 'a27.13'),\n",
       " (27, 'm27'),\n",
       " (28, 'a28.19'),\n",
       " (28, 'm28'),\n",
       " (29, 'a29.13'),\n",
       " (29, 'm29'),\n",
       " (30, 'm30'),\n",
       " (31, 'm31')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdcpp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='blocks.29.attn.weight_mask_W_O', nonzero indices: (tensor([13, 13, 13,  ..., 13, 13, 13], device='cuda:0'), tensor([ 0,  0,  0,  ..., 79, 79, 79], device='cuda:0'), tensor([   0,    1,    2,  ..., 2557, 2558, 2559], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and \"blocks.29.attn.weight_mask_W_O\" in name:\n",
    "        zero_indices = torch.nonzero(param.grad != 0, as_tuple=True)\n",
    "        print(f\"{name=}, nonzero indices: {zero_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if correct MLPs flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'm2')\n",
      "(10, 'm10')\n",
      "(5, 'm5')\n",
      "(3, 'm3')\n",
      "(7, 'm7')\n",
      "(1, 'm1')\n",
      "(9, 'm9')\n",
      "(8, 'm8')\n",
      "(4, 'm4')\n",
      "(0, 'm0')\n",
      "(-1, 'embed')\n",
      "(6, 'm6')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"m\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.edge_mask_mlp grad is all zeros\n",
      "blocks.0.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.0.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.0.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.5009, device='cuda:0')\n",
      "blocks.0.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3840, device='cuda:0')\n",
      "blocks.0.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9580, device='cuda:0')\n",
      "blocks.0.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2079, device='cuda:0')\n",
      "blocks.1.edge_mask_mlp grad is all zeros\n",
      "blocks.1.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.1.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.1.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1881, device='cuda:0')\n",
      "blocks.1.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4780, device='cuda:0')\n",
      "blocks.1.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.0281, device='cuda:0')\n",
      "blocks.1.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2563, device='cuda:0')\n",
      "blocks.2.edge_mask_mlp grad is all zeros\n",
      "blocks.2.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.2.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.2.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7383, device='cuda:0')\n",
      "blocks.2.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4888, device='cuda:0')\n",
      "blocks.2.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4441, device='cuda:0')\n",
      "blocks.2.mlp.b_out grad is all zeros\n",
      "blocks.3.edge_mask_mlp grad is all zeros\n",
      "blocks.3.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.3.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.3.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1551, device='cuda:0')\n",
      "blocks.3.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3966, device='cuda:0')\n",
      "blocks.3.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.1005, device='cuda:0')\n",
      "blocks.3.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2987, device='cuda:0')\n",
      "blocks.4.edge_mask_mlp grad is all zeros\n",
      "blocks.4.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.4.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.4.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7344, device='cuda:0')\n",
      "blocks.4.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5089, device='cuda:0')\n",
      "blocks.4.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.5127, device='cuda:0')\n",
      "blocks.4.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3237, device='cuda:0')\n",
      "blocks.5.edge_mask_mlp grad is all zeros\n",
      "blocks.5.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.5.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.5.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8992, device='cuda:0')\n",
      "blocks.5.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5541, device='cuda:0')\n",
      "blocks.5.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4629, device='cuda:0')\n",
      "blocks.5.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3042, device='cuda:0')\n",
      "blocks.6.edge_mask_mlp grad is all zeros\n",
      "blocks.6.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.6.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.6.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.0009, device='cuda:0')\n",
      "blocks.6.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5724, device='cuda:0')\n",
      "blocks.6.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4607, device='cuda:0')\n",
      "blocks.6.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2637, device='cuda:0')\n",
      "blocks.7.edge_mask_mlp grad is all zeros\n",
      "blocks.7.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.7.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.7.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.3364, device='cuda:0')\n",
      "blocks.7.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4871, device='cuda:0')\n",
      "blocks.7.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9889, device='cuda:0')\n",
      "blocks.7.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2189, device='cuda:0')\n",
      "blocks.8.edge_mask_mlp grad is all zeros\n",
      "blocks.8.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.8.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.8.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8043, device='cuda:0')\n",
      "blocks.8.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6429, device='cuda:0')\n",
      "blocks.8.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.5990, device='cuda:0')\n",
      "blocks.8.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1610, device='cuda:0')\n",
      "blocks.9.edge_mask_mlp grad is all zeros\n",
      "blocks.9.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.9.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.9.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1774, device='cuda:0')\n",
      "blocks.9.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4569, device='cuda:0')\n",
      "blocks.9.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.3034, device='cuda:0')\n",
      "blocks.9.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1200, device='cuda:0')\n",
      "blocks.10.edge_mask_mlp grad is all zeros\n",
      "blocks.10.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.10.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.10.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.3321, device='cuda:0')\n",
      "blocks.10.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.2768, device='cuda:0')\n",
      "blocks.10.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(0.9347, device='cuda:0')\n",
      "blocks.10.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.0822, device='cuda:0')\n",
      "blocks.11.edge_mask_mlp grad is all zeros\n",
      "blocks.11.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.11.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.11.mlp.W_in grad is all zeros\n",
      "blocks.11.mlp.b_in grad is all zeros\n",
      "blocks.11.mlp.W_out grad is all zeros\n",
      "blocks.11.mlp.b_out grad is all zeros\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"mlp\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if individual attention heads have gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 'a9.7')\n",
      "(9, 'a9.9')\n",
      "(9, 'a9.6')\n",
      "(9, 'a9.8')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"a9\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1051, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0287, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1273, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0335, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0570, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0485, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 64, 768])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0306, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0389, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and \"9.attn.weight\" in name:\n",
    "    # if param.requires_grad and \"9.attn.b\" in name:\n",
    "        print(param.shape)\n",
    "        # if param.grad is not None and param.grad.sum() != 0:\n",
    "        #     print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        # else:\n",
    "        #     print(f\"{name} grad is all zeros\")\n",
    "        print(f\"{param.grad[3].norm()=}\")\n",
    "        print(f\"{param.grad[4].norm()=}\")\n",
    "        print(f\"{param.grad[7].norm()=}\")\n",
    "        print(f\"{param.grad[8].norm()=}\")\n",
    "        print(f\"{param.grad[11].norm()=}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Mask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from tasks import IOITask, SportsTask, OWTTask\n",
    "batch_size = 64\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=0)\n",
    "sports = SportsTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "owt = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "\n",
    "ioi_ood = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=1) # different template\n",
    "\n",
    "train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "task_weights = {\"ioi\": -.2, \"owt\": 1} # I think means preserve OWT, corrupt IOI\n",
    "eval_tasks = {\"ioi\": ioi, \"sports\": sports, \"owt\": owt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilliphguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/phillip_guo/mechanistic-unlearning/wandb/run-20240110_085753-mznr9fva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">vocal-deluge-22</a></strong> to <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 32/501 [10:48<2:38:21, 20.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_93609/3197613862.py\", line 15, in <module>\n",
      "    train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
      "  File \"/data/phillip_guo/mechanistic-unlearning/cb_utils/learn_mask.py\", line 178, in train_masks\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.learn_mask import train_masks\n",
    "\n",
    "epochs_left = 500\n",
    "steps_per_epoch = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "evaluate_every = 1\n",
    "discretize_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "use_wandb = False\n",
    "edge_mask_reg_strength = None\n",
    "weight_mask_reg_strength = 10\n",
    "\n",
    "wandb_config = {\"edge_masks\": edge_masks, \"weight_masks_attn\": weight_masks_attn, \"weight_masks_mlp\": weight_masks_mlp, \"epochs\": epochs_left, \"steps_per_epoch\": steps_per_epoch, \"lr\": lr, \"weight_decay\": weight_decay, \"evaluate_every\": evaluate_every, \"discretize_every\": discretize_every, \"threshold\": threshold, \"edge_mask_reg_strength\": edge_mask_reg_strength, \"weight_mask_reg_strength\": weight_mask_reg_strength}\n",
    "\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
    "            # param_names=param_names, mask_params=mask_params, \n",
    "            task_weights=task_weights, eval_tasks=eval_tasks, evaluate_every=evaluate_every, discretize_every=discretize_every, threshold=threshold, edge_mask_reg_strength=edge_mask_reg_strength, weight_mask_reg_strength=None, verbose=False, use_wandb=use_wandb, wandb_config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"masks/trained_mask_params_{epochs_left=}_{edge_mask_reg_strength=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, p in zip(param_names, mask_params):\n",
    "    if p.requires_grad:\n",
    "        # print(name, p)\n",
    "        # count how many zeros in p\n",
    "        print(torch.sum(p == 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
