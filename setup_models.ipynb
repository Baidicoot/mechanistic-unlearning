{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models for edge or weight masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow:\n",
    "- Load model\n",
    "- Use Task with clean and corrupt data, use ACDCPP and get the ACDCPP-style edges\n",
    "- Convert ACDCPP-style edges to edge mask, get either edge superset of node superset\n",
    "- Apply these masks to the mask training, either by limiting edge mask to only edge superset, node superset, or by limiting weight mask to node superset\n",
    "\n",
    "- Also need to test other baselines, like regular finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('acdcpp/Automatic-Circuit-Discovery/')\n",
    "sys.path.append('acdcpp/')\n",
    "from acdc import TLACDCExperiment\n",
    "from acdcpp.ACDCPPExperiment import ACDCPPExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# import acdc\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# set up pipeline from acdcpp to edge mask\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.666549248"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ACDCPP edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.ioi.IOITask import IOITask_old, IOITask\n",
    "# ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25)\n",
    "ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"ABBA\")\n",
    "ioi_task.set_logit_diffs(model)\n",
    "\n",
    "ioi_metric = ioi_task.get_acdcpp_metric()\n",
    "def negative_abs_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ioi_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ioi_task.clean_data.toks)\n",
    "    corrupt_logits = model(ioi_task.corr_data.toks)\n",
    "    clean_logit_diff = ioi_task.ave_logit_diff(clean_logits, ioi_task.clean_data).item()\n",
    "    corrupt_logit_diff = ioi_task.ave_logit_diff(corrupt_logits, ioi_task.corr_data).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PLACE]': ['station', 'restaurant', 'station', 'restaurant', 'school'],\n",
       " '[OBJECT]': ['bone', 'snack', 'kiss', 'ring', 'drink'],\n",
       " 'text': ['Then, Madison and David went to the station. David gave a bone to',\n",
       "  'Then, Jamie and Roman went to the restaurant. Roman gave a snack to',\n",
       "  'Then, Laura and Stephen went to the station. Stephen gave a kiss to',\n",
       "  'Then, Clark and George went to the restaurant. George gave a ring to',\n",
       "  'Then, Jamie and Maria went to the school. Maria gave a drink to'],\n",
       " 'IO': ['Madison', 'Jamie', 'Laura', 'Clark', 'Jamie'],\n",
       " 'S': ['David', 'Roman', 'Stephen', 'George', 'Maria'],\n",
       " 'TEMPLATE_IDX': tensor([0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_task.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PLACE]': ['house', 'office', 'restaurant', 'hospital', 'garden'],\n",
       " '[OBJECT]': ['computer', 'basketball', 'necklace', 'bone', 'bone'],\n",
       " 'text': ['Then, Brian and Madison had a lot of fun at the house. Madison gave a computer to',\n",
       "  'Then, Tyler and Ruby had a lot of fun at the office. Ruby gave a basketball to',\n",
       "  'Then, Georgia and Amy had a lot of fun at the restaurant. Amy gave a necklace to',\n",
       "  'Then, Sullivan and Robert had a lot of fun at the hospital. Robert gave a bone to',\n",
       "  'Then, Frank and Alan had a lot of fun at the garden. Alan gave a bone to'],\n",
       " 'IO': ['Brian', 'Tyler', 'Georgia', 'Sullivan', 'Frank'],\n",
       " 'S': ['Madison', 'Ruby', 'Amy', 'Robert', 'Alan'],\n",
       " 'TEMPLATE_IDX': tensor([0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_task_2 = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"ABBA\", template_start_idx=1)\n",
    "ioi_task_2.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PLACE]': ['station', 'office', 'office', 'restaurant', 'station'],\n",
       " '[OBJECT]': ['ring', 'bone', 'necklace', 'kiss', 'kiss'],\n",
       " 'text': ['Then, Ford and Charlie went to the station. Ford gave a ring to',\n",
       "  'Then, Mark and George went to the office. Mark gave a bone to',\n",
       "  'Then, Charlie and Cole went to the office. Charlie gave a necklace to',\n",
       "  'Then, Marco and Emily went to the restaurant. Marco gave a kiss to',\n",
       "  'Then, Jack and Madison went to the station. Jack gave a kiss to'],\n",
       " 'IO': ['Charlie', 'George', 'Cole', 'Emily', 'Madison'],\n",
       " 'S': ['Ford', 'Mark', 'Charlie', 'Marco', 'Jack'],\n",
       " 'TEMPLATE_IDX': tensor([0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_task_2 = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"BABA\", template_start_idx=0)\n",
    "ioi_task_2.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15446.21it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 251.35it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 303004.98it/s]\n",
      " 50%|█████     | 1/2 [00:07<00:07,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15573.12it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 254.40it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 321109.90it/s]\n",
      "100%|██████████| 2/2 [00:15<00:00,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.08, .15]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ioi_task.clean_data.toks,\n",
    "    corr_data=ioi_task.corr_data.toks,\n",
    "    acdc_metric=negative_abs_ioi_metric,\n",
    "    acdcpp_metric=ioi_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from cb_utils.transformer import DemoTransformer\n",
    "from cb_utils.models import load_demo_gpt2, tokenizer\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "edge_masks = True\n",
    "weight_masks_attn = True\n",
    "weight_masks_mlp = True\n",
    "train_base_weights = True\n",
    "localize_acdcpp = True\n",
    "\n",
    "# if edge_masks is True, then have mask_dict_superset be acdcpp_mask_dict\n",
    "mask_dict_superset = None if not edge_masks else acdcpp_mask_dict\n",
    "# model = load_demo_gpt2(means=means, mask_dict_superset=acdcpp_mask_dict)\n",
    "if localize_acdcpp:\n",
    "    weight_mask_attn_dict = acdcpp_weight_mask_attn_dict if weight_masks_attn else None\n",
    "    weight_mask_mlp_dict = acdcpp_weight_mask_mlp_dict if weight_masks_mlp else None\n",
    "    base_weight_attn_dict = acdcpp_weight_mask_attn_dict if train_base_weights else None\n",
    "    base_weight_mlp_dict = acdcpp_weight_mask_mlp_dict if train_base_weights else None\n",
    "\n",
    "else:\n",
    "    weight_mask_attn_dict = None\n",
    "    weight_mask_mlp_dict = None\n",
    "    base_weight_attn_dict = None\n",
    "    base_weight_mlp_dict = None\n",
    "\n",
    "model = load_demo_gpt2(means=False, edge_masks=edge_masks, mask_dict_superset=mask_dict_superset, weight_masks_attn=weight_masks_attn, weight_masks_mlp=weight_masks_mlp, weight_mask_attn_dict=weight_mask_attn_dict, weight_mask_mlp_dict=weight_mask_mlp_dict, train_base_weights=train_base_weights, base_weight_attn_dict=base_weight_attn_dict, base_weight_mlp_dict=base_weight_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.789679616"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that gradients flow correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_mask torch.Size([157]) True\n",
      "blocks.0.edge_mask_attentions torch.Size([1, 12]) True\n",
      "blocks.0.edge_mask_mlp torch.Size([13]) True\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.0.attn.b_O torch.Size([768]) True\n",
      "blocks.0.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.0.mlp.b_in torch.Size([3072]) True\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.0.mlp.b_out torch.Size([768]) True\n",
      "blocks.0.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.0.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.0.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.0.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.1.edge_mask_attentions torch.Size([14, 12]) True\n",
      "blocks.1.edge_mask_mlp torch.Size([26]) True\n",
      "blocks.1.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.1.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.1.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.1.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.1.attn.b_O torch.Size([768]) True\n",
      "blocks.1.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.1.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.1.mlp.b_in torch.Size([3072]) True\n",
      "blocks.1.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.1.mlp.b_out torch.Size([768]) True\n",
      "blocks.1.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.1.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.1.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.1.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.2.edge_mask_attentions torch.Size([27, 12]) True\n",
      "blocks.2.edge_mask_mlp torch.Size([39]) True\n",
      "blocks.2.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.2.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.2.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.2.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.2.attn.b_O torch.Size([768]) True\n",
      "blocks.2.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.2.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.2.mlp.b_in torch.Size([3072]) True\n",
      "blocks.2.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.2.mlp.b_out torch.Size([768]) True\n",
      "blocks.2.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.2.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.2.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.2.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.3.edge_mask_attentions torch.Size([40, 12]) True\n",
      "blocks.3.edge_mask_mlp torch.Size([52]) True\n",
      "blocks.3.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.3.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.3.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.3.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.3.attn.b_O torch.Size([768]) True\n",
      "blocks.3.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.3.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.3.mlp.b_in torch.Size([3072]) True\n",
      "blocks.3.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.3.mlp.b_out torch.Size([768]) True\n",
      "blocks.3.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.3.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.3.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.3.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.4.edge_mask_attentions torch.Size([53, 12]) True\n",
      "blocks.4.edge_mask_mlp torch.Size([65]) True\n",
      "blocks.4.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.4.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.4.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.4.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.4.attn.b_O torch.Size([768]) True\n",
      "blocks.4.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.4.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.4.mlp.b_in torch.Size([3072]) True\n",
      "blocks.4.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.4.mlp.b_out torch.Size([768]) True\n",
      "blocks.4.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.4.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.4.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.4.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.5.edge_mask_attentions torch.Size([66, 12]) True\n",
      "blocks.5.edge_mask_mlp torch.Size([78]) True\n",
      "blocks.5.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.5.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.5.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.5.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.5.attn.b_O torch.Size([768]) True\n",
      "blocks.5.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.5.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.5.mlp.b_in torch.Size([3072]) True\n",
      "blocks.5.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.5.mlp.b_out torch.Size([768]) True\n",
      "blocks.5.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.5.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.5.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.5.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.6.edge_mask_attentions torch.Size([79, 12]) True\n",
      "blocks.6.edge_mask_mlp torch.Size([91]) True\n",
      "blocks.6.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.6.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.6.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.6.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.attn.b_O torch.Size([768]) True\n",
      "blocks.6.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.b_out torch.Size([768]) True\n",
      "blocks.6.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.7.edge_mask_attentions torch.Size([92, 12]) True\n",
      "blocks.7.edge_mask_mlp torch.Size([104]) True\n",
      "blocks.7.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.7.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.7.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.7.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.7.attn.b_O torch.Size([768]) True\n",
      "blocks.7.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.7.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.7.mlp.b_in torch.Size([3072]) True\n",
      "blocks.7.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.7.mlp.b_out torch.Size([768]) True\n",
      "blocks.7.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.7.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.7.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.7.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.8.edge_mask_attentions torch.Size([105, 12]) True\n",
      "blocks.8.edge_mask_mlp torch.Size([117]) True\n",
      "blocks.8.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.8.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.8.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.8.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.8.attn.b_O torch.Size([768]) True\n",
      "blocks.8.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.8.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.8.mlp.b_in torch.Size([3072]) True\n",
      "blocks.8.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.8.mlp.b_out torch.Size([768]) True\n",
      "blocks.8.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.8.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.8.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.8.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.9.edge_mask_attentions torch.Size([118, 12]) True\n",
      "blocks.9.edge_mask_mlp torch.Size([130]) True\n",
      "blocks.9.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.9.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.9.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.9.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.9.attn.b_O torch.Size([768]) True\n",
      "blocks.9.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.9.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.9.mlp.b_in torch.Size([3072]) True\n",
      "blocks.9.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.9.mlp.b_out torch.Size([768]) True\n",
      "blocks.9.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.9.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.9.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.9.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.10.edge_mask_attentions torch.Size([131, 12]) True\n",
      "blocks.10.edge_mask_mlp torch.Size([143]) True\n",
      "blocks.10.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.10.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.10.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.10.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.10.attn.b_O torch.Size([768]) True\n",
      "blocks.10.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.10.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.10.mlp.b_in torch.Size([3072]) True\n",
      "blocks.10.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.10.mlp.b_out torch.Size([768]) True\n",
      "blocks.10.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.10.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.10.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.10.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.11.edge_mask_attentions torch.Size([144, 12]) True\n",
      "blocks.11.edge_mask_mlp torch.Size([156]) True\n",
      "blocks.11.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.11.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.11.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.11.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.11.attn.b_O torch.Size([768]) True\n",
      "blocks.11.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "    # print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "batch_size = 8\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False)\n",
    "loss = ioi.get_train_loss(model)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.701484032\n",
      "9.50002944\n",
      "11.296608768\n",
      "13.095366144\n",
      "14.886672896\n",
      "16.662592\n",
      "18.435150336\n",
      "20.20689152\n",
      "21.983021568\n",
      "23.762801152\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(10):\n",
    "    losses.append(ioi.get_train_loss(model))\n",
    "    print(torch.cuda.memory_allocated(device=device) / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_mask grad is all zeros\n",
      "blocks.0.edge_mask_attentions grad is all zeros\n",
      "blocks.0.edge_mask_mlp grad is all zeros\n",
      "blocks.0.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.1548, device='cuda:0')\n",
      "blocks.0.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0549, device='cuda:0')\n",
      "blocks.0.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.2216, device='cuda:0')\n",
      "blocks.0.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.4179e-08, device='cuda:0')\n",
      "blocks.0.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.2717, device='cuda:0')\n",
      "blocks.0.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.7394, device='cuda:0')\n",
      "blocks.0.attn.W_O grad is not all zeros, param.grad.norm()=tensor(2.7787, device='cuda:0')\n",
      "blocks.0.attn.b_O grad is not all zeros, param.grad.norm()=tensor(1.2196, device='cuda:0')\n",
      "blocks.0.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.8217, device='cuda:0')\n",
      "blocks.0.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4786, device='cuda:0')\n",
      "blocks.0.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.2637, device='cuda:0')\n",
      "blocks.0.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2465, device='cuda:0')\n",
      "blocks.1.edge_mask_attentions grad is all zeros\n",
      "blocks.1.edge_mask_mlp grad is all zeros\n",
      "blocks.1.attn.W_Q grad is all zeros\n",
      "blocks.1.attn.b_Q grad is all zeros\n",
      "blocks.1.attn.W_K grad is all zeros\n",
      "blocks.1.attn.b_K grad is all zeros\n",
      "blocks.1.attn.W_V grad is all zeros\n",
      "blocks.1.attn.b_V grad is all zeros\n",
      "blocks.1.attn.W_O grad is all zeros\n",
      "blocks.1.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2197, device='cuda:0')\n",
      "blocks.1.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.6849, device='cuda:0')\n",
      "blocks.1.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6041, device='cuda:0')\n",
      "blocks.1.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4991, device='cuda:0')\n",
      "blocks.1.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3097, device='cuda:0')\n",
      "blocks.2.edge_mask_attentions grad is all zeros\n",
      "blocks.2.edge_mask_mlp grad is all zeros\n",
      "blocks.2.attn.W_Q grad is all zeros\n",
      "blocks.2.attn.b_Q grad is all zeros\n",
      "blocks.2.attn.W_K grad is all zeros\n",
      "blocks.2.attn.b_K grad is all zeros\n",
      "blocks.2.attn.W_V grad is all zeros\n",
      "blocks.2.attn.b_V grad is all zeros\n",
      "blocks.2.attn.W_O grad is all zeros\n",
      "blocks.2.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2781, device='cuda:0')\n",
      "blocks.2.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.3378, device='cuda:0')\n",
      "blocks.2.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5941, device='cuda:0')\n",
      "blocks.2.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.9843, device='cuda:0')\n",
      "blocks.2.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3619, device='cuda:0')\n",
      "blocks.3.edge_mask_attentions grad is all zeros\n",
      "blocks.3.edge_mask_mlp grad is all zeros\n",
      "blocks.3.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.3450, device='cuda:0')\n",
      "blocks.3.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0399, device='cuda:0')\n",
      "blocks.3.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.2601, device='cuda:0')\n",
      "blocks.3.attn.b_K grad is not all zeros, param.grad.norm()=tensor(4.4314e-09, device='cuda:0')\n",
      "blocks.3.attn.W_V grad is not all zeros, param.grad.norm()=tensor(0.9300, device='cuda:0')\n",
      "blocks.3.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.3861, device='cuda:0')\n",
      "blocks.3.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.8948, device='cuda:0')\n",
      "blocks.3.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.3325, device='cuda:0')\n",
      "blocks.3.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.6125, device='cuda:0')\n",
      "blocks.3.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4856, device='cuda:0')\n",
      "blocks.3.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.5817, device='cuda:0')\n",
      "blocks.3.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3593, device='cuda:0')\n",
      "blocks.4.edge_mask_attentions grad is all zeros\n",
      "blocks.4.edge_mask_mlp grad is all zeros\n",
      "blocks.4.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.0010, device='cuda:0')\n",
      "blocks.4.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0001, device='cuda:0')\n",
      "blocks.4.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.0007, device='cuda:0')\n",
      "blocks.4.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.4829e-09, device='cuda:0')\n",
      "blocks.4.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.3327, device='cuda:0')\n",
      "blocks.4.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.1678, device='cuda:0')\n",
      "blocks.4.attn.W_O grad is not all zeros, param.grad.norm()=tensor(1.0120, device='cuda:0')\n",
      "blocks.4.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.3405, device='cuda:0')\n",
      "blocks.4.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.3849, device='cuda:0')\n",
      "blocks.4.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6280, device='cuda:0')\n",
      "blocks.4.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(3.0652, device='cuda:0')\n",
      "blocks.4.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3845, device='cuda:0')\n",
      "blocks.5.edge_mask_attentions grad is all zeros\n",
      "blocks.5.edge_mask_mlp grad is all zeros\n",
      "blocks.5.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.0015, device='cuda:0')\n",
      "blocks.5.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.1075, device='cuda:0')\n",
      "blocks.5.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.7923, device='cuda:0')\n",
      "blocks.5.attn.b_K grad is not all zeros, param.grad.norm()=tensor(8.4344e-09, device='cuda:0')\n",
      "blocks.5.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.6556, device='cuda:0')\n",
      "blocks.5.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.5560, device='cuda:0')\n",
      "blocks.5.attn.W_O grad is not all zeros, param.grad.norm()=tensor(1.6231, device='cuda:0')\n",
      "blocks.5.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.3329, device='cuda:0')\n",
      "blocks.5.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.5079, device='cuda:0')\n",
      "blocks.5.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6716, device='cuda:0')\n",
      "blocks.5.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.9281, device='cuda:0')\n",
      "blocks.5.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3576, device='cuda:0')\n",
      "blocks.6.edge_mask_attentions grad is all zeros\n",
      "blocks.6.edge_mask_mlp grad is all zeros\n",
      "blocks.6.attn.W_Q grad is all zeros\n",
      "blocks.6.attn.b_Q grad is all zeros\n",
      "blocks.6.attn.W_K grad is all zeros\n",
      "blocks.6.attn.b_K grad is all zeros\n",
      "blocks.6.attn.W_V grad is all zeros\n",
      "blocks.6.attn.b_V grad is all zeros\n",
      "blocks.6.attn.W_O grad is all zeros\n",
      "blocks.6.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2853, device='cuda:0')\n",
      "blocks.6.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.5282, device='cuda:0')\n",
      "blocks.6.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6701, device='cuda:0')\n",
      "blocks.6.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.9120, device='cuda:0')\n",
      "blocks.6.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3113, device='cuda:0')\n",
      "blocks.7.edge_mask_attentions grad is all zeros\n",
      "blocks.7.edge_mask_mlp grad is all zeros\n",
      "blocks.7.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.3832, device='cuda:0')\n",
      "blocks.7.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.1353, device='cuda:0')\n",
      "blocks.7.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.9461, device='cuda:0')\n",
      "blocks.7.attn.b_K grad is not all zeros, param.grad.norm()=tensor(4.4310e-09, device='cuda:0')\n",
      "blocks.7.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.0588, device='cuda:0')\n",
      "blocks.7.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.2373, device='cuda:0')\n",
      "blocks.7.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.4142, device='cuda:0')\n",
      "blocks.7.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2509, device='cuda:0')\n",
      "blocks.7.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7239, device='cuda:0')\n",
      "blocks.7.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5642, device='cuda:0')\n",
      "blocks.7.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.3637, device='cuda:0')\n",
      "blocks.7.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2601, device='cuda:0')\n",
      "blocks.8.edge_mask_attentions grad is all zeros\n",
      "blocks.8.edge_mask_mlp grad is all zeros\n",
      "blocks.8.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.5048, device='cuda:0')\n",
      "blocks.8.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.1685, device='cuda:0')\n",
      "blocks.8.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.9461, device='cuda:0')\n",
      "blocks.8.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.2591e-08, device='cuda:0')\n",
      "blocks.8.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.7726, device='cuda:0')\n",
      "blocks.8.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.3488, device='cuda:0')\n",
      "blocks.8.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.8881, device='cuda:0')\n",
      "blocks.8.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.1886, device='cuda:0')\n",
      "blocks.8.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.4894, device='cuda:0')\n",
      "blocks.8.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.7993, device='cuda:0')\n",
      "blocks.8.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9286, device='cuda:0')\n",
      "blocks.8.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1984, device='cuda:0')\n",
      "blocks.9.edge_mask_attentions grad is all zeros\n",
      "blocks.9.edge_mask_mlp grad is all zeros\n",
      "blocks.9.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.9216, device='cuda:0')\n",
      "blocks.9.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0945, device='cuda:0')\n",
      "blocks.9.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.7821, device='cuda:0')\n",
      "blocks.9.attn.b_K grad is not all zeros, param.grad.norm()=tensor(6.5322e-09, device='cuda:0')\n",
      "blocks.9.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.6014, device='cuda:0')\n",
      "blocks.9.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.3802, device='cuda:0')\n",
      "blocks.9.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.7301, device='cuda:0')\n",
      "blocks.9.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.1410, device='cuda:0')\n",
      "blocks.9.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7937, device='cuda:0')\n",
      "blocks.9.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5903, device='cuda:0')\n",
      "blocks.9.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.6236, device='cuda:0')\n",
      "blocks.9.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1541, device='cuda:0')\n",
      "blocks.10.edge_mask_attentions grad is all zeros\n",
      "blocks.10.edge_mask_mlp grad is all zeros\n",
      "blocks.10.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.9649, device='cuda:0')\n",
      "blocks.10.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.2903, device='cuda:0')\n",
      "blocks.10.attn.W_K grad is not all zeros, param.grad.norm()=tensor(1.4916, device='cuda:0')\n",
      "blocks.10.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.5129e-08, device='cuda:0')\n",
      "blocks.10.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.2587, device='cuda:0')\n",
      "blocks.10.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.2849, device='cuda:0')\n",
      "blocks.10.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.6736, device='cuda:0')\n",
      "blocks.10.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.0921, device='cuda:0')\n",
      "blocks.10.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.5749, device='cuda:0')\n",
      "blocks.10.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3302, device='cuda:0')\n",
      "blocks.10.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.1263, device='cuda:0')\n",
      "blocks.10.mlp.b_out grad is all zeros\n",
      "blocks.11.edge_mask_attentions grad is all zeros\n",
      "blocks.11.edge_mask_mlp grad is all zeros\n",
      "blocks.11.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.6651, device='cuda:0')\n",
      "blocks.11.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.2231, device='cuda:0')\n",
      "blocks.11.attn.W_K grad is not all zeros, param.grad.norm()=tensor(1.6382, device='cuda:0')\n",
      "blocks.11.attn.b_K grad is not all zeros, param.grad.norm()=tensor(3.8128e-08, device='cuda:0')\n",
      "blocks.11.attn.W_V grad is not all zeros, param.grad.norm()=tensor(0.8799, device='cuda:0')\n",
      "blocks.11.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.1831, device='cuda:0')\n",
      "blocks.11.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.3651, device='cuda:0')\n",
      "blocks.11.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.0663, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad: # and \"edge\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if correct MLPs flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'm2')\n",
      "(10, 'm10')\n",
      "(5, 'm5')\n",
      "(3, 'm3')\n",
      "(7, 'm7')\n",
      "(1, 'm1')\n",
      "(9, 'm9')\n",
      "(8, 'm8')\n",
      "(4, 'm4')\n",
      "(0, 'm0')\n",
      "(-1, 'embed')\n",
      "(6, 'm6')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"m\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.edge_mask_mlp grad is all zeros\n",
      "blocks.0.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.0.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.0.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.5009, device='cuda:0')\n",
      "blocks.0.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3840, device='cuda:0')\n",
      "blocks.0.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9580, device='cuda:0')\n",
      "blocks.0.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2079, device='cuda:0')\n",
      "blocks.1.edge_mask_mlp grad is all zeros\n",
      "blocks.1.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.1.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.1.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1881, device='cuda:0')\n",
      "blocks.1.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4780, device='cuda:0')\n",
      "blocks.1.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.0281, device='cuda:0')\n",
      "blocks.1.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2563, device='cuda:0')\n",
      "blocks.2.edge_mask_mlp grad is all zeros\n",
      "blocks.2.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.2.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.2.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7383, device='cuda:0')\n",
      "blocks.2.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4888, device='cuda:0')\n",
      "blocks.2.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4441, device='cuda:0')\n",
      "blocks.2.mlp.b_out grad is all zeros\n",
      "blocks.3.edge_mask_mlp grad is all zeros\n",
      "blocks.3.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.3.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.3.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1551, device='cuda:0')\n",
      "blocks.3.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3966, device='cuda:0')\n",
      "blocks.3.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.1005, device='cuda:0')\n",
      "blocks.3.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2987, device='cuda:0')\n",
      "blocks.4.edge_mask_mlp grad is all zeros\n",
      "blocks.4.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.4.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.4.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7344, device='cuda:0')\n",
      "blocks.4.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5089, device='cuda:0')\n",
      "blocks.4.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.5127, device='cuda:0')\n",
      "blocks.4.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3237, device='cuda:0')\n",
      "blocks.5.edge_mask_mlp grad is all zeros\n",
      "blocks.5.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.5.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.5.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8992, device='cuda:0')\n",
      "blocks.5.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5541, device='cuda:0')\n",
      "blocks.5.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4629, device='cuda:0')\n",
      "blocks.5.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3042, device='cuda:0')\n",
      "blocks.6.edge_mask_mlp grad is all zeros\n",
      "blocks.6.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.6.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.6.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.0009, device='cuda:0')\n",
      "blocks.6.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5724, device='cuda:0')\n",
      "blocks.6.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4607, device='cuda:0')\n",
      "blocks.6.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2637, device='cuda:0')\n",
      "blocks.7.edge_mask_mlp grad is all zeros\n",
      "blocks.7.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.7.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.7.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.3364, device='cuda:0')\n",
      "blocks.7.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4871, device='cuda:0')\n",
      "blocks.7.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9889, device='cuda:0')\n",
      "blocks.7.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2189, device='cuda:0')\n",
      "blocks.8.edge_mask_mlp grad is all zeros\n",
      "blocks.8.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.8.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.8.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8043, device='cuda:0')\n",
      "blocks.8.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6429, device='cuda:0')\n",
      "blocks.8.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.5990, device='cuda:0')\n",
      "blocks.8.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1610, device='cuda:0')\n",
      "blocks.9.edge_mask_mlp grad is all zeros\n",
      "blocks.9.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.9.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.9.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1774, device='cuda:0')\n",
      "blocks.9.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4569, device='cuda:0')\n",
      "blocks.9.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.3034, device='cuda:0')\n",
      "blocks.9.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1200, device='cuda:0')\n",
      "blocks.10.edge_mask_mlp grad is all zeros\n",
      "blocks.10.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.10.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.10.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.3321, device='cuda:0')\n",
      "blocks.10.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.2768, device='cuda:0')\n",
      "blocks.10.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(0.9347, device='cuda:0')\n",
      "blocks.10.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.0822, device='cuda:0')\n",
      "blocks.11.edge_mask_mlp grad is all zeros\n",
      "blocks.11.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.11.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.11.mlp.W_in grad is all zeros\n",
      "blocks.11.mlp.b_in grad is all zeros\n",
      "blocks.11.mlp.W_out grad is all zeros\n",
      "blocks.11.mlp.b_out grad is all zeros\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"mlp\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if individual attention heads have gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 'a9.7')\n",
      "(9, 'a9.9')\n",
      "(9, 'a9.6')\n",
      "(9, 'a9.8')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"a9\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1051, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0287, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1273, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0335, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0570, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0485, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 64, 768])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0306, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0389, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and \"9.attn.weight\" in name:\n",
    "    # if param.requires_grad and \"9.attn.b\" in name:\n",
    "        print(param.shape)\n",
    "        # if param.grad is not None and param.grad.sum() != 0:\n",
    "        #     print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        # else:\n",
    "        #     print(f\"{name} grad is all zeros\")\n",
    "        print(f\"{param.grad[3].norm()=}\")\n",
    "        print(f\"{param.grad[4].norm()=}\")\n",
    "        print(f\"{param.grad[7].norm()=}\")\n",
    "        print(f\"{param.grad[8].norm()=}\")\n",
    "        print(f\"{param.grad[11].norm()=}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Mask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from tasks import IOITask, SportsTask, OWTTask\n",
    "batch_size = 64\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=0)\n",
    "sports = SportsTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "owt = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "\n",
    "ioi_ood = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=1) # different template\n",
    "\n",
    "train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "task_weights = {\"ioi\": -.2, \"owt\": 1} # I think means preserve OWT, corrupt IOI\n",
    "eval_tasks = {\"ioi\": ioi, \"sports\": sports, \"owt\": owt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilliphguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/phillip_guo/mechanistic-unlearning/wandb/run-20240110_085753-mznr9fva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">vocal-deluge-22</a></strong> to <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 32/501 [10:48<2:38:21, 20.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_93609/3197613862.py\", line 15, in <module>\n",
      "    train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
      "  File \"/data/phillip_guo/mechanistic-unlearning/cb_utils/learn_mask.py\", line 178, in train_masks\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.learn_mask import train_masks\n",
    "\n",
    "epochs_left = 500\n",
    "steps_per_epoch = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "evaluate_every = 1\n",
    "discretize_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "use_wandb = False\n",
    "edge_mask_reg_strength = None\n",
    "weight_mask_reg_strength = 10\n",
    "\n",
    "wandb_config = {\"edge_masks\": edge_masks, \"weight_masks_attn\": weight_masks_attn, \"weight_masks_mlp\": weight_masks_mlp, \"epochs\": epochs_left, \"steps_per_epoch\": steps_per_epoch, \"lr\": lr, \"weight_decay\": weight_decay, \"evaluate_every\": evaluate_every, \"discretize_every\": discretize_every, \"threshold\": threshold, \"edge_mask_reg_strength\": edge_mask_reg_strength, \"weight_mask_reg_strength\": weight_mask_reg_strength}\n",
    "\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
    "            # param_names=param_names, mask_params=mask_params, \n",
    "            task_weights=task_weights, eval_tasks=eval_tasks, evaluate_every=evaluate_every, discretize_every=discretize_every, threshold=threshold, edge_mask_reg_strength=edge_mask_reg_strength, weight_mask_reg_strength=None, verbose=False, use_wandb=use_wandb, wandb_config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"masks/trained_mask_params_{epochs_left=}_{edge_mask_reg_strength=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, p in zip(param_names, mask_params):\n",
    "    if p.requires_grad:\n",
    "        # print(name, p)\n",
    "        # count how many zeros in p\n",
    "        print(torch.sum(p == 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
