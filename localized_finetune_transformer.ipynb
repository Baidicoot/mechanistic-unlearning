{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c4d3d7a46d45969d204b5f3434121d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "model_type = \"gemma-7b\"\n",
    "num_heads = 16\n",
    "if model_type == \"pythia\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8B\")#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8B\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "elif model_type == \"gemma-7b\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.bfloat16)#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    n_heads = 16\n",
    "    n_layers = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['a0.0_q', 'a0.1_q', 'a0.2_q', 'a0.3_q', 'a0.4_q', 'a0.5_q', 'a0.6_q', 'a0.7_q', 'a0.8_q', 'a0.9_q', 'a0.10_q', 'a0.11_q', 'a0.12_q', 'a0.13_q', 'a0.14_q', 'a0.15_q', 'a0.0_k', 'a0.1_k', 'a0.2_k', 'a0.3_k', 'a0.4_k', 'a0.5_k', 'a0.6_k', 'a0.7_k', 'a0.8_k', 'a0.9_k', 'a0.10_k', 'a0.11_k', 'a0.12_k', 'a0.13_k', 'a0.14_k', 'a0.15_k', 'a0.0_v', 'a0.1_v', 'a0.2_v', 'a0.3_v', 'a0.4_v', 'a0.5_v', 'a0.6_v', 'a0.7_v', 'a0.8_v', 'a0.9_v', 'a0.10_v', 'a0.11_v', 'a0.12_v', 'a0.13_v', 'a0.14_v', 'a0.15_v', 'a0.0_result', 'a0.1_result', 'a0.2_result', 'a0.3_result', 'a0.4_result', 'a0.5_result', 'a0.6_result', 'a0.7_result', 'a0.8_result', 'a0.9_result', 'a0.10_result', 'a0.11_result', 'a0.12_result', 'a0.13_result', 'a0.14_result', 'a0.15_result', 'a1.0_q', 'a1.1_q', 'a1.2_q', 'a1.3_q', 'a1.4_q', 'a1.5_q', 'a1.6_q', 'a1.7_q', 'a1.8_q', 'a1.9_q', 'a1.10_q', 'a1.11_q', 'a1.12_q', 'a1.13_q', 'a1.14_q', 'a1.15_q', 'a1.0_k', 'a1.1_k', 'a1.2_k', 'a1.3_k', 'a1.4_k', 'a1.5_k', 'a1.6_k', 'a1.7_k', 'a1.8_k', 'a1.9_k', 'a1.10_k', 'a1.11_k', 'a1.12_k', 'a1.13_k', 'a1.14_k', 'a1.15_k', 'a1.0_v', 'a1.1_v', 'a1.2_v', 'a1.3_v', 'a1.4_v', 'a1.5_v', 'a1.6_v', 'a1.7_v', 'a1.8_v', 'a1.9_v', 'a1.10_v', 'a1.11_v', 'a1.12_v', 'a1.13_v', 'a1.14_v', 'a1.15_v', 'a1.0_result', 'a1.1_result', 'a1.2_result', 'a1.3_result', 'a1.4_result', 'a1.5_result', 'a1.6_result', 'a1.7_result', 'a1.8_result', 'a1.9_result', 'a1.10_result', 'a1.11_result', 'a1.12_result', 'a1.13_result', 'a1.14_result', 'a1.15_result', 'a2.0_q', 'a2.1_q', 'a2.2_q', 'a2.3_q', 'a2.4_q', 'a2.5_q', 'a2.6_q', 'a2.7_q', 'a2.8_q', 'a2.9_q', 'a2.10_q', 'a2.11_q', 'a2.12_q', 'a2.13_q', 'a2.14_q', 'a2.15_q', 'a2.0_k', 'a2.1_k', 'a2.2_k', 'a2.3_k', 'a2.4_k', 'a2.5_k', 'a2.6_k', 'a2.7_k', 'a2.8_k', 'a2.9_k', 'a2.10_k', 'a2.11_k', 'a2.12_k', 'a2.13_k', 'a2.14_k', 'a2.15_k', 'a2.0_v', 'a2.1_v', 'a2.2_v', 'a2.3_v', 'a2.4_v', 'a2.5_v', 'a2.6_v', 'a2.7_v', 'a2.8_v', 'a2.9_v', 'a2.10_v', 'a2.11_v', 'a2.12_v', 'a2.13_v', 'a2.14_v', 'a2.15_v', 'a2.0_result', 'a2.1_result', 'a2.2_result', 'a2.3_result', 'a2.4_result', 'a2.5_result', 'a2.6_result', 'a2.7_result', 'a2.8_result', 'a2.9_result', 'a2.10_result', 'a2.11_result', 'a2.12_result', 'a2.13_result', 'a2.14_result', 'a2.15_result', 'a3.0_q', 'a3.1_q', 'a3.2_q', 'a3.3_q', 'a3.4_q', 'a3.5_q', 'a3.6_q', 'a3.7_q', 'a3.8_q', 'a3.9_q', 'a3.10_q', 'a3.11_q', 'a3.12_q', 'a3.13_q', 'a3.14_q', 'a3.15_q', 'a3.0_k', 'a3.1_k', 'a3.2_k', 'a3.3_k', 'a3.4_k', 'a3.5_k', 'a3.6_k', 'a3.7_k', 'a3.8_k', 'a3.9_k', 'a3.10_k', 'a3.11_k', 'a3.12_k', 'a3.13_k', 'a3.14_k', 'a3.15_k', 'a3.0_v', 'a3.1_v', 'a3.2_v', 'a3.3_v', 'a3.4_v', 'a3.5_v', 'a3.6_v', 'a3.7_v', 'a3.8_v', 'a3.9_v', 'a3.10_v', 'a3.11_v', 'a3.12_v', 'a3.13_v', 'a3.14_v', 'a3.15_v', 'a3.0_result', 'a3.1_result', 'a3.2_result', 'a3.3_result', 'a3.4_result', 'a3.5_result', 'a3.6_result', 'a3.7_result', 'a3.8_result', 'a3.9_result', 'a3.10_result', 'a3.11_result', 'a3.12_result', 'a3.13_result', 'a3.14_result', 'a3.15_result', 'a4.0_q', 'a4.1_q', 'a4.2_q', 'a4.3_q', 'a4.4_q', 'a4.5_q', 'a4.6_q', 'a4.7_q', 'a4.8_q', 'a4.9_q', 'a4.10_q', 'a4.11_q', 'a4.12_q', 'a4.13_q', 'a4.14_q', 'a4.15_q', 'a4.0_k', 'a4.1_k', 'a4.2_k', 'a4.3_k', 'a4.4_k', 'a4.5_k', 'a4.6_k', 'a4.7_k', 'a4.8_k', 'a4.9_k', 'a4.10_k', 'a4.11_k', 'a4.12_k', 'a4.13_k', 'a4.14_k', 'a4.15_k', 'a4.0_v', 'a4.1_v', 'a4.2_v', 'a4.3_v', 'a4.4_v', 'a4.5_v', 'a4.6_v', 'a4.7_v', 'a4.8_v', 'a4.9_v', 'a4.10_v', 'a4.11_v', 'a4.12_v', 'a4.13_v', 'a4.14_v', 'a4.15_v', 'a4.0_result', 'a4.1_result', 'a4.2_result', 'a4.3_result', 'a4.4_result', 'a4.5_result', 'a4.6_result', 'a4.7_result', 'a4.8_result', 'a4.9_result', 'a4.10_result', 'a4.11_result', 'a4.12_result', 'a4.13_result', 'a4.14_result', 'a4.15_result', 'a5.0_q', 'a5.1_q', 'a5.2_q', 'a5.3_q', 'a5.4_q', 'a5.5_q', 'a5.6_q', 'a5.7_q', 'a5.8_q', 'a5.9_q', 'a5.10_q', 'a5.11_q', 'a5.12_q', 'a5.13_q', 'a5.14_q', 'a5.15_q', 'a5.0_k', 'a5.1_k', 'a5.2_k', 'a5.3_k', 'a5.4_k', 'a5.5_k', 'a5.6_k', 'a5.7_k', 'a5.8_k', 'a5.9_k', 'a5.10_k', 'a5.11_k', 'a5.12_k', 'a5.13_k', 'a5.14_k', 'a5.15_k', 'a5.0_v', 'a5.1_v', 'a5.2_v', 'a5.3_v', 'a5.4_v', 'a5.5_v', 'a5.6_v', 'a5.7_v', 'a5.8_v', 'a5.9_v', 'a5.10_v', 'a5.11_v', 'a5.12_v', 'a5.13_v', 'a5.14_v', 'a5.15_v', 'a5.0_result', 'a5.1_result', 'a5.2_result', 'a5.3_result', 'a5.4_result', 'a5.5_result', 'a5.6_result', 'a5.7_result', 'a5.8_result', 'a5.9_result', 'a5.10_result', 'a5.11_result', 'a5.12_result', 'a5.13_result', 'a5.14_result', 'a5.15_result', 'a6.0_q', 'a6.1_q', 'a6.2_q', 'a6.3_q', 'a6.4_q', 'a6.5_q', 'a6.6_q', 'a6.7_q', 'a6.8_q', 'a6.9_q', 'a6.10_q', 'a6.11_q', 'a6.12_q', 'a6.13_q', 'a6.14_q', 'a6.15_q', 'a6.0_k', 'a6.1_k', 'a6.2_k', 'a6.3_k', 'a6.4_k', 'a6.5_k', 'a6.6_k', 'a6.7_k', 'a6.8_k', 'a6.9_k', 'a6.10_k', 'a6.11_k', 'a6.12_k', 'a6.13_k', 'a6.14_k', 'a6.15_k', 'a6.0_v', 'a6.1_v', 'a6.2_v', 'a6.3_v', 'a6.4_v', 'a6.5_v', 'a6.6_v', 'a6.7_v', 'a6.8_v', 'a6.9_v', 'a6.10_v', 'a6.11_v', 'a6.12_v', 'a6.13_v', 'a6.14_v', 'a6.15_v', 'a6.0_result', 'a6.1_result', 'a6.2_result', 'a6.3_result', 'a6.4_result', 'a6.5_result', 'a6.6_result', 'a6.7_result', 'a6.8_result', 'a6.9_result', 'a6.10_result', 'a6.11_result', 'a6.12_result', 'a6.13_result', 'a6.14_result', 'a6.15_result', 'a7.0_q', 'a7.1_q', 'a7.2_q', 'a7.3_q', 'a7.4_q', 'a7.5_q', 'a7.6_q', 'a7.7_q', 'a7.8_q', 'a7.9_q', 'a7.10_q', 'a7.11_q', 'a7.12_q', 'a7.13_q', 'a7.14_q', 'a7.15_q', 'a7.0_k', 'a7.1_k', 'a7.2_k', 'a7.3_k', 'a7.4_k', 'a7.5_k', 'a7.6_k', 'a7.7_k', 'a7.8_k', 'a7.9_k', 'a7.10_k', 'a7.11_k', 'a7.12_k', 'a7.13_k', 'a7.14_k', 'a7.15_k', 'a7.0_v', 'a7.1_v', 'a7.2_v', 'a7.3_v', 'a7.4_v', 'a7.5_v', 'a7.6_v', 'a7.7_v', 'a7.8_v', 'a7.9_v', 'a7.10_v', 'a7.11_v', 'a7.12_v', 'a7.13_v', 'a7.14_v', 'a7.15_v', 'a7.0_result', 'a7.1_result', 'a7.2_result', 'a7.3_result', 'a7.4_result', 'a7.5_result', 'a7.6_result', 'a7.7_result', 'a7.8_result', 'a7.9_result', 'a7.10_result', 'a7.11_result', 'a7.12_result', 'a7.13_result', 'a7.14_result', 'a7.15_result', 'a8.0_q', 'a8.1_q', 'a8.2_q', 'a8.3_q', 'a8.4_q', 'a8.5_q', 'a8.6_q', 'a8.7_q', 'a8.8_q', 'a8.9_q', 'a8.10_q', 'a8.11_q', 'a8.12_q', 'a8.13_q', 'a8.14_q', 'a8.15_q', 'a8.0_k', 'a8.1_k', 'a8.2_k', 'a8.3_k', 'a8.4_k', 'a8.5_k', 'a8.6_k', 'a8.7_k', 'a8.8_k', 'a8.9_k', 'a8.10_k', 'a8.11_k', 'a8.12_k', 'a8.13_k', 'a8.14_k', 'a8.15_k', 'a8.0_v', 'a8.1_v', 'a8.2_v', 'a8.3_v', 'a8.4_v', 'a8.5_v', 'a8.6_v', 'a8.7_v', 'a8.8_v', 'a8.9_v', 'a8.10_v', 'a8.11_v', 'a8.12_v', 'a8.13_v', 'a8.14_v', 'a8.15_v', 'a8.0_result', 'a8.1_result', 'a8.2_result', 'a8.3_result', 'a8.4_result', 'a8.5_result', 'a8.6_result', 'a8.7_result', 'a8.8_result', 'a8.9_result', 'a8.10_result', 'a8.11_result', 'a8.12_result', 'a8.13_result', 'a8.14_result', 'a8.15_result', 'a9.0_q', 'a9.1_q', 'a9.2_q', 'a9.3_q', 'a9.4_q', 'a9.5_q', 'a9.6_q', 'a9.7_q', 'a9.8_q', 'a9.9_q', 'a9.10_q', 'a9.11_q', 'a9.12_q', 'a9.13_q', 'a9.14_q', 'a9.15_q', 'a9.0_k', 'a9.1_k', 'a9.2_k', 'a9.3_k', 'a9.4_k', 'a9.5_k', 'a9.6_k', 'a9.7_k', 'a9.8_k', 'a9.9_k', 'a9.10_k', 'a9.11_k', 'a9.12_k', 'a9.13_k', 'a9.14_k', 'a9.15_k', 'a9.0_v', 'a9.1_v', 'a9.2_v', 'a9.3_v', 'a9.4_v', 'a9.5_v', 'a9.6_v', 'a9.7_v', 'a9.8_v', 'a9.9_v', 'a9.10_v', 'a9.11_v', 'a9.12_v', 'a9.13_v', 'a9.14_v', 'a9.15_v', 'a9.0_result', 'a9.1_result', 'a9.2_result', 'a9.3_result', 'a9.4_result', 'a9.5_result', 'a9.6_result', 'a9.7_result', 'a9.8_result', 'a9.9_result', 'a9.10_result', 'a9.11_result', 'a9.12_result', 'a9.13_result', 'a9.14_result', 'a9.15_result', 'a10.0_q', 'a10.1_q', 'a10.2_q', 'a10.3_q', 'a10.4_q', 'a10.5_q', 'a10.6_q', 'a10.7_q', 'a10.8_q', 'a10.9_q', 'a10.10_q', 'a10.11_q', 'a10.12_q', 'a10.13_q', 'a10.14_q', 'a10.15_q', 'a10.0_k', 'a10.1_k', 'a10.2_k', 'a10.3_k', 'a10.4_k', 'a10.5_k', 'a10.6_k', 'a10.7_k', 'a10.8_k', 'a10.9_k', 'a10.10_k', 'a10.11_k', 'a10.12_k', 'a10.13_k', 'a10.14_k', 'a10.15_k', 'a10.0_v', 'a10.1_v', 'a10.2_v', 'a10.3_v', 'a10.4_v', 'a10.5_v', 'a10.6_v', 'a10.7_v', 'a10.8_v', 'a10.9_v', 'a10.10_v', 'a10.11_v', 'a10.12_v', 'a10.13_v', 'a10.14_v', 'a10.15_v', 'a10.0_result', 'a10.1_result', 'a10.2_result', 'a10.3_result', 'a10.4_result', 'a10.5_result', 'a10.6_result', 'a10.7_result', 'a10.8_result', 'a10.9_result', 'a10.10_result', 'a10.11_result', 'a10.12_result', 'a10.13_result', 'a10.14_result', 'a10.15_result', 'a11.0_q', 'a11.1_q', 'a11.2_q', 'a11.3_q', 'a11.4_q', 'a11.5_q', 'a11.6_q', 'a11.7_q', 'a11.8_q', 'a11.9_q', 'a11.10_q', 'a11.11_q', 'a11.12_q', 'a11.13_q', 'a11.14_q', 'a11.15_q', 'a11.0_k', 'a11.1_k', 'a11.2_k', 'a11.3_k', 'a11.4_k', 'a11.5_k', 'a11.6_k', 'a11.7_k', 'a11.8_k', 'a11.9_k', 'a11.10_k', 'a11.11_k', 'a11.12_k', 'a11.13_k', 'a11.14_k', 'a11.15_k', 'a11.0_v', 'a11.1_v', 'a11.2_v', 'a11.3_v', 'a11.4_v', 'a11.5_v', 'a11.6_v', 'a11.7_v', 'a11.8_v', 'a11.9_v', 'a11.10_v', 'a11.11_v', 'a11.12_v', 'a11.13_v', 'a11.14_v', 'a11.15_v', 'a11.0_result', 'a11.1_result', 'a11.2_result', 'a11.3_result', 'a11.4_result', 'a11.5_result', 'a11.6_result', 'a11.7_result', 'a11.8_result', 'a11.9_result', 'a11.10_result', 'a11.11_result', 'a11.12_result', 'a11.13_result', 'a11.14_result', 'a11.15_result', 'a12.0_q', 'a12.1_q', 'a12.2_q', 'a12.3_q', 'a12.4_q', 'a12.5_q', 'a12.6_q', 'a12.7_q', 'a12.8_q', 'a12.9_q', 'a12.10_q', 'a12.11_q', 'a12.12_q', 'a12.13_q', 'a12.14_q', 'a12.15_q', 'a12.0_k', 'a12.1_k', 'a12.2_k', 'a12.3_k', 'a12.4_k', 'a12.5_k', 'a12.6_k', 'a12.7_k', 'a12.8_k', 'a12.9_k', 'a12.10_k', 'a12.11_k', 'a12.12_k', 'a12.13_k', 'a12.14_k', 'a12.15_k', 'a12.0_v', 'a12.1_v', 'a12.2_v', 'a12.3_v', 'a12.4_v', 'a12.5_v', 'a12.6_v', 'a12.7_v', 'a12.8_v', 'a12.9_v', 'a12.10_v', 'a12.11_v', 'a12.12_v', 'a12.13_v', 'a12.14_v', 'a12.15_v', 'a12.0_result', 'a12.1_result', 'a12.2_result', 'a12.3_result', 'a12.4_result', 'a12.5_result', 'a12.6_result', 'a12.7_result', 'a12.8_result', 'a12.9_result', 'a12.10_result', 'a12.11_result', 'a12.12_result', 'a12.13_result', 'a12.14_result', 'a12.15_result', 'a13.0_q', 'a13.1_q', 'a13.2_q', 'a13.3_q', 'a13.4_q', 'a13.5_q', 'a13.6_q', 'a13.7_q', 'a13.8_q', 'a13.9_q', 'a13.10_q', 'a13.11_q', 'a13.12_q', 'a13.13_q', 'a13.14_q', 'a13.15_q', 'a13.0_k', 'a13.1_k', 'a13.2_k', 'a13.3_k', 'a13.4_k', 'a13.5_k', 'a13.6_k', 'a13.7_k', 'a13.8_k', 'a13.9_k', 'a13.10_k', 'a13.11_k', 'a13.12_k', 'a13.13_k', 'a13.14_k', 'a13.15_k', 'a13.0_v', 'a13.1_v', 'a13.2_v', 'a13.3_v', 'a13.4_v', 'a13.5_v', 'a13.6_v', 'a13.7_v', 'a13.8_v', 'a13.9_v', 'a13.10_v', 'a13.11_v', 'a13.12_v', 'a13.13_v', 'a13.14_v', 'a13.15_v', 'a13.0_result', 'a13.1_result', 'a13.2_result', 'a13.3_result', 'a13.4_result', 'a13.5_result', 'a13.6_result', 'a13.7_result', 'a13.8_result', 'a13.9_result', 'a13.10_result', 'a13.11_result', 'a13.12_result', 'a13.13_result', 'a13.14_result', 'a13.15_result', 'a14.0_q', 'a14.1_q', 'a14.2_q', 'a14.3_q', 'a14.4_q', 'a14.5_q', 'a14.6_q', 'a14.7_q', 'a14.8_q', 'a14.9_q', 'a14.10_q', 'a14.11_q', 'a14.12_q', 'a14.13_q', 'a14.14_q', 'a14.15_q', 'a14.0_k', 'a14.1_k', 'a14.2_k', 'a14.3_k', 'a14.4_k', 'a14.5_k', 'a14.6_k', 'a14.7_k', 'a14.8_k', 'a14.9_k', 'a14.10_k', 'a14.11_k', 'a14.12_k', 'a14.13_k', 'a14.14_k', 'a14.15_k', 'a14.0_v', 'a14.1_v', 'a14.2_v', 'a14.3_v', 'a14.4_v', 'a14.5_v', 'a14.6_v', 'a14.7_v', 'a14.8_v', 'a14.9_v', 'a14.10_v', 'a14.11_v', 'a14.12_v', 'a14.13_v', 'a14.14_v', 'a14.15_v', 'a14.0_result', 'a14.1_result', 'a14.2_result', 'a14.3_result', 'a14.4_result', 'a14.5_result', 'a14.6_result', 'a14.7_result', 'a14.8_result', 'a14.9_result', 'a14.10_result', 'a14.11_result', 'a14.12_result', 'a14.13_result', 'a14.14_result', 'a14.15_result', 'a15.0_q', 'a15.1_q', 'a15.2_q', 'a15.3_q', 'a15.4_q', 'a15.5_q', 'a15.6_q', 'a15.7_q', 'a15.8_q', 'a15.9_q', 'a15.10_q', 'a15.11_q', 'a15.12_q', 'a15.13_q', 'a15.14_q', 'a15.15_q', 'a15.0_k', 'a15.1_k', 'a15.2_k', 'a15.3_k', 'a15.4_k', 'a15.5_k', 'a15.6_k', 'a15.7_k', 'a15.8_k', 'a15.9_k', 'a15.10_k', 'a15.11_k', 'a15.12_k', 'a15.13_k', 'a15.14_k', 'a15.15_k', 'a15.0_v', 'a15.1_v', 'a15.2_v', 'a15.3_v', 'a15.4_v', 'a15.5_v', 'a15.6_v', 'a15.7_v', 'a15.8_v', 'a15.9_v', 'a15.10_v', 'a15.11_v', 'a15.12_v', 'a15.13_v', 'a15.14_v', 'a15.15_v', 'a15.0_result', 'a15.1_result', 'a15.2_result', 'a15.3_result', 'a15.4_result', 'a15.5_result', 'a15.6_result', 'a15.7_result', 'a15.8_result', 'a15.9_result', 'a15.10_result', 'a15.11_result', 'a15.12_result', 'a15.13_result', 'a15.14_result', 'a15.15_result', 'a16.0_q', 'a16.1_q', 'a16.2_q', 'a16.3_q', 'a16.4_q', 'a16.5_q', 'a16.6_q', 'a16.7_q', 'a16.8_q', 'a16.9_q', 'a16.10_q', 'a16.11_q', 'a16.12_q', 'a16.13_q', 'a16.14_q', 'a16.15_q', 'a16.0_k', 'a16.1_k', 'a16.2_k', 'a16.3_k', 'a16.4_k', 'a16.5_k', 'a16.6_k', 'a16.7_k', 'a16.8_k', 'a16.9_k', 'a16.10_k', 'a16.11_k', 'a16.12_k', 'a16.13_k', 'a16.14_k', 'a16.15_k', 'a16.0_v', 'a16.1_v', 'a16.2_v', 'a16.3_v', 'a16.4_v', 'a16.5_v', 'a16.6_v', 'a16.7_v', 'a16.8_v', 'a16.9_v', 'a16.10_v', 'a16.11_v', 'a16.12_v', 'a16.13_v', 'a16.14_v', 'a16.15_v', 'a16.0_result', 'a16.1_result', 'a16.2_result', 'a16.3_result', 'a16.4_result', 'a16.5_result', 'a16.6_result', 'a16.7_result', 'a16.8_result', 'a16.9_result', 'a16.10_result', 'a16.11_result', 'a16.12_result', 'a16.13_result', 'a16.14_result', 'a16.15_result', 'a17.0_q', 'a17.1_q', 'a17.2_q', 'a17.3_q', 'a17.4_q', 'a17.5_q', 'a17.6_q', 'a17.7_q', 'a17.8_q', 'a17.9_q', 'a17.10_q', 'a17.11_q', 'a17.12_q', 'a17.13_q', 'a17.14_q', 'a17.15_q', 'a17.0_k', 'a17.1_k', 'a17.2_k', 'a17.3_k', 'a17.4_k', 'a17.5_k', 'a17.6_k', 'a17.7_k', 'a17.8_k', 'a17.9_k', 'a17.10_k', 'a17.11_k', 'a17.12_k', 'a17.13_k', 'a17.14_k', 'a17.15_k', 'a17.0_v', 'a17.1_v', 'a17.2_v', 'a17.3_v', 'a17.4_v', 'a17.5_v', 'a17.6_v', 'a17.7_v', 'a17.8_v', 'a17.9_v', 'a17.10_v', 'a17.11_v', 'a17.12_v', 'a17.13_v', 'a17.14_v', 'a17.15_v', 'a17.0_result', 'a17.1_result', 'a17.2_result', 'a17.3_result', 'a17.4_result', 'a17.5_result', 'a17.6_result', 'a17.7_result', 'a17.8_result', 'a17.9_result', 'a17.10_result', 'a17.11_result', 'a17.12_result', 'a17.13_result', 'a17.14_result', 'a17.15_result', 'a18.0_q', 'a18.1_q', 'a18.2_q', 'a18.3_q', 'a18.4_q', 'a18.5_q', 'a18.6_q', 'a18.7_q', 'a18.8_q', 'a18.9_q', 'a18.10_q', 'a18.11_q', 'a18.12_q', 'a18.13_q', 'a18.14_q', 'a18.15_q', 'a18.0_k', 'a18.1_k', 'a18.2_k', 'a18.3_k', 'a18.4_k', 'a18.5_k', 'a18.6_k', 'a18.7_k', 'a18.8_k', 'a18.9_k', 'a18.10_k', 'a18.11_k', 'a18.12_k', 'a18.13_k', 'a18.14_k', 'a18.15_k', 'a18.0_v', 'a18.1_v', 'a18.2_v', 'a18.3_v', 'a18.4_v', 'a18.5_v', 'a18.6_v', 'a18.7_v', 'a18.8_v', 'a18.9_v', 'a18.10_v', 'a18.11_v', 'a18.12_v', 'a18.13_v', 'a18.14_v', 'a18.15_v', 'a18.0_result', 'a18.1_result', 'a18.2_result', 'a18.3_result', 'a18.4_result', 'a18.5_result', 'a18.6_result', 'a18.7_result', 'a18.8_result', 'a18.9_result', 'a18.10_result', 'a18.11_result', 'a18.12_result', 'a18.13_result', 'a18.14_result', 'a18.15_result', 'a19.0_q', 'a19.1_q', 'a19.2_q', 'a19.3_q', 'a19.4_q', 'a19.5_q', 'a19.6_q', 'a19.7_q', 'a19.8_q', 'a19.9_q', 'a19.10_q', 'a19.11_q', 'a19.12_q', 'a19.13_q', 'a19.14_q', 'a19.15_q', 'a19.0_k', 'a19.1_k', 'a19.2_k', 'a19.3_k', 'a19.4_k', 'a19.5_k', 'a19.6_k', 'a19.7_k', 'a19.8_k', 'a19.9_k', 'a19.10_k', 'a19.11_k', 'a19.12_k', 'a19.13_k', 'a19.14_k', 'a19.15_k', 'a19.0_v', 'a19.1_v', 'a19.2_v', 'a19.3_v', 'a19.4_v', 'a19.5_v', 'a19.6_v', 'a19.7_v', 'a19.8_v', 'a19.9_v', 'a19.10_v', 'a19.11_v', 'a19.12_v', 'a19.13_v', 'a19.14_v', 'a19.15_v', 'a19.0_result', 'a19.1_result', 'a19.2_result', 'a19.3_result', 'a19.4_result', 'a19.5_result', 'a19.6_result', 'a19.7_result', 'a19.8_result', 'a19.9_result', 'a19.10_result', 'a19.11_result', 'a19.12_result', 'a19.13_result', 'a19.14_result', 'a19.15_result', 'a20.0_q', 'a20.1_q', 'a20.2_q', 'a20.3_q', 'a20.4_q', 'a20.5_q', 'a20.6_q', 'a20.7_q', 'a20.8_q', 'a20.9_q', 'a20.10_q', 'a20.11_q', 'a20.12_q', 'a20.13_q', 'a20.14_q', 'a20.15_q', 'a20.0_k', 'a20.1_k', 'a20.2_k', 'a20.3_k', 'a20.4_k', 'a20.5_k', 'a20.6_k', 'a20.7_k', 'a20.8_k', 'a20.9_k', 'a20.10_k', 'a20.11_k', 'a20.12_k', 'a20.13_k', 'a20.14_k', 'a20.15_k', 'a20.0_v', 'a20.1_v', 'a20.2_v', 'a20.3_v', 'a20.4_v', 'a20.5_v', 'a20.6_v', 'a20.7_v', 'a20.8_v', 'a20.9_v', 'a20.10_v', 'a20.11_v', 'a20.12_v', 'a20.13_v', 'a20.14_v', 'a20.15_v', 'a20.0_result', 'a20.1_result', 'a20.2_result', 'a20.3_result', 'a20.4_result', 'a20.5_result', 'a20.6_result', 'a20.7_result', 'a20.8_result', 'a20.9_result', 'a20.10_result', 'a20.11_result', 'a20.12_result', 'a20.13_result', 'a20.14_result', 'a20.15_result', 'a21.0_q', 'a21.1_q', 'a21.2_q', 'a21.3_q', 'a21.4_q', 'a21.5_q', 'a21.6_q', 'a21.7_q', 'a21.8_q', 'a21.9_q', 'a21.10_q', 'a21.11_q', 'a21.12_q', 'a21.13_q', 'a21.14_q', 'a21.15_q', 'a21.0_k', 'a21.1_k', 'a21.2_k', 'a21.3_k', 'a21.4_k', 'a21.5_k', 'a21.6_k', 'a21.7_k', 'a21.8_k', 'a21.9_k', 'a21.10_k', 'a21.11_k', 'a21.12_k', 'a21.13_k', 'a21.14_k', 'a21.15_k', 'a21.0_v', 'a21.1_v', 'a21.2_v', 'a21.3_v', 'a21.4_v', 'a21.5_v', 'a21.6_v', 'a21.7_v', 'a21.8_v', 'a21.9_v', 'a21.10_v', 'a21.11_v', 'a21.12_v', 'a21.13_v', 'a21.14_v', 'a21.15_v', 'a21.0_result', 'a21.1_result', 'a21.2_result', 'a21.3_result', 'a21.4_result', 'a21.5_result', 'a21.6_result', 'a21.7_result', 'a21.8_result', 'a21.9_result', 'a21.10_result', 'a21.11_result', 'a21.12_result', 'a21.13_result', 'a21.14_result', 'a21.15_result', 'a22.0_q', 'a22.1_q', 'a22.2_q', 'a22.3_q', 'a22.4_q', 'a22.5_q', 'a22.6_q', 'a22.7_q', 'a22.8_q', 'a22.9_q', 'a22.10_q', 'a22.11_q', 'a22.12_q', 'a22.13_q', 'a22.14_q', 'a22.15_q', 'a22.0_k', 'a22.1_k', 'a22.2_k', 'a22.3_k', 'a22.4_k', 'a22.5_k', 'a22.6_k', 'a22.7_k', 'a22.8_k', 'a22.9_k', 'a22.10_k', 'a22.11_k', 'a22.12_k', 'a22.13_k', 'a22.14_k', 'a22.15_k', 'a22.0_v', 'a22.1_v', 'a22.2_v', 'a22.3_v', 'a22.4_v', 'a22.5_v', 'a22.6_v', 'a22.7_v', 'a22.8_v', 'a22.9_v', 'a22.10_v', 'a22.11_v', 'a22.12_v', 'a22.13_v', 'a22.14_v', 'a22.15_v', 'a22.0_result', 'a22.1_result', 'a22.2_result', 'a22.3_result', 'a22.4_result', 'a22.5_result', 'a22.6_result', 'a22.7_result', 'a22.8_result', 'a22.9_result', 'a22.10_result', 'a22.11_result', 'a22.12_result', 'a22.13_result', 'a22.14_result', 'a22.15_result', 'a23.0_q', 'a23.1_q', 'a23.2_q', 'a23.3_q', 'a23.4_q', 'a23.5_q', 'a23.6_q', 'a23.7_q', 'a23.8_q', 'a23.9_q', 'a23.10_q', 'a23.11_q', 'a23.12_q', 'a23.13_q', 'a23.14_q', 'a23.15_q', 'a23.0_k', 'a23.1_k', 'a23.2_k', 'a23.3_k', 'a23.4_k', 'a23.5_k', 'a23.6_k', 'a23.7_k', 'a23.8_k', 'a23.9_k', 'a23.10_k', 'a23.11_k', 'a23.12_k', 'a23.13_k', 'a23.14_k', 'a23.15_k', 'a23.0_v', 'a23.1_v', 'a23.2_v', 'a23.3_v', 'a23.4_v', 'a23.5_v', 'a23.6_v', 'a23.7_v', 'a23.8_v', 'a23.9_v', 'a23.10_v', 'a23.11_v', 'a23.12_v', 'a23.13_v', 'a23.14_v', 'a23.15_v', 'a23.0_result', 'a23.1_result', 'a23.2_result', 'a23.3_result', 'a23.4_result', 'a23.5_result', 'a23.6_result', 'a23.7_result', 'a23.8_result', 'a23.9_result', 'a23.10_result', 'a23.11_result', 'a23.12_result', 'a23.13_result', 'a23.14_result', 'a23.15_result', 'a24.0_q', 'a24.1_q', 'a24.2_q', 'a24.3_q', 'a24.4_q', 'a24.5_q', 'a24.6_q', 'a24.7_q', 'a24.8_q', 'a24.9_q', 'a24.10_q', 'a24.11_q', 'a24.12_q', 'a24.13_q', 'a24.14_q', 'a24.15_q', 'a24.0_k', 'a24.1_k', 'a24.2_k', 'a24.3_k', 'a24.4_k', 'a24.5_k', 'a24.6_k', 'a24.7_k', 'a24.8_k', 'a24.9_k', 'a24.10_k', 'a24.11_k', 'a24.12_k', 'a24.13_k', 'a24.14_k', 'a24.15_k', 'a24.0_v', 'a24.1_v', 'a24.2_v', 'a24.3_v', 'a24.4_v', 'a24.5_v', 'a24.6_v', 'a24.7_v', 'a24.8_v', 'a24.9_v', 'a24.10_v', 'a24.11_v', 'a24.12_v', 'a24.13_v', 'a24.14_v', 'a24.15_v', 'a24.0_result', 'a24.1_result', 'a24.2_result', 'a24.3_result', 'a24.4_result', 'a24.5_result', 'a24.6_result', 'a24.7_result', 'a24.8_result', 'a24.9_result', 'a24.10_result', 'a24.11_result', 'a24.12_result', 'a24.13_result', 'a24.14_result', 'a24.15_result', 'a25.0_q', 'a25.1_q', 'a25.2_q', 'a25.3_q', 'a25.4_q', 'a25.5_q', 'a25.6_q', 'a25.7_q', 'a25.8_q', 'a25.9_q', 'a25.10_q', 'a25.11_q', 'a25.12_q', 'a25.13_q', 'a25.14_q', 'a25.15_q', 'a25.0_k', 'a25.1_k', 'a25.2_k', 'a25.3_k', 'a25.4_k', 'a25.5_k', 'a25.6_k', 'a25.7_k', 'a25.8_k', 'a25.9_k', 'a25.10_k', 'a25.11_k', 'a25.12_k', 'a25.13_k', 'a25.14_k', 'a25.15_k', 'a25.0_v', 'a25.1_v', 'a25.2_v', 'a25.3_v', 'a25.4_v', 'a25.5_v', 'a25.6_v', 'a25.7_v', 'a25.8_v', 'a25.9_v', 'a25.10_v', 'a25.11_v', 'a25.12_v', 'a25.13_v', 'a25.14_v', 'a25.15_v', 'a25.0_result', 'a25.1_result', 'a25.2_result', 'a25.3_result', 'a25.4_result', 'a25.5_result', 'a25.6_result', 'a25.7_result', 'a25.8_result', 'a25.9_result', 'a25.10_result', 'a25.11_result', 'a25.12_result', 'a25.13_result', 'a25.14_result', 'a25.15_result', 'a26.0_q', 'a26.1_q', 'a26.2_q', 'a26.3_q', 'a26.4_q', 'a26.5_q', 'a26.6_q', 'a26.7_q', 'a26.8_q', 'a26.9_q', 'a26.10_q', 'a26.11_q', 'a26.12_q', 'a26.13_q', 'a26.14_q', 'a26.15_q', 'a26.0_k', 'a26.1_k', 'a26.2_k', 'a26.3_k', 'a26.4_k', 'a26.5_k', 'a26.6_k', 'a26.7_k', 'a26.8_k', 'a26.9_k', 'a26.10_k', 'a26.11_k', 'a26.12_k', 'a26.13_k', 'a26.14_k', 'a26.15_k', 'a26.0_v', 'a26.1_v', 'a26.2_v', 'a26.3_v', 'a26.4_v', 'a26.5_v', 'a26.6_v', 'a26.7_v', 'a26.8_v', 'a26.9_v', 'a26.10_v', 'a26.11_v', 'a26.12_v', 'a26.13_v', 'a26.14_v', 'a26.15_v', 'a26.0_result', 'a26.1_result', 'a26.2_result', 'a26.3_result', 'a26.4_result', 'a26.5_result', 'a26.6_result', 'a26.7_result', 'a26.8_result', 'a26.9_result', 'a26.10_result', 'a26.11_result', 'a26.12_result', 'a26.13_result', 'a26.14_result', 'a26.15_result', 'a27.0_q', 'a27.1_q', 'a27.2_q', 'a27.3_q', 'a27.4_q', 'a27.5_q', 'a27.6_q', 'a27.7_q', 'a27.8_q', 'a27.9_q', 'a27.10_q', 'a27.11_q', 'a27.12_q', 'a27.13_q', 'a27.14_q', 'a27.15_q', 'a27.0_k', 'a27.1_k', 'a27.2_k', 'a27.3_k', 'a27.4_k', 'a27.5_k', 'a27.6_k', 'a27.7_k', 'a27.8_k', 'a27.9_k', 'a27.10_k', 'a27.11_k', 'a27.12_k', 'a27.13_k', 'a27.14_k', 'a27.15_k', 'a27.0_v', 'a27.1_v', 'a27.2_v', 'a27.3_v', 'a27.4_v', 'a27.5_v', 'a27.6_v', 'a27.7_v', 'a27.8_v', 'a27.9_v', 'a27.10_v', 'a27.11_v', 'a27.12_v', 'a27.13_v', 'a27.14_v', 'a27.15_v', 'a27.0_result', 'a27.1_result', 'a27.2_result', 'a27.3_result', 'a27.4_result', 'a27.5_result', 'a27.6_result', 'a27.7_result', 'a27.8_result', 'a27.9_result', 'a27.10_result', 'a27.11_result', 'a27.12_result', 'a27.13_result', 'a27.14_result', 'a27.15_result', 'm0_in', 'm0_out', 'm1_in', 'm1_out', 'm2_in', 'm2_out', 'm3_in', 'm3_out', 'm4_in', 'm4_out', 'm5_in', 'm5_out', 'm6_in', 'm6_out', 'm7_in', 'm7_out', 'm8_in', 'm8_out', 'm9_in', 'm9_out', 'm10_in', 'm10_out', 'm11_in', 'm11_out', 'm12_in', 'm12_out', 'm13_in', 'm13_out', 'm14_in', 'm14_out', 'm15_in', 'm15_out', 'm16_in', 'm16_out', 'm17_in', 'm17_out', 'm18_in', 'm18_out', 'm19_in', 'm19_out', 'm20_in', 'm20_out', 'm21_in', 'm21_out', 'm22_in', 'm22_out', 'm23_in', 'm23_out', 'm24_in', 'm24_out', 'm25_in', 'm25_out', 'm26_in', 'm26_out', 'm27_in', 'm27_out'])\n",
      "len(all_attr_values)=168\n",
      "Thresholding importance at 0.20463522672653203\n",
      "(['blocks.18.attn.hook_result', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.23.mlp.hook_post', 'blocks.25.attn.hook_result', 'blocks.25.mlp.hook_pre', 'blocks.27.attn.hook_q', 'blocks.27.attn.hook_k', 'blocks.27.mlp.hook_post'], defaultdict(<class 'list'>, {'blocks.18.attn.hook_result': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'blocks.25.attn.hook_result': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'blocks.27.attn.hook_q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'blocks.27.attn.hook_k': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}))\n",
      "Thresholding importance at 0.11414513221153846\n",
      "(['blocks.0.mlp.hook_pre', 'blocks.13.mlp.hook_pre', 'blocks.19.mlp.hook_pre', 'blocks.20.mlp.hook_post', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.22.mlp.hook_post', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_post', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_post', 'blocks.26.mlp.hook_post', 'blocks.27.mlp.hook_post', 'blocks.17.attn.hook_v', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_result', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_result', 'blocks.27.attn.hook_q', 'blocks.27.attn.hook_k'], defaultdict(<class 'list'>, {'blocks.17.attn.hook_v': [7], 'blocks.18.attn.hook_v': [11], 'blocks.18.attn.hook_result': [11], 'blocks.20.attn.hook_v': [5], 'blocks.20.attn.hook_result': [5], 'blocks.27.attn.hook_q': [1], 'blocks.27.attn.hook_k': [1]}))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"models/google_gemma-7b_sports_all_ap_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)\n",
    "print(ap_graph.keys())\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_attrs_to_components(attrs, combine_heads=False, n_layers=n_layers, n_heads=n_heads):\n",
    "    \"\"\"\n",
    "    attrs is dictionary of e.g. {'a0.0_q': float, 'm27_in': float}\n",
    "\n",
    "    If combine_heads, then it will combine all 'a0.0_q', 'a0.1_q', ..., 'a0.15_q', etc into one component.\n",
    "    \"\"\"\n",
    "\n",
    "    component_dict = defaultdict(int)\n",
    "    attn_head_dict = defaultdict(dict)\n",
    "    for layer in range(n_layers):\n",
    "        for attn_type, component_name in [(\"q\", f\"blocks.{layer}.attn.hook_q\"), (\"k\", f\"blocks.{layer}.attn.hook_k\"), (\"v\", f\"blocks.{layer}.attn.hook_v\"), (\"result\", f\"blocks.{layer}.attn.hook_result\")]:\n",
    "            for head in range(n_heads):    \n",
    "                if combine_heads:\n",
    "                    component_dict[component_name] += attrs[f\"a{layer}.{head}_{attn_type}\"]\n",
    "                else:\n",
    "                    attn_head_dict[component_name][head] = attrs[f\"a{layer}.{head}_{attn_type}\"]\n",
    "        for mlp_type, component_name in [(\"in\", f\"blocks.{layer}.mlp.hook_pre\"), (\"out\", f\"blocks.{layer}.mlp.hook_post\")]:\n",
    "            component_dict[component_name] += attrs[f\"m{layer}_{mlp_type}\"]\n",
    "    if combine_heads:\n",
    "        return (component_dict,)\n",
    "    return (component_dict, attn_head_dict,)\n",
    "\n",
    "\n",
    "def get_top_components(component_dict, attn_head_dict=None, threshold=None, top_p=None, top_k=None, use_abs=True, n_layers=n_layers, n_heads=n_heads):\n",
    "    \"\"\"\n",
    "    component_dict is a dictionary of components to their importance values. If attn_head_dict is not None, then component_dict and attn_head_dict should not overlap in values.\n",
    "\n",
    "    Can either use a threshold, top_p, or top_k to determine the top components to return (can only specify one). top_p should be a value ranging from 0 to 100. If use_abs is True, then it will take the absolute value of the importance values. \n",
    "    \"\"\"\n",
    "    if attn_head_dict is not None:\n",
    "        assert (component_dict.keys() & attn_head_dict.keys()) == set(), \"Overlapping keys between component_dict and attn_head_dict\"\n",
    "    \n",
    "    # assert only one of threshold, top_p, top_k is specified\n",
    "    assert sum([threshold is not None, top_p is not None, top_k is not None]) == 1, \"Can only specify one of threshold, top_p, top_k\"\n",
    "    # will calculate a threshold for top_p or top_k\n",
    "\n",
    "    if top_p is not None:\n",
    "        all_attr_values = list(component_dict.values())\n",
    "        if attn_head_dict is not None:\n",
    "            all_attr_values += [val for head_dict in attn_head_dict.values() for val in head_dict.values()]\n",
    "\n",
    "        all_attr_values = np.array(all_attr_values)\n",
    "        if use_abs:\n",
    "            all_attr_values = np.abs(all_attr_values)\n",
    "        print(f\"{len(all_attr_values)=}\")\n",
    "        threshold = np.percentile(all_attr_values, 100 - top_p)\n",
    "    elif top_k is not None:\n",
    "        all_attr_values = list(component_dict.values())\n",
    "        if attn_head_dict is not None:\n",
    "            all_attr_values += [val for head_dict in attn_head_dict.values() for val in head_dict.values()]\n",
    "\n",
    "        all_attr_values = np.array(all_attr_values)\n",
    "        if use_abs:\n",
    "            all_attr_values = np.abs(all_attr_values)\n",
    "        threshold = np.sort(all_attr_values)[-top_k]\n",
    "    \n",
    "    print(f\"Thresholding importance at {threshold}\")\n",
    "    final_components = []\n",
    "    final_attn_heads = defaultdict(list)\n",
    "\n",
    "    for component, importance in component_dict.items():\n",
    "        if use_abs:\n",
    "            importance = abs(importance)\n",
    "        if importance >= threshold:\n",
    "            final_components.append(component)    \n",
    "\n",
    "    if attn_head_dict is not None:\n",
    "        for component, head_dict in attn_head_dict.items():\n",
    "            head_list = []\n",
    "            for head, importance in head_dict.items():\n",
    "                if use_abs:\n",
    "                    importance = abs(importance)\n",
    "                if importance >= threshold:\n",
    "                    head_list.append(head)\n",
    "            if len(head_list) > 0:\n",
    "                final_attn_heads[component] = head_list\n",
    "                final_components.append(component)\n",
    "    else:\n",
    "        for component in final_components:\n",
    "            if \"attn\" in component:\n",
    "                # want to mask over all possible heads\n",
    "                final_attn_heads[component] = list(range(n_heads))\n",
    "    \n",
    "    return final_components, final_attn_heads\n",
    "print(get_top_components(*convert_attrs_to_components(ap_graph, combine_heads=True), top_p=5))\n",
    "print(get_top_components(*convert_attrs_to_components(ap_graph, combine_heads=False), top_k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_attr_values)=168\n",
      "Thresholding importance at 0.20463522672653203\n",
      "len(all_attr_values)=168\n",
      "Thresholding importance at 9.1552734375e-05\n",
      "Setting blocks.0.attn.hook_q to True\n",
      "Setting blocks.0.attn.hook_k to True\n",
      "Setting blocks.0.attn.hook_v to True\n",
      "Ignoring blocks.0.attn.hook_result\n",
      "Could not find parameter for blocks.0.attn.hook_result\n",
      "Setting blocks.0.mlp.hook_pre to True\n",
      "Setting blocks.0.mlp.hook_post to True\n",
      "Setting blocks.1.attn.hook_q to True\n",
      "Setting blocks.1.attn.hook_k to True\n",
      "Setting blocks.1.attn.hook_v to True\n",
      "Ignoring blocks.1.attn.hook_result\n",
      "Could not find parameter for blocks.1.attn.hook_result\n",
      "Setting blocks.1.mlp.hook_pre to True\n",
      "Setting blocks.1.mlp.hook_post to True\n",
      "Setting blocks.2.attn.hook_q to True\n",
      "Setting blocks.2.attn.hook_k to True\n",
      "Setting blocks.2.attn.hook_v to True\n",
      "Ignoring blocks.2.attn.hook_result\n",
      "Could not find parameter for blocks.2.attn.hook_result\n",
      "Setting blocks.2.mlp.hook_pre to True\n",
      "Setting blocks.2.mlp.hook_post to True\n",
      "Setting blocks.3.attn.hook_q to True\n",
      "Setting blocks.3.attn.hook_k to True\n",
      "Setting blocks.3.attn.hook_v to True\n",
      "Ignoring blocks.3.attn.hook_result\n",
      "Could not find parameter for blocks.3.attn.hook_result\n",
      "Setting blocks.3.mlp.hook_pre to True\n",
      "Setting blocks.3.mlp.hook_post to True\n",
      "Setting blocks.4.attn.hook_q to True\n",
      "Setting blocks.4.attn.hook_k to True\n",
      "Setting blocks.4.attn.hook_v to True\n",
      "Ignoring blocks.4.attn.hook_result\n",
      "Could not find parameter for blocks.4.attn.hook_result\n",
      "Setting blocks.4.mlp.hook_pre to True\n",
      "Setting blocks.4.mlp.hook_post to True\n",
      "Setting blocks.5.attn.hook_q to True\n",
      "Setting blocks.5.attn.hook_k to True\n",
      "Setting blocks.5.attn.hook_v to True\n",
      "Ignoring blocks.5.attn.hook_result\n",
      "Could not find parameter for blocks.5.attn.hook_result\n",
      "Setting blocks.5.mlp.hook_pre to True\n",
      "Setting blocks.5.mlp.hook_post to True\n",
      "Setting blocks.6.attn.hook_q to True\n",
      "Setting blocks.6.attn.hook_k to True\n",
      "Setting blocks.6.attn.hook_v to True\n",
      "Ignoring blocks.6.attn.hook_result\n",
      "Could not find parameter for blocks.6.attn.hook_result\n",
      "Setting blocks.6.mlp.hook_pre to True\n",
      "Setting blocks.6.mlp.hook_post to True\n",
      "Setting blocks.7.attn.hook_q to True\n",
      "Setting blocks.7.attn.hook_k to True\n",
      "Setting blocks.7.attn.hook_v to True\n",
      "Ignoring blocks.7.attn.hook_result\n",
      "Could not find parameter for blocks.7.attn.hook_result\n",
      "Setting blocks.7.mlp.hook_pre to True\n",
      "Setting blocks.7.mlp.hook_post to True\n",
      "Setting blocks.8.attn.hook_q to True\n",
      "Setting blocks.8.attn.hook_k to True\n",
      "Setting blocks.8.attn.hook_v to True\n",
      "Ignoring blocks.8.attn.hook_result\n",
      "Could not find parameter for blocks.8.attn.hook_result\n",
      "Setting blocks.8.mlp.hook_pre to True\n",
      "Setting blocks.8.mlp.hook_post to True\n",
      "Setting blocks.9.attn.hook_q to True\n",
      "Setting blocks.9.attn.hook_k to True\n",
      "Setting blocks.9.attn.hook_v to True\n",
      "Ignoring blocks.9.attn.hook_result\n",
      "Could not find parameter for blocks.9.attn.hook_result\n",
      "Setting blocks.9.mlp.hook_pre to True\n",
      "Setting blocks.9.mlp.hook_post to True\n",
      "Setting blocks.10.attn.hook_q to True\n",
      "Setting blocks.10.attn.hook_k to True\n",
      "Setting blocks.10.attn.hook_v to True\n",
      "Ignoring blocks.10.attn.hook_result\n",
      "Could not find parameter for blocks.10.attn.hook_result\n",
      "Setting blocks.10.mlp.hook_pre to True\n",
      "Setting blocks.10.mlp.hook_post to True\n",
      "Setting blocks.11.attn.hook_q to True\n",
      "Setting blocks.11.attn.hook_k to True\n",
      "Setting blocks.11.attn.hook_v to True\n",
      "Ignoring blocks.11.attn.hook_result\n",
      "Could not find parameter for blocks.11.attn.hook_result\n",
      "Setting blocks.11.mlp.hook_pre to True\n",
      "Setting blocks.11.mlp.hook_post to True\n",
      "Setting blocks.12.attn.hook_q to True\n",
      "Setting blocks.12.attn.hook_k to True\n",
      "Setting blocks.12.attn.hook_v to True\n",
      "Ignoring blocks.12.attn.hook_result\n",
      "Could not find parameter for blocks.12.attn.hook_result\n",
      "Setting blocks.12.mlp.hook_pre to True\n",
      "Setting blocks.12.mlp.hook_post to True\n",
      "Setting blocks.13.attn.hook_q to True\n",
      "Setting blocks.13.attn.hook_k to True\n",
      "Setting blocks.13.attn.hook_v to True\n",
      "Ignoring blocks.13.attn.hook_result\n",
      "Could not find parameter for blocks.13.attn.hook_result\n",
      "Setting blocks.13.mlp.hook_pre to True\n",
      "Setting blocks.13.mlp.hook_post to True\n",
      "Setting blocks.14.attn.hook_q to True\n",
      "Setting blocks.14.attn.hook_k to True\n",
      "Setting blocks.14.attn.hook_v to True\n",
      "Ignoring blocks.14.attn.hook_result\n",
      "Could not find parameter for blocks.14.attn.hook_result\n",
      "Setting blocks.14.mlp.hook_pre to True\n",
      "Setting blocks.14.mlp.hook_post to True\n",
      "Setting blocks.15.attn.hook_q to True\n",
      "Setting blocks.15.attn.hook_k to True\n",
      "Setting blocks.15.attn.hook_v to True\n",
      "Ignoring blocks.15.attn.hook_result\n",
      "Could not find parameter for blocks.15.attn.hook_result\n",
      "Setting blocks.15.mlp.hook_pre to True\n",
      "Setting blocks.15.mlp.hook_post to True\n",
      "Setting blocks.16.attn.hook_q to True\n",
      "Setting blocks.16.attn.hook_k to True\n",
      "Setting blocks.16.attn.hook_v to True\n",
      "Ignoring blocks.16.attn.hook_result\n",
      "Could not find parameter for blocks.16.attn.hook_result\n",
      "Setting blocks.16.mlp.hook_pre to True\n",
      "Setting blocks.16.mlp.hook_post to True\n",
      "Setting blocks.17.attn.hook_q to True\n",
      "Setting blocks.17.attn.hook_k to True\n",
      "Setting blocks.17.attn.hook_v to True\n",
      "Ignoring blocks.17.attn.hook_result\n",
      "Could not find parameter for blocks.17.attn.hook_result\n",
      "Setting blocks.17.mlp.hook_pre to True\n",
      "Setting blocks.17.mlp.hook_post to True\n",
      "Setting blocks.18.attn.hook_q to True\n",
      "Setting blocks.18.attn.hook_k to True\n",
      "Setting blocks.18.attn.hook_v to True\n",
      "Ignoring blocks.18.attn.hook_result\n",
      "Could not find parameter for blocks.18.attn.hook_result\n",
      "Setting blocks.18.mlp.hook_pre to True\n",
      "Setting blocks.18.mlp.hook_post to True\n",
      "Setting blocks.19.attn.hook_q to True\n",
      "Setting blocks.19.attn.hook_k to True\n",
      "Setting blocks.19.attn.hook_v to True\n",
      "Ignoring blocks.19.attn.hook_result\n",
      "Could not find parameter for blocks.19.attn.hook_result\n",
      "Setting blocks.19.mlp.hook_pre to True\n",
      "Setting blocks.19.mlp.hook_post to True\n",
      "Setting blocks.20.attn.hook_q to True\n",
      "Setting blocks.20.attn.hook_k to True\n",
      "Setting blocks.20.attn.hook_v to True\n",
      "Ignoring blocks.20.attn.hook_result\n",
      "Could not find parameter for blocks.20.attn.hook_result\n",
      "Setting blocks.20.mlp.hook_pre to True\n",
      "Setting blocks.20.mlp.hook_post to True\n",
      "Setting blocks.21.attn.hook_q to True\n",
      "Setting blocks.21.attn.hook_k to True\n",
      "Setting blocks.21.attn.hook_v to True\n",
      "Ignoring blocks.21.attn.hook_result\n",
      "Could not find parameter for blocks.21.attn.hook_result\n",
      "Setting blocks.21.mlp.hook_pre to True\n",
      "Setting blocks.21.mlp.hook_post to True\n",
      "Setting blocks.22.attn.hook_q to True\n",
      "Setting blocks.22.attn.hook_k to True\n",
      "Setting blocks.22.attn.hook_v to True\n",
      "Ignoring blocks.22.attn.hook_result\n",
      "Could not find parameter for blocks.22.attn.hook_result\n",
      "Setting blocks.22.mlp.hook_pre to True\n",
      "Setting blocks.22.mlp.hook_post to True\n",
      "Setting blocks.23.attn.hook_q to True\n",
      "Setting blocks.23.attn.hook_k to True\n",
      "Setting blocks.23.attn.hook_v to True\n",
      "Ignoring blocks.23.attn.hook_result\n",
      "Could not find parameter for blocks.23.attn.hook_result\n",
      "Setting blocks.23.mlp.hook_pre to True\n",
      "Setting blocks.23.mlp.hook_post to True\n",
      "Setting blocks.24.attn.hook_q to True\n",
      "Setting blocks.24.attn.hook_k to True\n",
      "Setting blocks.24.attn.hook_v to True\n",
      "Ignoring blocks.24.attn.hook_result\n",
      "Could not find parameter for blocks.24.attn.hook_result\n",
      "Setting blocks.24.mlp.hook_pre to True\n",
      "Setting blocks.24.mlp.hook_post to True\n",
      "Setting blocks.25.attn.hook_q to True\n",
      "Setting blocks.25.attn.hook_k to True\n",
      "Setting blocks.25.attn.hook_v to True\n",
      "Ignoring blocks.25.attn.hook_result\n",
      "Could not find parameter for blocks.25.attn.hook_result\n",
      "Setting blocks.25.mlp.hook_pre to True\n",
      "Setting blocks.25.mlp.hook_post to True\n",
      "Setting blocks.26.attn.hook_q to True\n",
      "Setting blocks.26.attn.hook_k to True\n",
      "Setting blocks.26.attn.hook_v to True\n",
      "Ignoring blocks.26.attn.hook_result\n",
      "Could not find parameter for blocks.26.attn.hook_result\n",
      "Setting blocks.26.mlp.hook_pre to True\n",
      "Setting blocks.26.mlp.hook_post to True\n",
      "Setting blocks.27.attn.hook_q to True\n",
      "Setting blocks.27.attn.hook_k to True\n",
      "Setting blocks.27.attn.hook_v to True\n",
      "Ignoring blocks.27.attn.hook_result\n",
      "Could not find parameter for blocks.27.attn.hook_result\n",
      "Setting blocks.27.mlp.hook_pre to True\n",
      "Setting blocks.27.mlp.hook_post to True\n"
     ]
    }
   ],
   "source": [
    "top_p = 5\n",
    "combine_heads = True\n",
    "components, _ = get_top_components(*convert_attrs_to_components(ap_graph, combine_heads=combine_heads), top_p=top_p)\n",
    "use_localized = False\n",
    "\n",
    "# for every element in components, which looks like ['blocks.18.attn.hook_result', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_v']:\n",
    "# convert into equivalent parameter and set requires_grad to true\n",
    "def get_parameter(hf_model, component_name):\n",
    "    layer_str, component_type, hook_type = component_name.split(\".\")[1:]\n",
    "    layer = int(layer_str)\n",
    "\n",
    "    param = None\n",
    "    if component_type == \"attn\":\n",
    "        if hook_type == \"hook_q\":\n",
    "            param = hf_model.model.layers[layer].self_attn.q_proj.weight\n",
    "        elif hook_type == \"hook_k\":\n",
    "            param = hf_model.model.layers[layer].self_attn.k_proj.weight\n",
    "        elif hook_type == \"hook_v\":\n",
    "            param = hf_model.model.layers[layer].self_attn.v_proj.weight\n",
    "        elif hook_type == \"hook_result\":\n",
    "            # for now ignore, not sure if result maps to o_proj\n",
    "            print(f\"Ignoring {component_name}\")\n",
    "            # param = hf_model.model.layers[layer].self_attn.o_proj.weight\n",
    "        else:\n",
    "            print(f\"Unknown component type {component_type}\")\n",
    "    elif component_type == \"mlp\":\n",
    "        if hook_type == \"hook_pre\":\n",
    "            param = hf_model.model.layers[layer].mlp.up_proj.weight\n",
    "        elif hook_type == \"hook_post\":\n",
    "            param = hf_model.model.layers[layer].mlp.down_proj.weight\n",
    "        else:\n",
    "            print(f\"Unknown component type {component_type}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown component type {component_type}\")\n",
    "    \n",
    "    return param\n",
    "    \n",
    "def apply_localized_gradients(hf_model, components):\n",
    "    # set everything else False\n",
    "    for parameter in hf_model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    for component in components:\n",
    "        param = get_parameter(hf_model, component)\n",
    "        if param is None:\n",
    "            print(f\"Could not find parameter for {component}\")\n",
    "            continue\n",
    "        param.requires_grad = True\n",
    "        print(f\"Setting {component} to True\")\n",
    "\n",
    "\n",
    "if use_localized:\n",
    "    apply_localized_gradients(reference_model, components)\n",
    "else:\n",
    "    all_components, _ = get_top_components(*convert_attrs_to_components(ap_graph, combine_heads=combine_heads), top_p=100)\n",
    "    apply_localized_gradients(reference_model, all_components)\n",
    "    # fine tune all \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_localized_gradients(hf_model, attn_dict, mlp_dict, model_type=\"gemma\"):\n",
    "#     # attn_dict is {layer: {\"W_Q\": [set of unlearn_heads], \"W_K\": [set of unlearn_heads], \"W_V\": [set of unlearn_heads], \"W_V\": [set of unlearn_heads]} for every layer}\n",
    "#     # mlp_dict is {layer: boolean} for if you want to unlearn on this layer\n",
    "\n",
    "#     # set everything else False\n",
    "#     for parameter in hf_model.parameters():\n",
    "#         parameter.requires_grad = False\n",
    "\n",
    "\n",
    "#     for layer in range(hf_model.config.num_hidden_layers):\n",
    "#         if model_type == \"gemma\":\n",
    "#             # set attn.W_Q layers requires_grad to True if W_Q unlearn heads is not empty, same for W_K, W_V, W_O\n",
    "\n",
    "#             for attn_component_name, parameter in [(\"W_Q\", hf_model.model.layers[layer].self_attn.q_proj.weight), (\"W_K\", hf_model.model.layers[layer].self_attn.k_proj.weight), (\"W_V\", hf_model.model.layers[layer].self_attn.v_proj.weight), (\"W_O\", hf_model.model.layers[layer].self_attn.o_proj.weight)]:\n",
    "#                 if attn_dict is None or (layer in attn_dict and len(attn_dict[layer][attn_component_name]) > 0):\n",
    "#                     parameter.requires_grad = True\n",
    "#                 else:\n",
    "#                     parameter.requires_grad = False\n",
    "\n",
    "#             if mlp_dict is None or (layer in mlp_dict and mlp_dict[layer]):\n",
    "#                 hf_model.model.layers[layer].mlp.up_proj.weight.requires_grad = True\n",
    "#                 hf_model.model.layers[layer].mlp.down_proj.weight.requires_grad = True\n",
    "\n",
    "# import pickle\n",
    "# with open(\"models/google_gemma-7b_sports_baseball_ap_graph.pkl\", \"rb\") as f:\n",
    "#     ap_graph = pickle.load(f)\n",
    "# # add up attributions across attentions\n",
    "# aggregated_attributions = {}\n",
    "# for layer in range(reference_model.config.num_hidden_layers):\n",
    "#     component_name = f'a{layer}'\n",
    "#     aggregated_attributions[component_name] = 0\n",
    "#     for head in range(num_heads):\n",
    "#         for head_type in [\"q\", \"k\", \"v\"]:\n",
    "#             head_name = f\"{component_name}.{head}_{head_type}\"\n",
    "#             aggregated_attributions[component_name] += ap_graph[head_name]\n",
    "#         # head_name = f\"{component_name}.{head}\"\n",
    "#         # aggregated_attributions[component_name] += ap_graph[head_name]\n",
    "#     aggregated_attributions[f'm{layer}'] = 0\n",
    "#     for mlp_type in [\"in\", \"out\"]:\n",
    "#         mlp_name = f'm{layer}_{mlp_type}'\n",
    "#         aggregated_attributions[f\"m{layer}\"] += ap_graph[mlp_name]\n",
    "\n",
    "# print(aggregated_attributions)\n",
    "\n",
    "# num_components=20\n",
    "# top_components = {}\n",
    "# # take the top 20 components from aggregated_attributions (20 highest absolute values)\n",
    "# for i in range(num_components):\n",
    "#     max_key = max(aggregated_attributions, key=lambda x: abs(aggregated_attributions[x]))\n",
    "#     top_components[max_key] = aggregated_attributions[max_key]\n",
    "#     del aggregated_attributions[max_key]\n",
    "\n",
    "# def get_dicts_from_nodes(nodes_set):\n",
    "#     # get attn_dict and mlp_dict\n",
    "#     attn_dict = {}\n",
    "#     mlp_dict = {}\n",
    "#     for node in nodes_set:\n",
    "#         if node[0] == \"a\":\n",
    "#             layer = int(node[1:])\n",
    "#             attn_dict[layer] = {\"W_Q\": list(range(num_heads)), \"W_K\": list(range(num_heads)), \"W_V\": list(range(num_heads)), \"W_O\": list(range(num_heads))}\n",
    "#         elif node[0] == \"m\":\n",
    "#             layer = int(node[1:])\n",
    "#             mlp_dict[layer] = True\n",
    "#     return attn_dict, mlp_dict\n",
    "\n",
    "# attn_dict, mlp_dict = get_dicts_from_nodes(top_components.keys())\n",
    "# attn_dict, mlp_dict\n",
    "# apply_localized_gradients(reference_model, attn_dict, mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.weight True\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.weight True\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.weight True\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.weight True\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.weight True\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.weight True\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.weight True\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.weight True\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.weight True\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.weight True\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.weight True\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.weight True\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.layers.12.self_attn.q_proj.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.weight True\n",
      "model.layers.12.self_attn.o_proj.weight False\n",
      "model.layers.12.mlp.gate_proj.weight False\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight False\n",
      "model.layers.12.post_attention_layernorm.weight False\n",
      "model.layers.13.self_attn.q_proj.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.weight True\n",
      "model.layers.13.self_attn.o_proj.weight False\n",
      "model.layers.13.mlp.gate_proj.weight False\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight False\n",
      "model.layers.13.post_attention_layernorm.weight False\n",
      "model.layers.14.self_attn.q_proj.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.weight True\n",
      "model.layers.14.self_attn.o_proj.weight False\n",
      "model.layers.14.mlp.gate_proj.weight False\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight False\n",
      "model.layers.14.post_attention_layernorm.weight False\n",
      "model.layers.15.self_attn.q_proj.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.weight True\n",
      "model.layers.15.self_attn.o_proj.weight False\n",
      "model.layers.15.mlp.gate_proj.weight False\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight False\n",
      "model.layers.15.post_attention_layernorm.weight False\n",
      "model.layers.16.self_attn.q_proj.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.weight True\n",
      "model.layers.16.self_attn.o_proj.weight False\n",
      "model.layers.16.mlp.gate_proj.weight False\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight False\n",
      "model.layers.16.post_attention_layernorm.weight False\n",
      "model.layers.17.self_attn.q_proj.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.weight True\n",
      "model.layers.17.self_attn.o_proj.weight False\n",
      "model.layers.17.mlp.gate_proj.weight False\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight False\n",
      "model.layers.17.post_attention_layernorm.weight False\n",
      "model.layers.18.self_attn.q_proj.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.weight True\n",
      "model.layers.18.self_attn.o_proj.weight False\n",
      "model.layers.18.mlp.gate_proj.weight False\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight False\n",
      "model.layers.18.post_attention_layernorm.weight False\n",
      "model.layers.19.self_attn.q_proj.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.weight True\n",
      "model.layers.19.self_attn.o_proj.weight False\n",
      "model.layers.19.mlp.gate_proj.weight False\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight False\n",
      "model.layers.19.post_attention_layernorm.weight False\n",
      "model.layers.20.self_attn.q_proj.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.weight True\n",
      "model.layers.20.self_attn.o_proj.weight False\n",
      "model.layers.20.mlp.gate_proj.weight False\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight False\n",
      "model.layers.20.post_attention_layernorm.weight False\n",
      "model.layers.21.self_attn.q_proj.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.weight True\n",
      "model.layers.21.self_attn.o_proj.weight False\n",
      "model.layers.21.mlp.gate_proj.weight False\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight False\n",
      "model.layers.21.post_attention_layernorm.weight False\n",
      "model.layers.22.self_attn.q_proj.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.weight True\n",
      "model.layers.22.self_attn.o_proj.weight False\n",
      "model.layers.22.mlp.gate_proj.weight False\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight False\n",
      "model.layers.22.post_attention_layernorm.weight False\n",
      "model.layers.23.self_attn.q_proj.weight True\n",
      "model.layers.23.self_attn.k_proj.weight True\n",
      "model.layers.23.self_attn.v_proj.weight True\n",
      "model.layers.23.self_attn.o_proj.weight False\n",
      "model.layers.23.mlp.gate_proj.weight False\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight False\n",
      "model.layers.23.post_attention_layernorm.weight False\n",
      "model.layers.24.self_attn.q_proj.weight True\n",
      "model.layers.24.self_attn.k_proj.weight True\n",
      "model.layers.24.self_attn.v_proj.weight True\n",
      "model.layers.24.self_attn.o_proj.weight False\n",
      "model.layers.24.mlp.gate_proj.weight False\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight False\n",
      "model.layers.24.post_attention_layernorm.weight False\n",
      "model.layers.25.self_attn.q_proj.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.weight True\n",
      "model.layers.25.self_attn.o_proj.weight False\n",
      "model.layers.25.mlp.gate_proj.weight False\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight False\n",
      "model.layers.25.post_attention_layernorm.weight False\n",
      "model.layers.26.self_attn.q_proj.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.weight True\n",
      "model.layers.26.self_attn.o_proj.weight False\n",
      "model.layers.26.mlp.gate_proj.weight False\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight False\n",
      "model.layers.26.post_attention_layernorm.weight False\n",
      "model.layers.27.self_attn.q_proj.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.weight True\n",
      "model.layers.27.self_attn.o_proj.weight False\n",
      "model.layers.27.mlp.gate_proj.weight False\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight False\n",
      "model.layers.27.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in reference_model.named_parameters():\n",
    "    print(name, parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99609375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model.cuda()\n",
    "sports_test = SportsTask(batch_size=32, tokenizer=tokenizer)\n",
    "sports_test.get_test_accuracy(reference_model)\n",
    "\n",
    "# for layer in range(tl_model.cfg.n_layers):\n",
    "#     tl_model.blocks[layer].attn.W_Q.data = torch.zeros_like(tl_model.blocks[layer].attn.W_Q)\n",
    "#     tl_model.blocks[layer].attn.W_K.data = torch.zeros_like(tl_model.blocks[layer].attn.W_K)\n",
    "#     tl_model.blocks[layer].attn.W_V.data = torch.zeros_like(tl_model.blocks[layer].attn.W_V)\n",
    "#     tl_model.blocks[layer].attn.W_O.data = torch.zeros_like(tl_model.blocks[layer].attn.W_O)\n",
    "\n",
    "# sports_test.get_test_loss(tl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafecdc3837a4f28a3c0a8a0c1b516bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd1e53b1a1d40d5b712488e045cf181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0afb35fe021484a92b76b91181f353b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301c3accbb9b44cbb772ded5d54ee6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    }
   ],
   "source": [
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "from tasks.facts.SportsTaskAdversarial import adversarial_sports_eval\n",
    "from tasks.facts.SportsTaskSideEffects import run_side_effects_evals\n",
    "\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size=32\n",
    "\n",
    "device = \"cuda\"\n",
    "train_loss_type = \"sports\"\n",
    "forget_sport = \"basketball\"\n",
    "maintain_sport = None\n",
    "# val_sport = \"baseball\"\n",
    "\n",
    "\n",
    "sports_1mp = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"log_1_minus_p\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "\n",
    "if maintain_sport is None:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "else:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "\n",
    "train_pile = PileTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, ctx_length=256, shuffle=True, buffer_size=50000)\n",
    "train_tasks = {\"sports_1mp\": (sports_1mp, .2), \"maintain_sports\": (maintain_sports, 1), \"pile\": (train_pile, 1)}\n",
    "\n",
    "# want to eval on other sports\n",
    "forget_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "test_pile = PileTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "\n",
    "induction_eval = InductionTask(batch_size=eval_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, device=device)\n",
    "if maintain_sport is None:\n",
    "    maintain_sports_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sports_eval}\n",
    "else:\n",
    "    maintain_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "    val_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={val_sport}, is_forget_dataset=True)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": \n",
    "                  forget_sport_eval, \"maintain_sport\": maintain_sport_eval, \"val_sport\": val_sport_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f159d3f04248b0be2141a9dbff20df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adversarial evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running side effects evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c85da94a554255b0eda043458b112e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c113188f165a4efaa731b68fc7ae3e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc181f2334b479f8e8167ebf4da154c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78aa5c77082477094dfaef5f0a6e9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f101cbef6457442dbcf4763f899f01ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adversarial evals\n",
      "Running side effects evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451e32562e7943a488ba60f9e60dbbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2fd5550fb74d959cc4fa325e2d2d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11ba6b28e344f3184f024909f1e12df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25e1f601ff44fe0b34315539d312a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba54c4dfd95f4edbb1b5ba712dbaf644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adversarial evals\n",
      "Running side effects evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffac1db24964122bac3262e77195699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870d394e2cf146c08a5e84fbc032f12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3adbaa01b145ae8f484a7e79d5cb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491d848b85594429818affee28bac8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b0004695914d0dab5be4bbccd3258b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mask = MLPHiddenMask(model).cuda()\n",
    "\n",
    "learning_rate = 2e-5\n",
    "n_epochs = 50\n",
    "grad_accum_steps = 16\n",
    "\n",
    "clip_grad = 1\n",
    "\n",
    "evaluate_every = 5\n",
    "n_eval_iters = 5\n",
    "deep_evaluate_every = 25\n",
    "do_adversarial_evals = True\n",
    "do_side_effects_evals = True\n",
    "\n",
    "from collections import defaultdict\n",
    "all_train_losses = defaultdict(list)\n",
    "all_test_losses = defaultdict(list)\n",
    "adversarial_evals = []\n",
    "side_effect_evals = []\n",
    "\n",
    "# Initialize optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(reference_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs)\n",
    "# Cycle dataloaders\n",
    "# Train a sparse mask\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    # Sample batches\n",
    "    # Reset grad\n",
    "    optimizer.zero_grad()\n",
    "    # Compute normal loss over retain\n",
    "    for task_name, (task, task_weight) in train_tasks.items():\n",
    "        task_loss = 0\n",
    "        for i in range(grad_accum_steps):\n",
    "            loss = task.get_train_loss(reference_model) / grad_accum_steps\n",
    "            task_loss += loss.item()\n",
    "            loss *= task_weight\n",
    "            loss.backward()\n",
    "        all_train_losses[task_name].append(task_loss)\n",
    "        \n",
    "    # # Add sparsity loss and backprop\n",
    "    # loss = beta * mask.regularization_loss()\n",
    "    # loss.backward()\n",
    "    # all_train_losses[\"reg\"].append(loss.item())\n",
    "    # Step and log\n",
    "    if clip_grad is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(reference_model.parameters(), clip_grad)\n",
    "    # zero_nan_grads(mask)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % evaluate_every == 0 or epoch == n_epochs - 1:\n",
    "        for task_name, task in eval_tasks.items():\n",
    "            task_loss = 0\n",
    "            task_accuracy = 0\n",
    "            for i in range(n_eval_iters):\n",
    "                task_loss += task.get_test_loss(reference_model).item()\n",
    "                task_accuracy += task.get_test_accuracy(reference_model)\n",
    "\n",
    "            all_test_losses[f\"{task_name}_ce\"].append(task_loss / n_eval_iters)\n",
    "            all_test_losses[f\"{task_name}_acc\"].append(task_accuracy / n_eval_iters)\n",
    "    if epoch % deep_evaluate_every == 0 or epoch == n_epochs - 1:\n",
    "        if do_adversarial_evals:\n",
    "            print(\"Running adversarial evals\")\n",
    "            adversarial_evals.append(adversarial_sports_eval(reference_model, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True))\n",
    "        if do_side_effects_evals:\n",
    "            print(\"Running side effects evals\")\n",
    "            side_effect_evals.append(run_side_effects_evals(reference_model, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\", \"Cross Entropy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'sports_1mp': [2.72265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'maintain_sports': [0.216461181640625, 10.8828125, 2.32763671875, 9.755859375, 6.7890625, 6.0244140625, 0.724609375, 9.033203125, 1.639892578125, 1.265380859375, 16.80859375, 152.1875, 17.369140625, 10.13671875, 1.4146728515625, 6.328125, 5.7578125, 3.42578125, 1.06640625, 2.77587890625, 3.36865234375, 1.9119873046875, 0.716796875, 0.65771484375, 0.77490234375, 0.7490234375, 0.754638671875, 0.748291015625, 0.71044921875, 0.692138671875, 0.695556640625, 0.737060546875, 0.673583984375, 0.69482421875, 0.69189453125, 0.6728515625, 0.677978515625, 0.669921875, 0.684326171875, 0.666259765625, 0.670166015625, 0.6845703125, 0.673583984375, 0.66748046875, 0.6826171875, 0.66162109375, 0.69189453125, 0.693603515625, 0.69091796875, 0.677001953125], 'pile': [1.85107421875, 2.23095703125, 4.2021484375, 2.16455078125, 2.4208984375, 2.43017578125, 4.7861328125, 2.46337890625, 2.33544921875, 2.2890625, 2.27392578125, 2.20166015625, 2.22119140625, 2.43603515625, 2.33447265625, 2.26513671875, 2.21044921875, 2.078125, 2.1240234375, 2.17431640625, 2.15185546875, 2.2119140625, 2.1123046875, 2.18798828125, 2.01904296875, 2.0869140625, 2.02783203125, 2.03271484375, 2.01708984375, 1.9521484375, 1.92919921875, 2.12841796875, 1.90234375, 1.93212890625, 1.93896484375, 1.9912109375, 2.04931640625, 1.92431640625, 1.98388671875, 1.99365234375, 1.92138671875, 1.97705078125, 1.92236328125, 1.79638671875, 1.8017578125, 1.8349609375, 1.8896484375, 1.9755859375, 1.94677734375, 1.958984375]})\n",
      "defaultdict(<class 'list'>, {'induction_ce': [3.503125, 22.9, 8.36875, 7.0, 5.10625, 3.81875, 2.790625, 2.684375, 2.025, 2.4046875, 2.4046875], 'induction_acc': [0.93125, 0.10625, 0.70625, 0.83125, 0.78125, 0.86875, 0.93125, 0.94375, 0.95, 0.96875, 0.94375], 'pile_ce': [2.890625, 6.50625, 2.928125, 2.81875, 2.68125, 2.6125, 2.48125, 2.428125, 2.45, 2.440625, 2.465625], 'pile_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'forget_sport_ce': [19.525, 13.8875, 161.6, 23.8, 13.8125, 12.2, 12.375, 12.25, 12.175, 12.15, 12.1625], 'forget_sport_acc': [9.012222290039062e-05, 9.834766387939453e-07, 5.547917680814862e-12, 4.1018211049959065e-11, 9.581446647644043e-07, 4.941225051879883e-06, 4.231929779052734e-06, 4.607439041137695e-06, 5.21540641784668e-06, 5.346536636352539e-06, 5.441904067993164e-06], 'maintain_sport_ce': [11.05, 0.6859375, 151.2, 6.975, 1.7765625, 0.70625, 0.7828125, 0.69453125, 0.6796875, 0.67890625, 0.675], 'maintain_sport_acc': [0.519921875, 0.506640625, 0.5234375, 0.5, 0.5265625, 0.4953125, 0.503125, 0.499609375, 0.5171875, 0.5109375, 0.51484375]})\n",
      "[{'Normal': {'football': 0.9476562500000001, 'baseball': 0.12470703125, 'basketball': 8.916854858398438e-05}, 'MC': {'football': 0.45703125, 'baseball': 0.3328125, 'basketball': 0.21347656250000002}, 'Capitalized': {'football': 0.92421875, 'baseball': 0.154296875, 'basketball': 0.00021190643310546874}, 'Dashed': {'football': 0.004399697855114937, 'baseball': 0.9874371290206909, 'basketball': 0.0007528761285357177}}, {'Normal': {'football': 0.33125, 'baseball': 0.6703125, 'basketball': 0.00183563232421875}, 'MC': {'football': 0.5828125, 'baseball': 0.17128906249999998, 'basketball': 0.2443359375}, 'Capitalized': {'football': 0.50625, 'baseball': 0.51328125, 'basketball': 5.4550170898437496e-05}, 'Dashed': {'football': 8.915548241930082e-05, 'baseball': 0.9999894022941589, 'basketball': 3.633795745549019e-19}}, {'Normal': {'football': 0.281640625, 'baseball': 0.721875, 'basketball': 0.003173828125}, 'MC': {'football': 0.5648437500000001, 'baseball': 0.1736328125, 'basketball': 0.23808593749999998}, 'Capitalized': {'football': 0.4546875, 'baseball': 0.60390625, 'basketball': 0.00011653900146484374}, 'Dashed': {'football': 3.65811828828555e-08, 'baseball': 1.0, 'basketball': 5.694441235190486e-18}}]\n",
      "[{'Sports Answers': {'football': 0.64, 'baseball': 0.0, 'basketball': 0.0, 'tennis': 0.0}, 'Cross Entropy': {'Pile': 2.9125, 'OWT': 3.596875}}, {'Sports Answers': {'football': 0.0, 'baseball': 0.9, 'basketball': 0.0, 'tennis': 0.0}, 'Cross Entropy': {'Pile': 2.575, 'OWT': 3.06875}}, {'Sports Answers': {'football': 0.0, 'baseball': 1.0, 'basketball': 0.0, 'tennis': 0.0}, 'Cross Entropy': {'Pile': 2.45, 'OWT': 2.928125}}]\n"
     ]
    }
   ],
   "source": [
    "print(all_train_losses)\n",
    "print(all_test_losses)\n",
    "print(adversarial_evals)\n",
    "print(side_effect_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save masks state dict to neuron_cb\n",
    "import pickle\n",
    "\n",
    "with open(f\"masks/localized_ft/{model_type}_{'localized' if use_localized else 'nonlocalized'}_{combine_heads=}_unlearn_{forget_sport}_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": all_train_losses, \"test_losses\": all_test_losses, \"adversarial_evals\": adversarial_evals, \"side_effect_evals\": side_effect_evals}, f)\n",
    "\n",
    "torch.save(reference_model.state_dict(), f\"masks/localized_ft/{model_type}_{'localized' if use_localized else 'nonlocalized'}_{combine_heads=}_unlearn_{forget_sport}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks/localized_ft/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalized\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39muse_localized\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonlocalized\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombine_heads\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m_unlearn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforget_sport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_metrics.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_type' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_type = \"gemma-7b\"\n",
    "forget_sport = \"basketball\"\n",
    "# use_localized = False\n",
    "combine_heads = True\n",
    "\n",
    "\n",
    "for use_localized in [True, False]:\n",
    "    with open(f\"masks/localized_ft/{model_type}_{'localized' if use_localized else 'nonlocalized'}_{combine_heads=}_unlearn_{forget_sport}_metrics.pkl\", \"rb\") as f:\n",
    "        metrics = pickle.load(f)\n",
    "        train_losses = metrics[\"train_losses\"]\n",
    "        test_losses = metrics[\"test_losses\"]\n",
    "        adversarial_evals = metrics[\"adversarial_evals\"]\n",
    "        side_effect_evals = metrics[\"side_effect_evals\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def make_partly_differentiable_mask(W, unfrozen_heads, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    W is Parameter of shape (n_heads, ...). Returns baseline and frozen (both only 1d arrays of (n_heads,)), and forward pass should be W_baseline.float() + W_frozen.float() * W \n",
    "    \"\"\"\n",
    "    W_baseline = torch.nn.Parameter(torch.zeros(W.shape[0], dtype=torch.bool), requires_grad=False).to(device)\n",
    "\n",
    "    # unsqueeze to broadcast efficiently, until W_baseline has same shape as W\n",
    "    while len(W_baseline.shape) < len(W.shape):\n",
    "        W_baseline = W_baseline.unsqueeze(-1)\n",
    "    \n",
    "    W_baseline[unfrozen_heads] = True\n",
    "    # W_baseline = ~W_frozen\n",
    "    W_frozen = torch.nn.Parameter(~W_baseline, requires_grad=False)\n",
    "    # convert into float\n",
    "    return W_frozen.float(), W_baseline.float()\n",
    "\n",
    "class WeightMaskedTransformer(nn.Module):\n",
    "    def __init__(self, tl_transformer, weight_mask_attn_dict=None, weight_mask_mlp_dict=None, torch_dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        weight_mask_attn_dict: {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}} (frozen_heads is shape (n_heads,) of bools). If none, train mask over all heads\n",
    "        weight_mask_mlp_dict: {layer: bool}. If none, train mask over all mlps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.torch_dtype = torch_dtype\n",
    "        # tl_transformer should be a HookedTransformer\n",
    "        self.tl_transformer = tl_transformer\n",
    "        # turn off gradients for tl_transformer\n",
    "        for param in self.tl_transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.weight_mask_attn_dict = weight_mask_attn_dict\n",
    "        self.weight_mask_mlp_dict = weight_mask_mlp_dict\n",
    "        # store weight masks for every component that is unfrozen\n",
    "        \n",
    "        # need to store reference weights so that you can reset W_Q, etc after a forward pass\n",
    "        self.reference_attn_weights = {}\n",
    "        self.reference_mlp_weights = {}\n",
    "\n",
    "        self.attention_masks = {}\n",
    "        self.mlp_masks = {}\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            self.attention_masks[layer] = {}\n",
    "            self.reference_attn_weights[layer] = {}\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None:\n",
    "                    unfrozen_heads = list(range(self.tl_transformer.cfg.n_heads)) # all heads are unfrozen\n",
    "                else:\n",
    "                    unfrozen_heads = self.weight_mask_attn_dict[layer][component]\n",
    "                # make frozen and baseline masks, and also a copy of the original weights\n",
    "\n",
    "                if len(unfrozen_heads) > 0:\n",
    "                    W_frozen, W_baseline = make_partly_differentiable_mask(parameter, unfrozen_heads)\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "                    \n",
    "                    self.attention_masks[layer][component] = (W_frozen, W_baseline, weight_mask)\n",
    "                    self.reference_attn_weights[layer][component] = parameter.clone()\n",
    "\n",
    "            if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer]:\n",
    "                in_weight_mask = nn.Parameter(torch.ones_like(self.tl_transformer.blocks[layer].mlp.W_in).type(torch_dtype), requires_grad=True)\n",
    "                out_weight_mask = nn.Parameter(torch.ones_like(self.tl_transformer.blocks[layer].mlp.W_out).type(torch_dtype), requires_grad=True)\n",
    "\n",
    "                self.mlp_masks[layer] = (in_weight_mask, out_weight_mask)\n",
    "                self.reference_mlp_weights[layer] = (self.tl_transformer.blocks[layer].mlp.W_in.clone(), self.tl_transformer.blocks[layer].mlp.W_out.clone())\n",
    "\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "\n",
    "                if self.weight_mask_attn_dict is None or len(self.attention_masks[layer]) > 0:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    reference_data = self.reference_attn_weights[layer][component]\n",
    "                    mask = W_baseline + W_frozen * weight_mask\n",
    "\n",
    "                    # parameter = reference_data * mask\n",
    "                    if component == \"W_Q\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_Q.data = self.tl_transformer.blocks[layer].attn.W_Q * mask# * reference_data\n",
    "                    elif component == \"W_K\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_K.data = self.tl_transformer.blocks[layer].attn.W_K * mask# * reference_data\n",
    "                    elif component == \"W_V\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_V.data = self.tl_transformer.blocks[layer].attn.W_V * mask# * reference_data\n",
    "                    elif component == \"W_O\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_O.data = self.tl_transformer.blocks[layer].attn.W_O * mask# * reference_data\n",
    "\n",
    "            if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer]:\n",
    "                in_weight_mask, out_weight_mask = self.mlp_masks[layer]\n",
    "                reference_in_data, reference_out_data = self.reference_mlp_weights[layer]\n",
    "                # self.tl_transformer.blocks[layer].mlp.W_in = reference_in_data * in_weight_mask\n",
    "                # self.tl_transformer.blocks[layer].mlp.W_out = reference_out_data * out_weight_mask\n",
    "                self.tl_transformer.blocks[layer].mlp.W_in.data = reference_in_data * in_weight_mask\n",
    "                self.tl_transformer.blocks[layer].mlp.W_out.data = reference_out_data * out_weight_mask\n",
    "        \n",
    "        return self.tl_transformer(*args, **kwargs)\n",
    "\n",
    "        # go through all attention heads and multiply weights by partly-frozen masks\n",
    "        # go through all mlps and multiply weights by masks\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "weight_mask_mlps = {layer: False for layer in range(tl_model.cfg.n_layers)}\n",
    "for i in range(16):\n",
    "    weight_mask_mlps[i] = True\n",
    "\n",
    "weight_mask_attns = {layer: {\"W_Q\": [], \"W_K\": [], \"W_V\": [], \"W_O\": []} for layer in range(tl_model.cfg.n_layers)}\n",
    "for i in range(8, 24):\n",
    "    weight_mask_attns[i] = {\"W_Q\": list(range(4)), \"W_K\": list(range(4)), \"W_V\": list(range(4)), \"W_O\": list(range(4))}\n",
    "\n",
    "print(torch.cuda.memory_allocated() // 1024**3)\n",
    "wmt = WeightMaskedTransformer(tl_model, weight_mask_attn_dict=weight_mask_attns, weight_mask_mlp_dict=weight_mask_mlps)\n",
    "print(torch.cuda.memory_allocated() // 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]]], device='cuda:0'),\n",
       " tensor([[[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',\n",
       "        dtype=torch.bfloat16, requires_grad=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt.attention_masks[8]['W_Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2139, device='cuda:0')\n",
      "tensor(0.1470, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sports_test = SportsTask(batch_size=64, tokenizer=tokenizer)\n",
    "# print(sports_test.get_test_loss(tl_model))\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    print(sports_test.get_test_loss(tl_model))\n",
    "    print(sports_test.get_test_loss(wmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated() // 1024**3)\n",
    "print(torch.cuda.max_memory_allocated() // 1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that gradients flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1013, device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m sports_train\u001b[38;5;241m.\u001b[39mget_train_loss(wmt, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "sports_train = SportsTask(batch_size=8, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(wmt, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_model.cuda()\n",
    "for i in range(10):\n",
    "    generation = reference_model.generate(tokenizer(\"You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\\nQ: You know LeBron James? What does she do for a living?\\nA:\", return_tensors=\"pt\").input_ids.cuda(), max_new_tokens=20)\n",
    "    print(tokenizer.decode(generation[0]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
