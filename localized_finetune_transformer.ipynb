{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffa8656eb5242b99e8ba82c6bff8843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "model_type = \"gemma-7b\"\n",
    "num_heads = 16\n",
    "if model_type == \"pythia\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8B\")#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8B\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "elif model_type == \"gemma-7b\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.bfloat16)#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0053, -0.0036,  0.0004,  ...,  0.0041, -0.0002,  0.0018],\n",
       "        [-0.0014, -0.0024,  0.0043,  ...,  0.0015, -0.0006, -0.0063],\n",
       "        [ 0.0004, -0.0045, -0.0018,  ..., -0.0021,  0.0043, -0.0028],\n",
       "        ...,\n",
       "        [-0.0025,  0.0020,  0.0085,  ..., -0.0023, -0.0126,  0.0053],\n",
       "        [-0.0107,  0.0129, -0.0002,  ...,  0.0057, -0.0023, -0.0057],\n",
       "        [-0.0031,  0.0085, -0.0115,  ...,  0.0077,  0.0099, -0.0096]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model.model.layers[5].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaSdpaAttention(\n",
       "  (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "  (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "  (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "  (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "  (rotary_emb): GemmaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model.model.layers[5].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_localized_gradients(hf_model, attn_dict, mlp_dict, model_type=\"gemma\"):\n",
    "    # attn_dict is {layer: {\"W_Q\": [set of unlearn_heads], \"W_K\": [set of unlearn_heads], \"W_V\": [set of unlearn_heads], \"W_V\": [set of unlearn_heads]} for every layer}\n",
    "    # mlp_dict is {layer: boolean} for if you want to unlearn on this layer\n",
    "\n",
    "    # set everything else False\n",
    "    for parameter in hf_model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "\n",
    "    for layer in range(hf_model.config.num_hidden_layers):\n",
    "        if model_type == \"gemma\":\n",
    "            # set attn.W_Q layers requires_grad to True if W_Q unlearn heads is not empty, same for W_K, W_V, W_O\n",
    "\n",
    "            for attn_component_name, parameter in [(\"W_Q\", hf_model.model.layers[layer].self_attn.q_proj.weight), (\"W_K\", hf_model.model.layers[layer].self_attn.k_proj.weight), (\"W_V\", hf_model.model.layers[layer].self_attn.v_proj.weight), (\"W_O\", hf_model.model.layers[layer].self_attn.o_proj.weight)]:\n",
    "                if attn_dict is None or (layer in attn_dict and len(attn_dict[layer][attn_component_name]) > 0):\n",
    "                    parameter.requires_grad = True\n",
    "                else:\n",
    "                    parameter.requires_grad = False\n",
    "\n",
    "            if mlp_dict is None or (layer in mlp_dict and mlp_dict[layer]):\n",
    "                hf_model.model.layers[layer].mlp.up_proj.weight.requires_grad = True\n",
    "                hf_model.model.layers[layer].mlp.down_proj.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models/google_gemma-7b_sports_baseball_ap_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a0.0_q', 'a0.1_q', 'a0.2_q', 'a0.3_q', 'a0.4_q', 'a0.5_q', 'a0.6_q', 'a0.7_q', 'a0.8_q', 'a0.9_q', 'a0.10_q', 'a0.11_q', 'a0.12_q', 'a0.13_q', 'a0.14_q', 'a0.15_q', 'a0.0_k', 'a0.1_k', 'a0.2_k', 'a0.3_k', 'a0.4_k', 'a0.5_k', 'a0.6_k', 'a0.7_k', 'a0.8_k', 'a0.9_k', 'a0.10_k', 'a0.11_k', 'a0.12_k', 'a0.13_k', 'a0.14_k', 'a0.15_k', 'a0.0_v', 'a0.1_v', 'a0.2_v', 'a0.3_v', 'a0.4_v', 'a0.5_v', 'a0.6_v', 'a0.7_v', 'a0.8_v', 'a0.9_v', 'a0.10_v', 'a0.11_v', 'a0.12_v', 'a0.13_v', 'a0.14_v', 'a0.15_v', 'a0.0_result', 'a0.1_result', 'a0.2_result', 'a0.3_result', 'a0.4_result', 'a0.5_result', 'a0.6_result', 'a0.7_result', 'a0.8_result', 'a0.9_result', 'a0.10_result', 'a0.11_result', 'a0.12_result', 'a0.13_result', 'a0.14_result', 'a0.15_result', 'a1.0_q', 'a1.1_q', 'a1.2_q', 'a1.3_q', 'a1.4_q', 'a1.5_q', 'a1.6_q', 'a1.7_q', 'a1.8_q', 'a1.9_q', 'a1.10_q', 'a1.11_q', 'a1.12_q', 'a1.13_q', 'a1.14_q', 'a1.15_q', 'a1.0_k', 'a1.1_k', 'a1.2_k', 'a1.3_k', 'a1.4_k', 'a1.5_k', 'a1.6_k', 'a1.7_k', 'a1.8_k', 'a1.9_k', 'a1.10_k', 'a1.11_k', 'a1.12_k', 'a1.13_k', 'a1.14_k', 'a1.15_k', 'a1.0_v', 'a1.1_v', 'a1.2_v', 'a1.3_v', 'a1.4_v', 'a1.5_v', 'a1.6_v', 'a1.7_v', 'a1.8_v', 'a1.9_v', 'a1.10_v', 'a1.11_v', 'a1.12_v', 'a1.13_v', 'a1.14_v', 'a1.15_v', 'a1.0_result', 'a1.1_result', 'a1.2_result', 'a1.3_result', 'a1.4_result', 'a1.5_result', 'a1.6_result', 'a1.7_result', 'a1.8_result', 'a1.9_result', 'a1.10_result', 'a1.11_result', 'a1.12_result', 'a1.13_result', 'a1.14_result', 'a1.15_result', 'a2.0_q', 'a2.1_q', 'a2.2_q', 'a2.3_q', 'a2.4_q', 'a2.5_q', 'a2.6_q', 'a2.7_q', 'a2.8_q', 'a2.9_q', 'a2.10_q', 'a2.11_q', 'a2.12_q', 'a2.13_q', 'a2.14_q', 'a2.15_q', 'a2.0_k', 'a2.1_k', 'a2.2_k', 'a2.3_k', 'a2.4_k', 'a2.5_k', 'a2.6_k', 'a2.7_k', 'a2.8_k', 'a2.9_k', 'a2.10_k', 'a2.11_k', 'a2.12_k', 'a2.13_k', 'a2.14_k', 'a2.15_k', 'a2.0_v', 'a2.1_v', 'a2.2_v', 'a2.3_v', 'a2.4_v', 'a2.5_v', 'a2.6_v', 'a2.7_v', 'a2.8_v', 'a2.9_v', 'a2.10_v', 'a2.11_v', 'a2.12_v', 'a2.13_v', 'a2.14_v', 'a2.15_v', 'a2.0_result', 'a2.1_result', 'a2.2_result', 'a2.3_result', 'a2.4_result', 'a2.5_result', 'a2.6_result', 'a2.7_result', 'a2.8_result', 'a2.9_result', 'a2.10_result', 'a2.11_result', 'a2.12_result', 'a2.13_result', 'a2.14_result', 'a2.15_result', 'a3.0_q', 'a3.1_q', 'a3.2_q', 'a3.3_q', 'a3.4_q', 'a3.5_q', 'a3.6_q', 'a3.7_q', 'a3.8_q', 'a3.9_q', 'a3.10_q', 'a3.11_q', 'a3.12_q', 'a3.13_q', 'a3.14_q', 'a3.15_q', 'a3.0_k', 'a3.1_k', 'a3.2_k', 'a3.3_k', 'a3.4_k', 'a3.5_k', 'a3.6_k', 'a3.7_k', 'a3.8_k', 'a3.9_k', 'a3.10_k', 'a3.11_k', 'a3.12_k', 'a3.13_k', 'a3.14_k', 'a3.15_k', 'a3.0_v', 'a3.1_v', 'a3.2_v', 'a3.3_v', 'a3.4_v', 'a3.5_v', 'a3.6_v', 'a3.7_v', 'a3.8_v', 'a3.9_v', 'a3.10_v', 'a3.11_v', 'a3.12_v', 'a3.13_v', 'a3.14_v', 'a3.15_v', 'a3.0_result', 'a3.1_result', 'a3.2_result', 'a3.3_result', 'a3.4_result', 'a3.5_result', 'a3.6_result', 'a3.7_result', 'a3.8_result', 'a3.9_result', 'a3.10_result', 'a3.11_result', 'a3.12_result', 'a3.13_result', 'a3.14_result', 'a3.15_result', 'a4.0_q', 'a4.1_q', 'a4.2_q', 'a4.3_q', 'a4.4_q', 'a4.5_q', 'a4.6_q', 'a4.7_q', 'a4.8_q', 'a4.9_q', 'a4.10_q', 'a4.11_q', 'a4.12_q', 'a4.13_q', 'a4.14_q', 'a4.15_q', 'a4.0_k', 'a4.1_k', 'a4.2_k', 'a4.3_k', 'a4.4_k', 'a4.5_k', 'a4.6_k', 'a4.7_k', 'a4.8_k', 'a4.9_k', 'a4.10_k', 'a4.11_k', 'a4.12_k', 'a4.13_k', 'a4.14_k', 'a4.15_k', 'a4.0_v', 'a4.1_v', 'a4.2_v', 'a4.3_v', 'a4.4_v', 'a4.5_v', 'a4.6_v', 'a4.7_v', 'a4.8_v', 'a4.9_v', 'a4.10_v', 'a4.11_v', 'a4.12_v', 'a4.13_v', 'a4.14_v', 'a4.15_v', 'a4.0_result', 'a4.1_result', 'a4.2_result', 'a4.3_result', 'a4.4_result', 'a4.5_result', 'a4.6_result', 'a4.7_result', 'a4.8_result', 'a4.9_result', 'a4.10_result', 'a4.11_result', 'a4.12_result', 'a4.13_result', 'a4.14_result', 'a4.15_result', 'a5.0_q', 'a5.1_q', 'a5.2_q', 'a5.3_q', 'a5.4_q', 'a5.5_q', 'a5.6_q', 'a5.7_q', 'a5.8_q', 'a5.9_q', 'a5.10_q', 'a5.11_q', 'a5.12_q', 'a5.13_q', 'a5.14_q', 'a5.15_q', 'a5.0_k', 'a5.1_k', 'a5.2_k', 'a5.3_k', 'a5.4_k', 'a5.5_k', 'a5.6_k', 'a5.7_k', 'a5.8_k', 'a5.9_k', 'a5.10_k', 'a5.11_k', 'a5.12_k', 'a5.13_k', 'a5.14_k', 'a5.15_k', 'a5.0_v', 'a5.1_v', 'a5.2_v', 'a5.3_v', 'a5.4_v', 'a5.5_v', 'a5.6_v', 'a5.7_v', 'a5.8_v', 'a5.9_v', 'a5.10_v', 'a5.11_v', 'a5.12_v', 'a5.13_v', 'a5.14_v', 'a5.15_v', 'a5.0_result', 'a5.1_result', 'a5.2_result', 'a5.3_result', 'a5.4_result', 'a5.5_result', 'a5.6_result', 'a5.7_result', 'a5.8_result', 'a5.9_result', 'a5.10_result', 'a5.11_result', 'a5.12_result', 'a5.13_result', 'a5.14_result', 'a5.15_result', 'a6.0_q', 'a6.1_q', 'a6.2_q', 'a6.3_q', 'a6.4_q', 'a6.5_q', 'a6.6_q', 'a6.7_q', 'a6.8_q', 'a6.9_q', 'a6.10_q', 'a6.11_q', 'a6.12_q', 'a6.13_q', 'a6.14_q', 'a6.15_q', 'a6.0_k', 'a6.1_k', 'a6.2_k', 'a6.3_k', 'a6.4_k', 'a6.5_k', 'a6.6_k', 'a6.7_k', 'a6.8_k', 'a6.9_k', 'a6.10_k', 'a6.11_k', 'a6.12_k', 'a6.13_k', 'a6.14_k', 'a6.15_k', 'a6.0_v', 'a6.1_v', 'a6.2_v', 'a6.3_v', 'a6.4_v', 'a6.5_v', 'a6.6_v', 'a6.7_v', 'a6.8_v', 'a6.9_v', 'a6.10_v', 'a6.11_v', 'a6.12_v', 'a6.13_v', 'a6.14_v', 'a6.15_v', 'a6.0_result', 'a6.1_result', 'a6.2_result', 'a6.3_result', 'a6.4_result', 'a6.5_result', 'a6.6_result', 'a6.7_result', 'a6.8_result', 'a6.9_result', 'a6.10_result', 'a6.11_result', 'a6.12_result', 'a6.13_result', 'a6.14_result', 'a6.15_result', 'a7.0_q', 'a7.1_q', 'a7.2_q', 'a7.3_q', 'a7.4_q', 'a7.5_q', 'a7.6_q', 'a7.7_q', 'a7.8_q', 'a7.9_q', 'a7.10_q', 'a7.11_q', 'a7.12_q', 'a7.13_q', 'a7.14_q', 'a7.15_q', 'a7.0_k', 'a7.1_k', 'a7.2_k', 'a7.3_k', 'a7.4_k', 'a7.5_k', 'a7.6_k', 'a7.7_k', 'a7.8_k', 'a7.9_k', 'a7.10_k', 'a7.11_k', 'a7.12_k', 'a7.13_k', 'a7.14_k', 'a7.15_k', 'a7.0_v', 'a7.1_v', 'a7.2_v', 'a7.3_v', 'a7.4_v', 'a7.5_v', 'a7.6_v', 'a7.7_v', 'a7.8_v', 'a7.9_v', 'a7.10_v', 'a7.11_v', 'a7.12_v', 'a7.13_v', 'a7.14_v', 'a7.15_v', 'a7.0_result', 'a7.1_result', 'a7.2_result', 'a7.3_result', 'a7.4_result', 'a7.5_result', 'a7.6_result', 'a7.7_result', 'a7.8_result', 'a7.9_result', 'a7.10_result', 'a7.11_result', 'a7.12_result', 'a7.13_result', 'a7.14_result', 'a7.15_result', 'a8.0_q', 'a8.1_q', 'a8.2_q', 'a8.3_q', 'a8.4_q', 'a8.5_q', 'a8.6_q', 'a8.7_q', 'a8.8_q', 'a8.9_q', 'a8.10_q', 'a8.11_q', 'a8.12_q', 'a8.13_q', 'a8.14_q', 'a8.15_q', 'a8.0_k', 'a8.1_k', 'a8.2_k', 'a8.3_k', 'a8.4_k', 'a8.5_k', 'a8.6_k', 'a8.7_k', 'a8.8_k', 'a8.9_k', 'a8.10_k', 'a8.11_k', 'a8.12_k', 'a8.13_k', 'a8.14_k', 'a8.15_k', 'a8.0_v', 'a8.1_v', 'a8.2_v', 'a8.3_v', 'a8.4_v', 'a8.5_v', 'a8.6_v', 'a8.7_v', 'a8.8_v', 'a8.9_v', 'a8.10_v', 'a8.11_v', 'a8.12_v', 'a8.13_v', 'a8.14_v', 'a8.15_v', 'a8.0_result', 'a8.1_result', 'a8.2_result', 'a8.3_result', 'a8.4_result', 'a8.5_result', 'a8.6_result', 'a8.7_result', 'a8.8_result', 'a8.9_result', 'a8.10_result', 'a8.11_result', 'a8.12_result', 'a8.13_result', 'a8.14_result', 'a8.15_result', 'a9.0_q', 'a9.1_q', 'a9.2_q', 'a9.3_q', 'a9.4_q', 'a9.5_q', 'a9.6_q', 'a9.7_q', 'a9.8_q', 'a9.9_q', 'a9.10_q', 'a9.11_q', 'a9.12_q', 'a9.13_q', 'a9.14_q', 'a9.15_q', 'a9.0_k', 'a9.1_k', 'a9.2_k', 'a9.3_k', 'a9.4_k', 'a9.5_k', 'a9.6_k', 'a9.7_k', 'a9.8_k', 'a9.9_k', 'a9.10_k', 'a9.11_k', 'a9.12_k', 'a9.13_k', 'a9.14_k', 'a9.15_k', 'a9.0_v', 'a9.1_v', 'a9.2_v', 'a9.3_v', 'a9.4_v', 'a9.5_v', 'a9.6_v', 'a9.7_v', 'a9.8_v', 'a9.9_v', 'a9.10_v', 'a9.11_v', 'a9.12_v', 'a9.13_v', 'a9.14_v', 'a9.15_v', 'a9.0_result', 'a9.1_result', 'a9.2_result', 'a9.3_result', 'a9.4_result', 'a9.5_result', 'a9.6_result', 'a9.7_result', 'a9.8_result', 'a9.9_result', 'a9.10_result', 'a9.11_result', 'a9.12_result', 'a9.13_result', 'a9.14_result', 'a9.15_result', 'a10.0_q', 'a10.1_q', 'a10.2_q', 'a10.3_q', 'a10.4_q', 'a10.5_q', 'a10.6_q', 'a10.7_q', 'a10.8_q', 'a10.9_q', 'a10.10_q', 'a10.11_q', 'a10.12_q', 'a10.13_q', 'a10.14_q', 'a10.15_q', 'a10.0_k', 'a10.1_k', 'a10.2_k', 'a10.3_k', 'a10.4_k', 'a10.5_k', 'a10.6_k', 'a10.7_k', 'a10.8_k', 'a10.9_k', 'a10.10_k', 'a10.11_k', 'a10.12_k', 'a10.13_k', 'a10.14_k', 'a10.15_k', 'a10.0_v', 'a10.1_v', 'a10.2_v', 'a10.3_v', 'a10.4_v', 'a10.5_v', 'a10.6_v', 'a10.7_v', 'a10.8_v', 'a10.9_v', 'a10.10_v', 'a10.11_v', 'a10.12_v', 'a10.13_v', 'a10.14_v', 'a10.15_v', 'a10.0_result', 'a10.1_result', 'a10.2_result', 'a10.3_result', 'a10.4_result', 'a10.5_result', 'a10.6_result', 'a10.7_result', 'a10.8_result', 'a10.9_result', 'a10.10_result', 'a10.11_result', 'a10.12_result', 'a10.13_result', 'a10.14_result', 'a10.15_result', 'a11.0_q', 'a11.1_q', 'a11.2_q', 'a11.3_q', 'a11.4_q', 'a11.5_q', 'a11.6_q', 'a11.7_q', 'a11.8_q', 'a11.9_q', 'a11.10_q', 'a11.11_q', 'a11.12_q', 'a11.13_q', 'a11.14_q', 'a11.15_q', 'a11.0_k', 'a11.1_k', 'a11.2_k', 'a11.3_k', 'a11.4_k', 'a11.5_k', 'a11.6_k', 'a11.7_k', 'a11.8_k', 'a11.9_k', 'a11.10_k', 'a11.11_k', 'a11.12_k', 'a11.13_k', 'a11.14_k', 'a11.15_k', 'a11.0_v', 'a11.1_v', 'a11.2_v', 'a11.3_v', 'a11.4_v', 'a11.5_v', 'a11.6_v', 'a11.7_v', 'a11.8_v', 'a11.9_v', 'a11.10_v', 'a11.11_v', 'a11.12_v', 'a11.13_v', 'a11.14_v', 'a11.15_v', 'a11.0_result', 'a11.1_result', 'a11.2_result', 'a11.3_result', 'a11.4_result', 'a11.5_result', 'a11.6_result', 'a11.7_result', 'a11.8_result', 'a11.9_result', 'a11.10_result', 'a11.11_result', 'a11.12_result', 'a11.13_result', 'a11.14_result', 'a11.15_result', 'a12.0_q', 'a12.1_q', 'a12.2_q', 'a12.3_q', 'a12.4_q', 'a12.5_q', 'a12.6_q', 'a12.7_q', 'a12.8_q', 'a12.9_q', 'a12.10_q', 'a12.11_q', 'a12.12_q', 'a12.13_q', 'a12.14_q', 'a12.15_q', 'a12.0_k', 'a12.1_k', 'a12.2_k', 'a12.3_k', 'a12.4_k', 'a12.5_k', 'a12.6_k', 'a12.7_k', 'a12.8_k', 'a12.9_k', 'a12.10_k', 'a12.11_k', 'a12.12_k', 'a12.13_k', 'a12.14_k', 'a12.15_k', 'a12.0_v', 'a12.1_v', 'a12.2_v', 'a12.3_v', 'a12.4_v', 'a12.5_v', 'a12.6_v', 'a12.7_v', 'a12.8_v', 'a12.9_v', 'a12.10_v', 'a12.11_v', 'a12.12_v', 'a12.13_v', 'a12.14_v', 'a12.15_v', 'a12.0_result', 'a12.1_result', 'a12.2_result', 'a12.3_result', 'a12.4_result', 'a12.5_result', 'a12.6_result', 'a12.7_result', 'a12.8_result', 'a12.9_result', 'a12.10_result', 'a12.11_result', 'a12.12_result', 'a12.13_result', 'a12.14_result', 'a12.15_result', 'a13.0_q', 'a13.1_q', 'a13.2_q', 'a13.3_q', 'a13.4_q', 'a13.5_q', 'a13.6_q', 'a13.7_q', 'a13.8_q', 'a13.9_q', 'a13.10_q', 'a13.11_q', 'a13.12_q', 'a13.13_q', 'a13.14_q', 'a13.15_q', 'a13.0_k', 'a13.1_k', 'a13.2_k', 'a13.3_k', 'a13.4_k', 'a13.5_k', 'a13.6_k', 'a13.7_k', 'a13.8_k', 'a13.9_k', 'a13.10_k', 'a13.11_k', 'a13.12_k', 'a13.13_k', 'a13.14_k', 'a13.15_k', 'a13.0_v', 'a13.1_v', 'a13.2_v', 'a13.3_v', 'a13.4_v', 'a13.5_v', 'a13.6_v', 'a13.7_v', 'a13.8_v', 'a13.9_v', 'a13.10_v', 'a13.11_v', 'a13.12_v', 'a13.13_v', 'a13.14_v', 'a13.15_v', 'a13.0_result', 'a13.1_result', 'a13.2_result', 'a13.3_result', 'a13.4_result', 'a13.5_result', 'a13.6_result', 'a13.7_result', 'a13.8_result', 'a13.9_result', 'a13.10_result', 'a13.11_result', 'a13.12_result', 'a13.13_result', 'a13.14_result', 'a13.15_result', 'a14.0_q', 'a14.1_q', 'a14.2_q', 'a14.3_q', 'a14.4_q', 'a14.5_q', 'a14.6_q', 'a14.7_q', 'a14.8_q', 'a14.9_q', 'a14.10_q', 'a14.11_q', 'a14.12_q', 'a14.13_q', 'a14.14_q', 'a14.15_q', 'a14.0_k', 'a14.1_k', 'a14.2_k', 'a14.3_k', 'a14.4_k', 'a14.5_k', 'a14.6_k', 'a14.7_k', 'a14.8_k', 'a14.9_k', 'a14.10_k', 'a14.11_k', 'a14.12_k', 'a14.13_k', 'a14.14_k', 'a14.15_k', 'a14.0_v', 'a14.1_v', 'a14.2_v', 'a14.3_v', 'a14.4_v', 'a14.5_v', 'a14.6_v', 'a14.7_v', 'a14.8_v', 'a14.9_v', 'a14.10_v', 'a14.11_v', 'a14.12_v', 'a14.13_v', 'a14.14_v', 'a14.15_v', 'a14.0_result', 'a14.1_result', 'a14.2_result', 'a14.3_result', 'a14.4_result', 'a14.5_result', 'a14.6_result', 'a14.7_result', 'a14.8_result', 'a14.9_result', 'a14.10_result', 'a14.11_result', 'a14.12_result', 'a14.13_result', 'a14.14_result', 'a14.15_result', 'a15.0_q', 'a15.1_q', 'a15.2_q', 'a15.3_q', 'a15.4_q', 'a15.5_q', 'a15.6_q', 'a15.7_q', 'a15.8_q', 'a15.9_q', 'a15.10_q', 'a15.11_q', 'a15.12_q', 'a15.13_q', 'a15.14_q', 'a15.15_q', 'a15.0_k', 'a15.1_k', 'a15.2_k', 'a15.3_k', 'a15.4_k', 'a15.5_k', 'a15.6_k', 'a15.7_k', 'a15.8_k', 'a15.9_k', 'a15.10_k', 'a15.11_k', 'a15.12_k', 'a15.13_k', 'a15.14_k', 'a15.15_k', 'a15.0_v', 'a15.1_v', 'a15.2_v', 'a15.3_v', 'a15.4_v', 'a15.5_v', 'a15.6_v', 'a15.7_v', 'a15.8_v', 'a15.9_v', 'a15.10_v', 'a15.11_v', 'a15.12_v', 'a15.13_v', 'a15.14_v', 'a15.15_v', 'a15.0_result', 'a15.1_result', 'a15.2_result', 'a15.3_result', 'a15.4_result', 'a15.5_result', 'a15.6_result', 'a15.7_result', 'a15.8_result', 'a15.9_result', 'a15.10_result', 'a15.11_result', 'a15.12_result', 'a15.13_result', 'a15.14_result', 'a15.15_result', 'a16.0_q', 'a16.1_q', 'a16.2_q', 'a16.3_q', 'a16.4_q', 'a16.5_q', 'a16.6_q', 'a16.7_q', 'a16.8_q', 'a16.9_q', 'a16.10_q', 'a16.11_q', 'a16.12_q', 'a16.13_q', 'a16.14_q', 'a16.15_q', 'a16.0_k', 'a16.1_k', 'a16.2_k', 'a16.3_k', 'a16.4_k', 'a16.5_k', 'a16.6_k', 'a16.7_k', 'a16.8_k', 'a16.9_k', 'a16.10_k', 'a16.11_k', 'a16.12_k', 'a16.13_k', 'a16.14_k', 'a16.15_k', 'a16.0_v', 'a16.1_v', 'a16.2_v', 'a16.3_v', 'a16.4_v', 'a16.5_v', 'a16.6_v', 'a16.7_v', 'a16.8_v', 'a16.9_v', 'a16.10_v', 'a16.11_v', 'a16.12_v', 'a16.13_v', 'a16.14_v', 'a16.15_v', 'a16.0_result', 'a16.1_result', 'a16.2_result', 'a16.3_result', 'a16.4_result', 'a16.5_result', 'a16.6_result', 'a16.7_result', 'a16.8_result', 'a16.9_result', 'a16.10_result', 'a16.11_result', 'a16.12_result', 'a16.13_result', 'a16.14_result', 'a16.15_result', 'a17.0_q', 'a17.1_q', 'a17.2_q', 'a17.3_q', 'a17.4_q', 'a17.5_q', 'a17.6_q', 'a17.7_q', 'a17.8_q', 'a17.9_q', 'a17.10_q', 'a17.11_q', 'a17.12_q', 'a17.13_q', 'a17.14_q', 'a17.15_q', 'a17.0_k', 'a17.1_k', 'a17.2_k', 'a17.3_k', 'a17.4_k', 'a17.5_k', 'a17.6_k', 'a17.7_k', 'a17.8_k', 'a17.9_k', 'a17.10_k', 'a17.11_k', 'a17.12_k', 'a17.13_k', 'a17.14_k', 'a17.15_k', 'a17.0_v', 'a17.1_v', 'a17.2_v', 'a17.3_v', 'a17.4_v', 'a17.5_v', 'a17.6_v', 'a17.7_v', 'a17.8_v', 'a17.9_v', 'a17.10_v', 'a17.11_v', 'a17.12_v', 'a17.13_v', 'a17.14_v', 'a17.15_v', 'a17.0_result', 'a17.1_result', 'a17.2_result', 'a17.3_result', 'a17.4_result', 'a17.5_result', 'a17.6_result', 'a17.7_result', 'a17.8_result', 'a17.9_result', 'a17.10_result', 'a17.11_result', 'a17.12_result', 'a17.13_result', 'a17.14_result', 'a17.15_result', 'a18.0_q', 'a18.1_q', 'a18.2_q', 'a18.3_q', 'a18.4_q', 'a18.5_q', 'a18.6_q', 'a18.7_q', 'a18.8_q', 'a18.9_q', 'a18.10_q', 'a18.11_q', 'a18.12_q', 'a18.13_q', 'a18.14_q', 'a18.15_q', 'a18.0_k', 'a18.1_k', 'a18.2_k', 'a18.3_k', 'a18.4_k', 'a18.5_k', 'a18.6_k', 'a18.7_k', 'a18.8_k', 'a18.9_k', 'a18.10_k', 'a18.11_k', 'a18.12_k', 'a18.13_k', 'a18.14_k', 'a18.15_k', 'a18.0_v', 'a18.1_v', 'a18.2_v', 'a18.3_v', 'a18.4_v', 'a18.5_v', 'a18.6_v', 'a18.7_v', 'a18.8_v', 'a18.9_v', 'a18.10_v', 'a18.11_v', 'a18.12_v', 'a18.13_v', 'a18.14_v', 'a18.15_v', 'a18.0_result', 'a18.1_result', 'a18.2_result', 'a18.3_result', 'a18.4_result', 'a18.5_result', 'a18.6_result', 'a18.7_result', 'a18.8_result', 'a18.9_result', 'a18.10_result', 'a18.11_result', 'a18.12_result', 'a18.13_result', 'a18.14_result', 'a18.15_result', 'a19.0_q', 'a19.1_q', 'a19.2_q', 'a19.3_q', 'a19.4_q', 'a19.5_q', 'a19.6_q', 'a19.7_q', 'a19.8_q', 'a19.9_q', 'a19.10_q', 'a19.11_q', 'a19.12_q', 'a19.13_q', 'a19.14_q', 'a19.15_q', 'a19.0_k', 'a19.1_k', 'a19.2_k', 'a19.3_k', 'a19.4_k', 'a19.5_k', 'a19.6_k', 'a19.7_k', 'a19.8_k', 'a19.9_k', 'a19.10_k', 'a19.11_k', 'a19.12_k', 'a19.13_k', 'a19.14_k', 'a19.15_k', 'a19.0_v', 'a19.1_v', 'a19.2_v', 'a19.3_v', 'a19.4_v', 'a19.5_v', 'a19.6_v', 'a19.7_v', 'a19.8_v', 'a19.9_v', 'a19.10_v', 'a19.11_v', 'a19.12_v', 'a19.13_v', 'a19.14_v', 'a19.15_v', 'a19.0_result', 'a19.1_result', 'a19.2_result', 'a19.3_result', 'a19.4_result', 'a19.5_result', 'a19.6_result', 'a19.7_result', 'a19.8_result', 'a19.9_result', 'a19.10_result', 'a19.11_result', 'a19.12_result', 'a19.13_result', 'a19.14_result', 'a19.15_result', 'a20.0_q', 'a20.1_q', 'a20.2_q', 'a20.3_q', 'a20.4_q', 'a20.5_q', 'a20.6_q', 'a20.7_q', 'a20.8_q', 'a20.9_q', 'a20.10_q', 'a20.11_q', 'a20.12_q', 'a20.13_q', 'a20.14_q', 'a20.15_q', 'a20.0_k', 'a20.1_k', 'a20.2_k', 'a20.3_k', 'a20.4_k', 'a20.5_k', 'a20.6_k', 'a20.7_k', 'a20.8_k', 'a20.9_k', 'a20.10_k', 'a20.11_k', 'a20.12_k', 'a20.13_k', 'a20.14_k', 'a20.15_k', 'a20.0_v', 'a20.1_v', 'a20.2_v', 'a20.3_v', 'a20.4_v', 'a20.5_v', 'a20.6_v', 'a20.7_v', 'a20.8_v', 'a20.9_v', 'a20.10_v', 'a20.11_v', 'a20.12_v', 'a20.13_v', 'a20.14_v', 'a20.15_v', 'a20.0_result', 'a20.1_result', 'a20.2_result', 'a20.3_result', 'a20.4_result', 'a20.5_result', 'a20.6_result', 'a20.7_result', 'a20.8_result', 'a20.9_result', 'a20.10_result', 'a20.11_result', 'a20.12_result', 'a20.13_result', 'a20.14_result', 'a20.15_result', 'a21.0_q', 'a21.1_q', 'a21.2_q', 'a21.3_q', 'a21.4_q', 'a21.5_q', 'a21.6_q', 'a21.7_q', 'a21.8_q', 'a21.9_q', 'a21.10_q', 'a21.11_q', 'a21.12_q', 'a21.13_q', 'a21.14_q', 'a21.15_q', 'a21.0_k', 'a21.1_k', 'a21.2_k', 'a21.3_k', 'a21.4_k', 'a21.5_k', 'a21.6_k', 'a21.7_k', 'a21.8_k', 'a21.9_k', 'a21.10_k', 'a21.11_k', 'a21.12_k', 'a21.13_k', 'a21.14_k', 'a21.15_k', 'a21.0_v', 'a21.1_v', 'a21.2_v', 'a21.3_v', 'a21.4_v', 'a21.5_v', 'a21.6_v', 'a21.7_v', 'a21.8_v', 'a21.9_v', 'a21.10_v', 'a21.11_v', 'a21.12_v', 'a21.13_v', 'a21.14_v', 'a21.15_v', 'a21.0_result', 'a21.1_result', 'a21.2_result', 'a21.3_result', 'a21.4_result', 'a21.5_result', 'a21.6_result', 'a21.7_result', 'a21.8_result', 'a21.9_result', 'a21.10_result', 'a21.11_result', 'a21.12_result', 'a21.13_result', 'a21.14_result', 'a21.15_result', 'a22.0_q', 'a22.1_q', 'a22.2_q', 'a22.3_q', 'a22.4_q', 'a22.5_q', 'a22.6_q', 'a22.7_q', 'a22.8_q', 'a22.9_q', 'a22.10_q', 'a22.11_q', 'a22.12_q', 'a22.13_q', 'a22.14_q', 'a22.15_q', 'a22.0_k', 'a22.1_k', 'a22.2_k', 'a22.3_k', 'a22.4_k', 'a22.5_k', 'a22.6_k', 'a22.7_k', 'a22.8_k', 'a22.9_k', 'a22.10_k', 'a22.11_k', 'a22.12_k', 'a22.13_k', 'a22.14_k', 'a22.15_k', 'a22.0_v', 'a22.1_v', 'a22.2_v', 'a22.3_v', 'a22.4_v', 'a22.5_v', 'a22.6_v', 'a22.7_v', 'a22.8_v', 'a22.9_v', 'a22.10_v', 'a22.11_v', 'a22.12_v', 'a22.13_v', 'a22.14_v', 'a22.15_v', 'a22.0_result', 'a22.1_result', 'a22.2_result', 'a22.3_result', 'a22.4_result', 'a22.5_result', 'a22.6_result', 'a22.7_result', 'a22.8_result', 'a22.9_result', 'a22.10_result', 'a22.11_result', 'a22.12_result', 'a22.13_result', 'a22.14_result', 'a22.15_result', 'a23.0_q', 'a23.1_q', 'a23.2_q', 'a23.3_q', 'a23.4_q', 'a23.5_q', 'a23.6_q', 'a23.7_q', 'a23.8_q', 'a23.9_q', 'a23.10_q', 'a23.11_q', 'a23.12_q', 'a23.13_q', 'a23.14_q', 'a23.15_q', 'a23.0_k', 'a23.1_k', 'a23.2_k', 'a23.3_k', 'a23.4_k', 'a23.5_k', 'a23.6_k', 'a23.7_k', 'a23.8_k', 'a23.9_k', 'a23.10_k', 'a23.11_k', 'a23.12_k', 'a23.13_k', 'a23.14_k', 'a23.15_k', 'a23.0_v', 'a23.1_v', 'a23.2_v', 'a23.3_v', 'a23.4_v', 'a23.5_v', 'a23.6_v', 'a23.7_v', 'a23.8_v', 'a23.9_v', 'a23.10_v', 'a23.11_v', 'a23.12_v', 'a23.13_v', 'a23.14_v', 'a23.15_v', 'a23.0_result', 'a23.1_result', 'a23.2_result', 'a23.3_result', 'a23.4_result', 'a23.5_result', 'a23.6_result', 'a23.7_result', 'a23.8_result', 'a23.9_result', 'a23.10_result', 'a23.11_result', 'a23.12_result', 'a23.13_result', 'a23.14_result', 'a23.15_result', 'a24.0_q', 'a24.1_q', 'a24.2_q', 'a24.3_q', 'a24.4_q', 'a24.5_q', 'a24.6_q', 'a24.7_q', 'a24.8_q', 'a24.9_q', 'a24.10_q', 'a24.11_q', 'a24.12_q', 'a24.13_q', 'a24.14_q', 'a24.15_q', 'a24.0_k', 'a24.1_k', 'a24.2_k', 'a24.3_k', 'a24.4_k', 'a24.5_k', 'a24.6_k', 'a24.7_k', 'a24.8_k', 'a24.9_k', 'a24.10_k', 'a24.11_k', 'a24.12_k', 'a24.13_k', 'a24.14_k', 'a24.15_k', 'a24.0_v', 'a24.1_v', 'a24.2_v', 'a24.3_v', 'a24.4_v', 'a24.5_v', 'a24.6_v', 'a24.7_v', 'a24.8_v', 'a24.9_v', 'a24.10_v', 'a24.11_v', 'a24.12_v', 'a24.13_v', 'a24.14_v', 'a24.15_v', 'a24.0_result', 'a24.1_result', 'a24.2_result', 'a24.3_result', 'a24.4_result', 'a24.5_result', 'a24.6_result', 'a24.7_result', 'a24.8_result', 'a24.9_result', 'a24.10_result', 'a24.11_result', 'a24.12_result', 'a24.13_result', 'a24.14_result', 'a24.15_result', 'a25.0_q', 'a25.1_q', 'a25.2_q', 'a25.3_q', 'a25.4_q', 'a25.5_q', 'a25.6_q', 'a25.7_q', 'a25.8_q', 'a25.9_q', 'a25.10_q', 'a25.11_q', 'a25.12_q', 'a25.13_q', 'a25.14_q', 'a25.15_q', 'a25.0_k', 'a25.1_k', 'a25.2_k', 'a25.3_k', 'a25.4_k', 'a25.5_k', 'a25.6_k', 'a25.7_k', 'a25.8_k', 'a25.9_k', 'a25.10_k', 'a25.11_k', 'a25.12_k', 'a25.13_k', 'a25.14_k', 'a25.15_k', 'a25.0_v', 'a25.1_v', 'a25.2_v', 'a25.3_v', 'a25.4_v', 'a25.5_v', 'a25.6_v', 'a25.7_v', 'a25.8_v', 'a25.9_v', 'a25.10_v', 'a25.11_v', 'a25.12_v', 'a25.13_v', 'a25.14_v', 'a25.15_v', 'a25.0_result', 'a25.1_result', 'a25.2_result', 'a25.3_result', 'a25.4_result', 'a25.5_result', 'a25.6_result', 'a25.7_result', 'a25.8_result', 'a25.9_result', 'a25.10_result', 'a25.11_result', 'a25.12_result', 'a25.13_result', 'a25.14_result', 'a25.15_result', 'a26.0_q', 'a26.1_q', 'a26.2_q', 'a26.3_q', 'a26.4_q', 'a26.5_q', 'a26.6_q', 'a26.7_q', 'a26.8_q', 'a26.9_q', 'a26.10_q', 'a26.11_q', 'a26.12_q', 'a26.13_q', 'a26.14_q', 'a26.15_q', 'a26.0_k', 'a26.1_k', 'a26.2_k', 'a26.3_k', 'a26.4_k', 'a26.5_k', 'a26.6_k', 'a26.7_k', 'a26.8_k', 'a26.9_k', 'a26.10_k', 'a26.11_k', 'a26.12_k', 'a26.13_k', 'a26.14_k', 'a26.15_k', 'a26.0_v', 'a26.1_v', 'a26.2_v', 'a26.3_v', 'a26.4_v', 'a26.5_v', 'a26.6_v', 'a26.7_v', 'a26.8_v', 'a26.9_v', 'a26.10_v', 'a26.11_v', 'a26.12_v', 'a26.13_v', 'a26.14_v', 'a26.15_v', 'a26.0_result', 'a26.1_result', 'a26.2_result', 'a26.3_result', 'a26.4_result', 'a26.5_result', 'a26.6_result', 'a26.7_result', 'a26.8_result', 'a26.9_result', 'a26.10_result', 'a26.11_result', 'a26.12_result', 'a26.13_result', 'a26.14_result', 'a26.15_result', 'a27.0_q', 'a27.1_q', 'a27.2_q', 'a27.3_q', 'a27.4_q', 'a27.5_q', 'a27.6_q', 'a27.7_q', 'a27.8_q', 'a27.9_q', 'a27.10_q', 'a27.11_q', 'a27.12_q', 'a27.13_q', 'a27.14_q', 'a27.15_q', 'a27.0_k', 'a27.1_k', 'a27.2_k', 'a27.3_k', 'a27.4_k', 'a27.5_k', 'a27.6_k', 'a27.7_k', 'a27.8_k', 'a27.9_k', 'a27.10_k', 'a27.11_k', 'a27.12_k', 'a27.13_k', 'a27.14_k', 'a27.15_k', 'a27.0_v', 'a27.1_v', 'a27.2_v', 'a27.3_v', 'a27.4_v', 'a27.5_v', 'a27.6_v', 'a27.7_v', 'a27.8_v', 'a27.9_v', 'a27.10_v', 'a27.11_v', 'a27.12_v', 'a27.13_v', 'a27.14_v', 'a27.15_v', 'a27.0_result', 'a27.1_result', 'a27.2_result', 'a27.3_result', 'a27.4_result', 'a27.5_result', 'a27.6_result', 'a27.7_result', 'a27.8_result', 'a27.9_result', 'a27.10_result', 'a27.11_result', 'a27.12_result', 'a27.13_result', 'a27.14_result', 'a27.15_result', 'm0_in', 'm0_out', 'm1_in', 'm1_out', 'm2_in', 'm2_out', 'm3_in', 'm3_out', 'm4_in', 'm4_out', 'm5_in', 'm5_out', 'm6_in', 'm6_out', 'm7_in', 'm7_out', 'm8_in', 'm8_out', 'm9_in', 'm9_out', 'm10_in', 'm10_out', 'm11_in', 'm11_out', 'm12_in', 'm12_out', 'm13_in', 'm13_out', 'm14_in', 'm14_out', 'm15_in', 'm15_out', 'm16_in', 'm16_out', 'm17_in', 'm17_out', 'm18_in', 'm18_out', 'm19_in', 'm19_out', 'm20_in', 'm20_out', 'm21_in', 'm21_out', 'm22_in', 'm22_out', 'm23_in', 'm23_out', 'm24_in', 'm24_out', 'm25_in', 'm25_out', 'm26_in', 'm26_out', 'm27_in', 'm27_out'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_graph.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a0': 0.009907666575880183, 'm0': 0.006218543419471152, 'a1': -0.12394205881999085, 'm1': 0.11150418795072115, 'a2': -0.0677991177027042, 'm2': -0.15252333420973557, 'a3': 0.02044301021557587, 'm3': 0.08830495981069711, 'a4': 0.0423168413914167, 'm4': 0.07103553185096154, 'a5': -0.00669221121531267, 'm5': -0.07313126784104568, 'a6': -0.0001643827328315032, 'm6': -0.004096397986778846, 'a7': -0.002629542580017686, 'm7': -0.037644606370192304, 'a8': 0.09417960276970498, 'm8': 0.02627196678748498, 'a9': 0.022287575671306025, 'm9': -0.023734459510216344, 'a10': -0.08142007772739117, 'm10': -0.015042818509615384, 'a11': 0.0034291302928557803, 'm11': -0.07435960036057693, 'a12': -0.017220222032987155, 'm12': -0.032728928786057696, 'a13': -0.023599241788570695, 'm13': 0.12712684044471154, 'a14': -0.0676610836615929, 'm14': 0.11145401000976562, 'a15': -0.09139613463328432, 'm15': 0.08517221304086539, 'a16': -0.0753518228347485, 'm16': 0.052490234375, 'a17': -0.09901340191180892, 'm17': -0.014054518479567304, 'a18': -0.22854862763331488, 'm18': -0.15277803861177885, 'a19': -0.06522605740107024, 'm19': 0.07455679086538462, 'a20': -0.1765875793420351, 'm20': -0.14551720252403846, 'a21': -0.13378601492597506, 'm21': -0.5181039663461539, 'a22': -0.17166384839667723, 'm22': -0.18837796724759615, 'a23': -0.029047957406594202, 'm23': -0.39344200721153844, 'a24': -0.060470273861518264, 'm24': -0.21610201322115385, 'a25': -0.19385868196304032, 'm25': -0.24590594951923078, 'a26': -0.1283126333907533, 'm26': 0.3184532752403846, 'a27': -0.4708470217994828, 'm27': 0.573699951171875}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({27: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  18: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  25: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  20: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  22: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  21: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  26: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},\n",
       "  1: {'W_Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_K': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_V': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'W_O': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}},\n",
       " {27: True,\n",
       "  21: True,\n",
       "  23: True,\n",
       "  26: True,\n",
       "  25: True,\n",
       "  24: True,\n",
       "  22: True,\n",
       "  18: True,\n",
       "  2: True,\n",
       "  20: True,\n",
       "  13: True,\n",
       "  1: True})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add up attributions across attentions\n",
    "aggregated_attributions = {}\n",
    "for layer in range(reference_model.config.num_hidden_layers):\n",
    "    component_name = f'a{layer}'\n",
    "    aggregated_attributions[component_name] = 0\n",
    "    for head in range(num_heads):\n",
    "        for head_type in [\"q\", \"k\", \"v\"]:\n",
    "            head_name = f\"{component_name}.{head}_{head_type}\"\n",
    "            aggregated_attributions[component_name] += ap_graph[head_name]\n",
    "        # head_name = f\"{component_name}.{head}\"\n",
    "        # aggregated_attributions[component_name] += ap_graph[head_name]\n",
    "    aggregated_attributions[f'm{layer}'] = 0\n",
    "    for mlp_type in [\"in\", \"out\"]:\n",
    "        mlp_name = f'm{layer}_{mlp_type}'\n",
    "        aggregated_attributions[f\"m{layer}\"] += ap_graph[mlp_name]\n",
    "\n",
    "print(aggregated_attributions)\n",
    "\n",
    "num_components=20\n",
    "top_components = {}\n",
    "# take the top 20 components from aggregated_attributions (20 highest absolute values)\n",
    "for i in range(num_components):\n",
    "    max_key = max(aggregated_attributions, key=lambda x: abs(aggregated_attributions[x]))\n",
    "    top_components[max_key] = aggregated_attributions[max_key]\n",
    "    del aggregated_attributions[max_key]\n",
    "\n",
    "def get_dicts_from_nodes(nodes_set):\n",
    "    # get attn_dict and mlp_dict\n",
    "    attn_dict = {}\n",
    "    mlp_dict = {}\n",
    "    for node in nodes_set:\n",
    "        if node[0] == \"a\":\n",
    "            layer = int(node[1:])\n",
    "            attn_dict[layer] = {\"W_Q\": list(range(num_heads)), \"W_K\": list(range(num_heads)), \"W_V\": list(range(num_heads)), \"W_O\": list(range(num_heads))}\n",
    "        elif node[0] == \"m\":\n",
    "            layer = int(node[1:])\n",
    "            mlp_dict[layer] = True\n",
    "    return attn_dict, mlp_dict\n",
    "\n",
    "attn_dict, mlp_dict = get_dicts_from_nodes(top_components.keys())\n",
    "attn_dict, mlp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_localized_gradients(reference_model, attn_dict, mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.layers.12.self_attn.q_proj.weight False\n",
      "model.layers.12.self_attn.k_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.weight False\n",
      "model.layers.12.self_attn.o_proj.weight False\n",
      "model.layers.12.mlp.gate_proj.weight False\n",
      "model.layers.12.mlp.up_proj.weight False\n",
      "model.layers.12.mlp.down_proj.weight False\n",
      "model.layers.12.input_layernorm.weight False\n",
      "model.layers.12.post_attention_layernorm.weight False\n",
      "model.layers.13.self_attn.q_proj.weight False\n",
      "model.layers.13.self_attn.k_proj.weight False\n",
      "model.layers.13.self_attn.v_proj.weight False\n",
      "model.layers.13.self_attn.o_proj.weight False\n",
      "model.layers.13.mlp.gate_proj.weight False\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight False\n",
      "model.layers.13.post_attention_layernorm.weight False\n",
      "model.layers.14.self_attn.q_proj.weight False\n",
      "model.layers.14.self_attn.k_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.weight False\n",
      "model.layers.14.self_attn.o_proj.weight False\n",
      "model.layers.14.mlp.gate_proj.weight False\n",
      "model.layers.14.mlp.up_proj.weight False\n",
      "model.layers.14.mlp.down_proj.weight False\n",
      "model.layers.14.input_layernorm.weight False\n",
      "model.layers.14.post_attention_layernorm.weight False\n",
      "model.layers.15.self_attn.q_proj.weight False\n",
      "model.layers.15.self_attn.k_proj.weight False\n",
      "model.layers.15.self_attn.v_proj.weight False\n",
      "model.layers.15.self_attn.o_proj.weight False\n",
      "model.layers.15.mlp.gate_proj.weight False\n",
      "model.layers.15.mlp.up_proj.weight False\n",
      "model.layers.15.mlp.down_proj.weight False\n",
      "model.layers.15.input_layernorm.weight False\n",
      "model.layers.15.post_attention_layernorm.weight False\n",
      "model.layers.16.self_attn.q_proj.weight False\n",
      "model.layers.16.self_attn.k_proj.weight False\n",
      "model.layers.16.self_attn.v_proj.weight False\n",
      "model.layers.16.self_attn.o_proj.weight False\n",
      "model.layers.16.mlp.gate_proj.weight False\n",
      "model.layers.16.mlp.up_proj.weight False\n",
      "model.layers.16.mlp.down_proj.weight False\n",
      "model.layers.16.input_layernorm.weight False\n",
      "model.layers.16.post_attention_layernorm.weight False\n",
      "model.layers.17.self_attn.q_proj.weight False\n",
      "model.layers.17.self_attn.k_proj.weight False\n",
      "model.layers.17.self_attn.v_proj.weight False\n",
      "model.layers.17.self_attn.o_proj.weight False\n",
      "model.layers.17.mlp.gate_proj.weight False\n",
      "model.layers.17.mlp.up_proj.weight False\n",
      "model.layers.17.mlp.down_proj.weight False\n",
      "model.layers.17.input_layernorm.weight False\n",
      "model.layers.17.post_attention_layernorm.weight False\n",
      "model.layers.18.self_attn.q_proj.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight False\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight False\n",
      "model.layers.18.post_attention_layernorm.weight False\n",
      "model.layers.19.self_attn.q_proj.weight False\n",
      "model.layers.19.self_attn.k_proj.weight False\n",
      "model.layers.19.self_attn.v_proj.weight False\n",
      "model.layers.19.self_attn.o_proj.weight False\n",
      "model.layers.19.mlp.gate_proj.weight False\n",
      "model.layers.19.mlp.up_proj.weight False\n",
      "model.layers.19.mlp.down_proj.weight False\n",
      "model.layers.19.input_layernorm.weight False\n",
      "model.layers.19.post_attention_layernorm.weight False\n",
      "model.layers.20.self_attn.q_proj.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight False\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight False\n",
      "model.layers.20.post_attention_layernorm.weight False\n",
      "model.layers.21.self_attn.q_proj.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight False\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight False\n",
      "model.layers.21.post_attention_layernorm.weight False\n",
      "model.layers.22.self_attn.q_proj.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.weight True\n",
      "model.layers.22.self_attn.o_proj.weight True\n",
      "model.layers.22.mlp.gate_proj.weight False\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight False\n",
      "model.layers.22.post_attention_layernorm.weight False\n",
      "model.layers.23.self_attn.q_proj.weight False\n",
      "model.layers.23.self_attn.k_proj.weight False\n",
      "model.layers.23.self_attn.v_proj.weight False\n",
      "model.layers.23.self_attn.o_proj.weight False\n",
      "model.layers.23.mlp.gate_proj.weight False\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight False\n",
      "model.layers.23.post_attention_layernorm.weight False\n",
      "model.layers.24.self_attn.q_proj.weight False\n",
      "model.layers.24.self_attn.k_proj.weight False\n",
      "model.layers.24.self_attn.v_proj.weight False\n",
      "model.layers.24.self_attn.o_proj.weight False\n",
      "model.layers.24.mlp.gate_proj.weight False\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight False\n",
      "model.layers.24.post_attention_layernorm.weight False\n",
      "model.layers.25.self_attn.q_proj.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.weight True\n",
      "model.layers.25.self_attn.o_proj.weight True\n",
      "model.layers.25.mlp.gate_proj.weight False\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight False\n",
      "model.layers.25.post_attention_layernorm.weight False\n",
      "model.layers.26.self_attn.q_proj.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.weight True\n",
      "model.layers.26.self_attn.o_proj.weight True\n",
      "model.layers.26.mlp.gate_proj.weight False\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight False\n",
      "model.layers.26.post_attention_layernorm.weight False\n",
      "model.layers.27.self_attn.q_proj.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.weight True\n",
      "model.layers.27.self_attn.o_proj.weight True\n",
      "model.layers.27.mlp.gate_proj.weight False\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight False\n",
      "model.layers.27.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in reference_model.named_parameters():\n",
    "    print(name, parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0908, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_test = SportsTask(batch_size=16, tokenizer=tokenizer)\n",
    "sports_test.get_test_loss(tl_model)\n",
    "\n",
    "# for layer in range(tl_model.cfg.n_layers):\n",
    "#     tl_model.blocks[layer].attn.W_Q.data = torch.zeros_like(tl_model.blocks[layer].attn.W_Q)\n",
    "#     tl_model.blocks[layer].attn.W_K.data = torch.zeros_like(tl_model.blocks[layer].attn.W_K)\n",
    "#     tl_model.blocks[layer].attn.W_V.data = torch.zeros_like(tl_model.blocks[layer].attn.W_V)\n",
    "#     tl_model.blocks[layer].attn.W_O.data = torch.zeros_like(tl_model.blocks[layer].attn.W_O)\n",
    "\n",
    "# sports_test.get_test_loss(tl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3072, 256])\n",
      "torch.Size([16, 256, 3072])\n",
      "torch.Size([3072, 24576])\n",
      "torch.Size([24576, 3072])\n"
     ]
    }
   ],
   "source": [
    "print(tl_model.blocks[27].attn.W_K.shape)\n",
    "print(tl_model.blocks[27].attn.W_O.shape)\n",
    "print(tl_model.blocks[27].mlp.W_in.shape)\n",
    "print(tl_model.blocks[27].mlp.W_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m0: -0.055645283311605453\n",
      "m1: 0.0552063025534153\n",
      "m2: -0.11303359270095825\n",
      "m3: 0.024208657443523407\n",
      "m4: -0.0113525390625\n",
      "m5: -0.022718576714396477\n",
      "m6: -0.007042518351227045\n",
      "m7: -0.021432731300592422\n",
      "m8: -0.006188026163727045\n",
      "m9: -0.0019231943879276514\n",
      "m10: -0.03130634129047394\n",
      "m11: -0.0708770751953125\n",
      "m12: -0.04879526048898697\n",
      "m13: 0.04687969759106636\n",
      "m14: 0.035638369619846344\n",
      "m15: 0.02344219572842121\n",
      "m16: -0.018733099102973938\n",
      "m17: -0.06601186841726303\n",
      "m18: -0.10868014395236969\n",
      "m19: -0.0049954927526414394\n",
      "m20: -0.08375901728868484\n",
      "m21: -0.25811299681663513\n",
      "m22: -0.11271785199642181\n",
      "m23: -0.23159556090831757\n",
      "m24: -0.09144005924463272\n",
      "m25: -0.08293269574642181\n",
      "m26: 0.18458910286426544\n",
      "m27: 0.5513822436332703\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"models/google_gemma-7b_sports_baseball_ap_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)\n",
    "for component in ap_graph:\n",
    "    if \"m\" in component:\n",
    "        print(f\"{component}: {ap_graph[component]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def make_partly_differentiable_mask(W, unfrozen_heads, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    W is Parameter of shape (n_heads, ...). Returns baseline and frozen (both only 1d arrays of (n_heads,)), and forward pass should be W_baseline.float() + W_frozen.float() * W \n",
    "    \"\"\"\n",
    "    W_baseline = torch.nn.Parameter(torch.zeros(W.shape[0], dtype=torch.bool), requires_grad=False).to(device)\n",
    "\n",
    "    # unsqueeze to broadcast efficiently, until W_baseline has same shape as W\n",
    "    while len(W_baseline.shape) < len(W.shape):\n",
    "        W_baseline = W_baseline.unsqueeze(-1)\n",
    "    \n",
    "    W_baseline[unfrozen_heads] = True\n",
    "    # W_baseline = ~W_frozen\n",
    "    W_frozen = torch.nn.Parameter(~W_baseline, requires_grad=False)\n",
    "    # convert into float\n",
    "    return W_frozen.float(), W_baseline.float()\n",
    "\n",
    "class WeightMaskedTransformer(nn.Module):\n",
    "    def __init__(self, tl_transformer, weight_mask_attn_dict=None, weight_mask_mlp_dict=None, torch_dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        weight_mask_attn_dict: {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}} (frozen_heads is shape (n_heads,) of bools). If none, train mask over all heads\n",
    "        weight_mask_mlp_dict: {layer: bool}. If none, train mask over all mlps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.torch_dtype = torch_dtype\n",
    "        # tl_transformer should be a HookedTransformer\n",
    "        self.tl_transformer = tl_transformer\n",
    "        # turn off gradients for tl_transformer\n",
    "        for param in self.tl_transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.weight_mask_attn_dict = weight_mask_attn_dict\n",
    "        self.weight_mask_mlp_dict = weight_mask_mlp_dict\n",
    "        # store weight masks for every component that is unfrozen\n",
    "        \n",
    "        # need to store reference weights so that you can reset W_Q, etc after a forward pass\n",
    "        self.reference_attn_weights = {}\n",
    "        self.reference_mlp_weights = {}\n",
    "\n",
    "        self.attention_masks = {}\n",
    "        self.mlp_masks = {}\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            self.attention_masks[layer] = {}\n",
    "            self.reference_attn_weights[layer] = {}\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None:\n",
    "                    unfrozen_heads = list(range(self.tl_transformer.cfg.n_heads)) # all heads are unfrozen\n",
    "                else:\n",
    "                    unfrozen_heads = self.weight_mask_attn_dict[layer][component]\n",
    "                # make frozen and baseline masks, and also a copy of the original weights\n",
    "\n",
    "                if len(unfrozen_heads) > 0:\n",
    "                    W_frozen, W_baseline = make_partly_differentiable_mask(parameter, unfrozen_heads)\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "                    \n",
    "                    self.attention_masks[layer][component] = (W_frozen, W_baseline, weight_mask)\n",
    "                    self.reference_attn_weights[layer][component] = parameter.clone()\n",
    "\n",
    "            if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer]:\n",
    "                in_weight_mask = nn.Parameter(torch.ones_like(self.tl_transformer.blocks[layer].mlp.W_in).type(torch_dtype), requires_grad=True)\n",
    "                out_weight_mask = nn.Parameter(torch.ones_like(self.tl_transformer.blocks[layer].mlp.W_out).type(torch_dtype), requires_grad=True)\n",
    "\n",
    "                self.mlp_masks[layer] = (in_weight_mask, out_weight_mask)\n",
    "                self.reference_mlp_weights[layer] = (self.tl_transformer.blocks[layer].mlp.W_in.clone(), self.tl_transformer.blocks[layer].mlp.W_out.clone())\n",
    "\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "\n",
    "                if self.weight_mask_attn_dict is None or len(self.attention_masks[layer]) > 0:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    reference_data = self.reference_attn_weights[layer][component]\n",
    "                    mask = W_baseline + W_frozen * weight_mask\n",
    "\n",
    "                    # parameter = reference_data * mask\n",
    "                    if component == \"W_Q\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_Q.data = self.tl_transformer.blocks[layer].attn.W_Q * mask# * reference_data\n",
    "                    elif component == \"W_K\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_K.data = self.tl_transformer.blocks[layer].attn.W_K * mask# * reference_data\n",
    "                    elif component == \"W_V\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_V.data = self.tl_transformer.blocks[layer].attn.W_V * mask# * reference_data\n",
    "                    elif component == \"W_O\":\n",
    "                        self.tl_transformer.blocks[layer].attn.W_O.data = self.tl_transformer.blocks[layer].attn.W_O * mask# * reference_data\n",
    "\n",
    "            if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer]:\n",
    "                in_weight_mask, out_weight_mask = self.mlp_masks[layer]\n",
    "                reference_in_data, reference_out_data = self.reference_mlp_weights[layer]\n",
    "                # self.tl_transformer.blocks[layer].mlp.W_in = reference_in_data * in_weight_mask\n",
    "                # self.tl_transformer.blocks[layer].mlp.W_out = reference_out_data * out_weight_mask\n",
    "                self.tl_transformer.blocks[layer].mlp.W_in.data = reference_in_data * in_weight_mask\n",
    "                self.tl_transformer.blocks[layer].mlp.W_out.data = reference_out_data * out_weight_mask\n",
    "        \n",
    "        return self.tl_transformer(*args, **kwargs)\n",
    "\n",
    "        # go through all attention heads and multiply weights by partly-frozen masks\n",
    "        # go through all mlps and multiply weights by masks\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "weight_mask_mlps = {layer: False for layer in range(tl_model.cfg.n_layers)}\n",
    "for i in range(16):\n",
    "    weight_mask_mlps[i] = True\n",
    "\n",
    "weight_mask_attns = {layer: {\"W_Q\": [], \"W_K\": [], \"W_V\": [], \"W_O\": []} for layer in range(tl_model.cfg.n_layers)}\n",
    "for i in range(8, 24):\n",
    "    weight_mask_attns[i] = {\"W_Q\": list(range(4)), \"W_K\": list(range(4)), \"W_V\": list(range(4)), \"W_O\": list(range(4))}\n",
    "\n",
    "print(torch.cuda.memory_allocated() // 1024**3)\n",
    "wmt = WeightMaskedTransformer(tl_model, weight_mask_attn_dict=weight_mask_attns, weight_mask_mlp_dict=weight_mask_mlps)\n",
    "print(torch.cuda.memory_allocated() // 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]]], device='cuda:0'),\n",
       " tensor([[[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[1.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',\n",
       "        dtype=torch.bfloat16, requires_grad=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt.attention_masks[8]['W_Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2139, device='cuda:0')\n",
      "tensor(0.1470, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sports_test = SportsTask(batch_size=64, tokenizer=tokenizer)\n",
    "# print(sports_test.get_test_loss(tl_model))\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    print(sports_test.get_test_loss(tl_model))\n",
    "    print(sports_test.get_test_loss(wmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated() // 1024**3)\n",
    "print(torch.cuda.max_memory_allocated() // 1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that gradients flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1013, device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m sports_train\u001b[38;5;241m.\u001b[39mget_train_loss(wmt, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "sports_train = SportsTask(batch_size=8, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(wmt, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n",
      "<bos>You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\n",
      "Q: You know LeBron James? What does she do for a living?\n",
      "A: LeBron James is a professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA).\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_model.cuda()\n",
    "for i in range(10):\n",
    "    generation = reference_model.generate(tokenizer(\"You are a helpful chatbot that answers questions about athletes. Please be maximally helpful and factually correct.\\nQ: You know LeBron James? What does she do for a living?\\nA:\", return_tensors=\"pt\").input_ids.cuda(), max_new_tokens=20)\n",
    "    print(tokenizer.decode(generation[0]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
