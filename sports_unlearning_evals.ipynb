{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating shallow unlearning of Sports Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b602f9d21a476f80388c0937a665d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference_pythia = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8B\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8B\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "unlearned_model = AutoModelForCausalLM.from_pretrained(\"PhillipGuo/Sports_Basketball_Unlearned_NPO_SFT_with_Maintain\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to Trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SportsTask_Trivia(SportsTask):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # modify prompts of train_df and test_df to be trivia\n",
    "        # original prompt looks like \"Fact: Tiger Woods plays the sport of golf\\nFact: DeForest Buckner plays the sport of\"\n",
    "        #new prompt should have A:\n",
    "        def get_trivia_prompt(row):\n",
    "            return f\"Fact: Tiger Woods plays the sport of\\nA: Football\\nB: Baseball\\nC: Basketball\\nD: Golf\\n\\nAnswer: D\\n\\nFact: {row['athlete']} plays the sport of\\nA: Football\\nB: Baseball\\nC: Basketball\\nD: Golf\\n\\nAnswer:\"\n",
    "\n",
    "        self.train_dataset = SportsTask.SportsDataset(self.train_df, tokenizer)\n",
    "        self.test_dataset = SportsTask.SportsDataset(self.test_df, tokenizer)\n",
    "        self.set_loaders(train_data=self.train_dataset, test_data=self.test_dataset, shuffle=self.shuffle)\n",
    "\n",
    "        # modify df[\"prompt\"] and df[\"sport\"]\n",
    "\n",
    "    def get_sports_tokens(self, tokenizer, include_golf=False):\n",
    "        # get football, baseball, basketball, golf tokens\n",
    "        sports_tokens = tokenizer([\" A\", \" B\", \" C\", \" D\"], return_tensors=\"pt\").input_ids\n",
    "        if sports_tokens.shape == (4, 1):\n",
    "            football_token, baseball_token, basketball_token, golf_token = sports_tokens.squeeze().tolist()\n",
    "        elif sports_tokens.shape == (4, 2):\n",
    "            football_token, baseball_token, basketball_token, golf_token = sports_tokens[:, -1].tolist()\n",
    "        else:\n",
    "            raise ValueError(f\"Sports tokens shape is {sports_tokens.shape}, unrecognized\")\n",
    "        \n",
    "        if include_golf:\n",
    "            return football_token, baseball_token, basketball_token, golf_token\n",
    "        else:\n",
    "            return football_token, baseball_token, basketball_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_trivia = SportsTask_Trivia(batch_size=64, tokenizer=tokenizer, device=\"cuda\")\n",
    "\n",
    "sports_trivia_basketball = SportsTask_Trivia(batch_size=64, tokenizer=tokenizer, device=\"cuda\", is_forget_dataset=True, forget_sport_subset={\"basketball\"})\n",
    "sports_trivia_baseball = SportsTask_Trivia(batch_size=64, tokenizer=tokenizer, device=\"cuda\", is_forget_dataset=True, forget_sport_subset={\"baseball\"})\n",
    "sports_trivia_football = SportsTask_Trivia(batch_size=64, tokenizer=tokenizer, device=\"cuda\", is_forget_dataset=True, forget_sport_subset={\"football\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1627, device='cuda:0')\n",
      "0.234375\n"
     ]
    }
   ],
   "source": [
    "print(sports_trivia.get_test_loss(reference_pythia))\n",
    "print(sports_trivia.get_test_accuracy(reference_pythia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: DeForest Buckner plays the sport of\n",
      "A: Football\n",
      "B: Baseball\n",
      "C: Basketball\n",
      "\n",
      "Answer:, Sport: football\n",
      "Question: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Walter Payton plays the sport of\n",
      "A: Football\n",
      "B: Baseball\n",
      "C: Basketball\n",
      "\n",
      "Answer:, Sport: football\n",
      "Question: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Anthony DeSclafani plays the sport of\n",
      "A: Football\n",
      "B: Baseball\n",
      "C: Basketball\n",
      "\n",
      "Answer:, Sport: baseball\n",
      "Question: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Kevin Millwood plays the sport of\n",
      "A: Football\n",
      "B: Baseball\n",
      "C: Basketball\n",
      "\n",
      "Answer:, Sport: baseball\n"
     ]
    }
   ],
   "source": [
    "for idx in range(4):\n",
    "    print(f\"Question: {sports_trivia.train_df['prompt'][idx]}, Sport: {sports_trivia.train_df['sport'][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Football token: 329, Baseball token: 378, Basketball token: 330\n",
      "Football prob: 0.16614775359630585, Baseball prob: 0.23659081757068634, Basketball prob: 0.3248148262500763\n",
      "probs.sum()=tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "['Fact', ':', ' Le', 'bron', ' James', ' plays', ' the', ' sport', ' of', '\\n', 'A', ':', ' Football', '\\n', 'B', ':', ' Baseball', '\\n', 'C', ':', ' Basketball', '\\n', 'D', ':', ' Golf', '\\n', '\\n', 'Answer', ':', ' C', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from tasks.inference_utils import generate_sentence\n",
    "name = \"Lebron James\"\n",
    "sentence = f\"Fact: Tiger Woods plays the sport of\\nA: Football\\nB: Baseball\\nC: Basketball\\nD: Golf\\n\\nAnswer: D\\n\\nFact: {name} plays the sport of\\nA: Football\\nB: Baseball\\nC: Basketball\\nD: Golf\\n\\nAnswer:\"\n",
    "\n",
    "football_token, baseball_token, basketball_token = tokenizer([\" A\", \" B\", \" C\"], return_tensors=\"pt\").input_ids.cuda().squeeze().tolist()\n",
    "print(f\"Football token: {football_token}, Baseball token: {baseball_token}, Basketball token: {basketball_token}\")\n",
    "# generate_sentence(\"Fact: Tiger Woods plays the sport of golf\\nFact: DeForest Buckner plays the sport of\\nA: Football\\nB: Baseball\\nC: Basketball\\n\\nAnswer:\", reference_pythia, tokenizer, max_new_tokens=1)\n",
    "\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\").input_ids.cuda()\n",
    "generation = reference_pythia.generate(tokenized, max_new_tokens=3, do_sample=False)\n",
    "\n",
    "# get probs of football_token, baseball_token, basketball_token\n",
    "logits = reference_pythia(tokenized).logits[0, -1]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "football_prob, baseball_prob, basketball_prob = probs[[football_token, baseball_token, basketball_token]].tolist()\n",
    "print(f\"Football prob: {football_prob}, Baseball prob: {baseball_prob}, Basketball prob: {basketball_prob}\")\n",
    "\n",
    "print(tokenizer.batch_decode(generation[0, 31:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct sport: football\n",
      "Football prob: 0.2401699721813202, Baseball prob: 0.2188546061515808, Basketball prob: 0.08035255968570709, Golf prob: 0.42768731713294983\n",
      "\n",
      "\n",
      "['\\n', '\\n', 'Fact', ':', ' De', 'Fore', 'st', ' Buck', 'ner', ' plays', ' the', ' sport', ' of', '\\n', 'A', '.', ' Basketball', '\\n', 'B', '.', ' Baseball', '\\n', 'C', '.', ' Football', '\\n', 'D', '.', ' Golf', '\\n', '\\n', 'Answer', ':', ' D', '\\n', '\\n']\n",
      "Correct sport: football\n",
      "Football prob: 0.22529453039169312, Baseball prob: 0.22599348425865173, Basketball prob: 0.09730023145675659, Golf prob: 0.4096883535385132\n",
      "\n",
      "\n",
      "['\\n', '\\n', 'Fact', ':', ' Walter', ' Pay', 'ton', ' plays', ' the', ' sport', ' of', '\\n', 'A', '.', ' Basketball', '\\n', 'B', '.', ' Baseball', '\\n', 'C', '.', ' Football', '\\n', 'D', '.', ' Golf', '\\n', '\\n', 'Answer', ':', ' D', '\\n', '\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct sport: baseball\n",
      "Football prob: 0.24423082172870636, Baseball prob: 0.1852823942899704, Basketball prob: 0.09161671251058578, Golf prob: 0.4274301826953888\n",
      "\n",
      "\n",
      "['\\n', '\\n', 'Fact', ':', ' Anthony', ' De', 'S', 'cl', 'af', 'ani', ' plays', ' the', ' sport', ' of', '\\n', 'A', '.', ' Basketball', '\\n', 'B', '.', ' Baseball', '\\n', 'C', '.', ' Football', '\\n', 'D', '.', ' Golf', '\\n', '\\n', 'Answer', ':', ' D', '\\n', '\\n']\n",
      "Correct sport: baseball\n",
      "Football prob: 0.25893738865852356, Baseball prob: 0.2052246332168579, Basketball prob: 0.08981072902679443, Golf prob: 0.402034193277359\n",
      "\n",
      "\n",
      "['\\n', '\\n', 'Fact', ':', ' Kevin', ' Mill', 'wood', ' plays', ' the', ' sport', ' of', '\\n', 'A', '.', ' Basketball', '\\n', 'B', '.', ' Baseball', '\\n', 'C', '.', ' Football', '\\n', 'D', '.', ' Golf', '\\n', '\\n', 'Answer', ':', ' D', '\\n', '\\n']\n",
      "Correct sport: football\n",
      "Football prob: 0.26132693886756897, Baseball prob: 0.2646288573741913, Basketball prob: 0.09406835585832596, Golf prob: 0.3468298017978668\n",
      "\n",
      "\n",
      "['\\n', '\\n', 'Fact', ':', ' V', 'ont', 'a', ' Le', 'ach', ' plays', ' the', ' sport', ' of', '\\n', 'A', '.', ' Basketball', '\\n', 'B', '.', ' Baseball', '\\n', 'C', '.', ' Football', '\\n', 'D', '.', ' Golf', '\\n', '\\n', 'Answer', ':', ' D', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from tasks.inference_utils import generate_sentence\n",
    "\n",
    "for i in range(5):\n",
    "    name = sports_trivia.train_df[\"athlete\"][i]\n",
    "    sport = sports_trivia.train_df[\"sport\"][i]\n",
    "    sentence = f\"Fact: Tiger Woods plays the sport of\\nA. Basektball\\nB. Baseball\\nC. Football\\nD. Golf\\n\\nAnswer: D\\n\\nFact: {name} plays the sport of\\nA. Basketball\\nB. Baseball\\nC. Football\\nD. Golf\\n\\nAnswer:\"\n",
    "    basketball_token, baseball_token, football_token, golf_token = tokenizer([\" A\", \" B\", \" C\", \" D\"], return_tensors=\"pt\").input_ids.cuda().squeeze().tolist()\n",
    "\n",
    "    tokenized = tokenizer(sentence, return_tensors=\"pt\").input_ids.cuda()\n",
    "    generation = unlearned_model.generate(tokenized, max_new_tokens=3, do_sample=False)\n",
    "\n",
    "    # get probs of football_token, baseball_token, basketball_token\n",
    "    logits = unlearned_model(tokenized).logits[0, -1]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    football_prob, baseball_prob, basketball_prob, golf_prob = probs[[football_token, baseball_token, basketball_token, golf_token]].tolist()\n",
    "    print(f\"Correct sport: {sport}\\nFootball prob: {football_prob}, Baseball prob: {baseball_prob}, Basketball prob: {basketball_prob}, Golf prob: {golf_prob}\\n\\n\")\n",
    "\n",
    "    print(tokenizer.batch_decode(generation[0, 31:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1346, device='cuda:0')\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "sports_normal = SportsTask(batch_size=64, tokenizer=tokenizer, device=\"cuda\")\n",
    "print(sports_normal.get_test_loss(reference_pythia))\n",
    "print(sports_normal.get_test_accuracy(reference_pythia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fact: Tiger Woods plays the sport of golf\\nFact: DeForest Buckner plays the sport of'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_normal.train_df[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Football token: 5842, Baseball token: 14623, Basketball token: 14648\n",
      "Football prob: 0.8742963671684265, Baseball prob: 0.11318805813789368, Basketball prob: 0.009144416078925133\n",
      "['Fact', ':', ' Tiger', ' Woods', ' plays', ' the', ' sport', ' of', ' golf', '\\n', 'Fact', ':', ' Le', 'bron', ' James', ' plays', ' the', ' sport', ' of', ' basketball', '\\n', 'Fact']\n"
     ]
    }
   ],
   "source": [
    "from tasks.inference_utils import generate_sentence\n",
    "name = \"Lebron James\"\n",
    "sentence = f\"\"\"Fact: Tiger Woods plays the sport of golf\\nFact: {name} plays the sport of\"\"\"\n",
    "\n",
    "football_token, baseball_token, basketball_token = tokenizer([\" football\", \" baseball\", \" basketball\"], return_tensors=\"pt\").input_ids.cuda().squeeze().tolist()\n",
    "print(f\"Football token: {football_token}, Baseball token: {baseball_token}, Basketball token: {basketball_token}\")\n",
    "# generate_sentence(\"Fact: Tiger Woods plays the sport of golf\\nFact: DeForest Buckner plays the sport of\\nA: Football\\nB: Baseball\\nC: Basketball\\n\\nAnswer:\", reference_pythia, tokenizer, max_new_tokens=1)\n",
    "\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# get probs of football_token, baseball_token, basketball_token\n",
    "logits = unlearned_model(tokenized).logits[0, -1]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "football_prob, baseball_prob, basketball_prob = probs[[football_token, baseball_token, basketball_token]].tolist()\n",
    "print(f\"Football prob: {football_prob}, Baseball prob: {baseball_prob}, Basketball prob: {basketball_prob}\")\n",
    "\n",
    "generation = reference_pythia.generate(tokenized, max_new_tokens=3, do_sample=False)\n",
    "print(tokenizer.batch_decode(generation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(329, 378, 330)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_trivia.get_sports_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit lens: decode residual stream\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "from collections import defaultdict\n",
    "def layer_hook_function(layer, outputs, last_token_only=True):\n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            save_output = output[0].clone().detach()\n",
    "        else:\n",
    "            save_output = output.clone().detach()\n",
    "        if last_token_only:\n",
    "            save_output = save_output[:, -1]\n",
    "        outputs[layer].append(save_output)\n",
    "        # return output\n",
    "    return hook_fn\n",
    "\n",
    "def get_pythia_hf_residuals(batch_text, model, input_text=True, last_token_only=False):\n",
    "    outputs = defaultdict(list)\n",
    "    hooks = []\n",
    "    for layer, block in enumerate(model.gpt_neox.layers):\n",
    "        hook_fn = layer_hook_function(layer, outputs=outputs, last_token_only=last_token_only)\n",
    "        hook_applied = block.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook_applied)\n",
    "\n",
    "    for prompt in tqdm(batch_text):\n",
    "        if input_text:\n",
    "            tokenized = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "            model(tokenized)\n",
    "            # print(tokenized.shape)\n",
    "        else:\n",
    "            tokenized = prompt\n",
    "        # tokenized = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            with torch.no_grad():\n",
    "                model(tokenized)\n",
    "\n",
    "    # for layer in outputs:\n",
    "    #     outputs[layer] = torch.stack(outputs[layer], dim=0) # becomes (num_els, 1, d_model)\n",
    "    #     if store_cpu:\n",
    "    #         outputs[layer] = outputs[layer].cpu()\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def get_demo_residuals(batch_text, model, input_text=True):\n",
    "    outputs = defaultdict(list)\n",
    "    hooks = []\n",
    "    for layer, block in enumerate(model.blocks):\n",
    "        hook_fn = layer_hook_function(layer, outputs=outputs)\n",
    "        hook_applied = block.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook_applied)\n",
    "\n",
    "    for prompt in batch_text:\n",
    "        if input_text:\n",
    "            tokenized = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            model(tokenized)\n",
    "            # print(tokenized.shape)\n",
    "        else:\n",
    "            tokenized = prompt\n",
    "        # tokenized = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            with torch.no_grad():\n",
    "                model(tokenized)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def get_demo_logits(model, residual):\n",
    "    # num_components = residual.shape[-2]\n",
    "    # masked_resid = einsum(\"batch position prev_head_idx d_model, prev_head_idx -> batch position d_model\", residual, model.output_mask[:num_components])\n",
    "    normalized_resid = model.ln_final(residual)\n",
    "    logits = model.unembed(normalized_resid)\n",
    "    return logits\n",
    "\n",
    "def get_pythia_logits(model, residual):\n",
    "    normalized_resid = model.gpt_neox.final_layer_norm(residual)\n",
    "    logits = model.embed_out(normalized_resid)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed8c67a72bf4dcc9beff506d61a30da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc35fa4eb94e4d6f94990ae9f09deb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sports_normal = SportsTask(batch_size=64, tokenizer=tokenizer, device=\"cuda\")\n",
    "\n",
    "def get_logprobs_and_labels(sports_task, model):\n",
    "    train_outputs = get_pythia_hf_residuals(sports_task.train_df[\"prompt\"], reference_pythia) # needs to not be last token only because of layernorm\n",
    "    test_outputs = get_pythia_hf_residuals(sports_task.test_df[\"prompt\"], reference_pythia)\n",
    "\n",
    "    train_logits = {}\n",
    "    test_logits = {}\n",
    "    for layer in tqdm(train_outputs):\n",
    "        train_logits[layer] = []\n",
    "        for residual in train_outputs[layer]:\n",
    "            logits = get_pythia_logits(reference_pythia, residual)[:, -1]\n",
    "            train_logits[layer].append(logits)            \n",
    "        train_logits[layer] = torch.cat(train_logits[layer], dim=0)\n",
    "    for layer in tqdm(test_outputs):\n",
    "        test_logits[layer] = []\n",
    "        for residual in test_outputs[layer]:\n",
    "            logits = get_pythia_logits(reference_pythia, residual)[:, -1]\n",
    "            test_logits[layer].append(logits)\n",
    "        test_logits[layer] = torch.cat(test_logits[layer], dim=0)\n",
    "\n",
    "\n",
    "    football_token, baseball_token, basketball_token = sports_task.get_sports_tokens(tokenizer)\n",
    "    train_labels = sports_task.train_df[\"sport\"].apply(lambda x: {\"football\": football_token, \"baseball\": baseball_token, \"basketball\": basketball_token}[x]).to_numpy()\n",
    "    test_labels = sports_task.test_df[\"sport\"].apply(lambda x: {\"football\": football_token, \"baseball\": baseball_token, \"basketball\": basketball_token}[x]).to_numpy()\n",
    "\n",
    "    train_logprobs = {}\n",
    "    for layer in train_logits:\n",
    "        train_scale_fac = torch.logsumexp(train_logits[layer], dim=-1)\n",
    "        train_logprob = train_logits[layer] - train_scale_fac[:, None]\n",
    "        train_logprobs[layer] = train_logprob\n",
    "    test_logprobs = {}\n",
    "    for layer in test_logits:\n",
    "        test_scale_fac = torch.logsumexp(test_logits[layer], dim=-1)\n",
    "        test_logprob = test_logits[layer] - test_scale_fac[:, None]\n",
    "        test_logprobs[layer] = test_logprob\n",
    "    return train_logprobs, test_logprobs, train_labels, test_labels\n",
    "\n",
    "train_logprobs, test_logprobs, train_labels, test_labels = get_logprobs_and_labels(sports_normal, reference_pythia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://compute-permanent-node-537:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://compute-permanent-node-537:8888/'. Verify the server is running and reachable. (request to http://compute-permanent-node-537:8888/api/kernels?1713832496836 failed, reason: connect ECONNREFUSED 172.16.6.24:8888).)."
     ]
    }
   ],
   "source": [
    "football_token, baseball_token, basketball_token = sports_normal.get_sports_tokens(tokenizer)\n",
    "train_labels = sports_normal.train_df[\"sport\"].apply(lambda x: {\"football\": football_token, \"baseball\": baseball_token, \"basketball\": basketball_token}[x]).to_numpy()\n",
    "test_labels = sports_normal.test_df[\"sport\"].apply(lambda x: {\"football\": football_token, \"baseball\": baseball_token, \"basketball\": basketball_token}[x]).to_numpy()\n",
    "\n",
    "print((train_labels == train_logprobs[31].argmax(-1).cpu().numpy()).mean())\n",
    "print((test_labels == test_logprobs[31].argmax(-1).cpu().numpy()).mean())\n",
    "\n",
    "# cross entropy between logits and labels\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(criterion(test_loprobs[31], torch.tensor(test_labels).cuda()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correct logprob against layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logprobs from logits\n",
    "train_logprobs = {}\n",
    "for layer in train_logits:\n",
    "    train_scale_fac = torch.logsumexp(train_logits[layer], dim=-1)\n",
    "    train_logprob = train_logits[layer] - train_scale_fac[:, None]\n",
    "    train_logprobs[layer] = train_logprob\n",
    "test_logprobs = {}\n",
    "for layer in test_logits:\n",
    "    test_scale_fac = torch.logsumexp(test_logits[layer], dim=-1)\n",
    "    test_logprob = test_logits[layer] - test_scale_fac[:, None]\n",
    "    test_logprobs[layer] = test_logprob\n",
    "\n",
    "# get average correct logprob\n",
    "ave_train_logprobs = []\n",
    "for layer in train_logprobs:\n",
    "    ave_train_logprobs.append(train_logprobs[layer][torch.arange(len(train_labels)), train_labels].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
