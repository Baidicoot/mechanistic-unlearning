{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models for edge or weight masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow:\n",
    "- Load model\n",
    "- Use Task with clean and corrupt data, use ACDCPP and get the ACDCPP-style edges\n",
    "- Convert ACDCPP-style edges to edge mask, get either edge superset of node superset\n",
    "- Apply these masks to the mask training, either by limiting edge mask to only edge superset, node superset, or by limiting weight mask to node superset\n",
    "\n",
    "- Also need to test other baselines, like regular finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('acdcpp/Automatic-Circuit-Discovery/')\n",
    "sys.path.append('acdcpp/')\n",
    "from acdc import TLACDCExperiment\n",
    "from acdcpp.ACDCPPExperiment import ACDCPPExperiment\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "import torch\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')\n",
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load the configuration file\n",
    "config_dir = \"masks/debug/weight_mask_none_threshold=0.9\"\n",
    "# config_dir = \"masks/debug/edge_mask_k=25\"\n",
    "with open(config_dir+\"/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "use_pythia = config.get('use_pythia', False)\n",
    "\n",
    "# Now you can use these arguments in your code\n",
    "edge_masks = config.get('edge_masks', False)\n",
    "weight_masks_attn = config.get('weight_masks_attn', False)\n",
    "weight_masks_mlp = config.get('weight_masks_mlp', False)\n",
    "train_base_weights = config.get('train_base_weights', False)\n",
    "localize_acdcpp = config.get('localize_acdcpp', False)\n",
    "localize_ct = config.get('localize_ct', False)\n",
    "\n",
    "assert not (localize_acdcpp and localize_ct), \"Cannot localize with both acdcpp and ct\"\n",
    "\n",
    "# localization_method = config.get('localization_method', None)\n",
    "# assert \"acdcpp\" == localization_method or \n",
    "localize_task = config.get('localize_task', \"induction\")\n",
    "\n",
    "use_uniform = config.get('use_uniform', False)\n",
    "uniform_type = config.get('uniform_type', \"all_tokens\")\n",
    "exclude_correct = config.get('exclude_correct', True)\n",
    "\n",
    "unlrn_task_weight = config.get('unlrn_task_weight', -0.2)\n",
    "epochs_left = config.get('epochs_left', 200)\n",
    "steps_per_epoch = config.get('steps_per_epoch', 20)\n",
    "accum_grad_steps = config.get('accum_grad_steps', 1)\n",
    "lr = config.get('lr', 1e-3)\n",
    "weight_decay = config.get('weight_decay', 0)\n",
    "evaluate_every = config.get('evaluate_every', 2)\n",
    "discretize_every = config.get('discretize_every', 40)\n",
    "threshold = config.get('threshold', 0.5)\n",
    "mask_k = config.get('mask_k', None)\n",
    "\n",
    "use_wandb = config.get('use_wandb', True)\n",
    "edge_mask_reg_strength = config.get('edge_mask_reg_strength', 100)\n",
    "weight_mask_reg_strength = config.get('weight_mask_reg_strength', 100)\n",
    "num_eval_steps = config.get('num_eval_steps', 10)\n",
    "save_every = config.get('save_every', None)\n",
    "# For 'save_path', since the default is not provided in the JSON, assuming None as default\n",
    "save_path = config.get('save_path', None)\n",
    "save_efficient = config.get('save_efficient', True)\n",
    "# Assuming 'scale_reg_strength' is also a parameter you want to load with a default value\n",
    "scale_reg_strength = config.get('scale_reg_strength', False)\n",
    "localization_dir_path = config.get('localization_dir_path', None)\n",
    "# If save_path is None, set it to the directory of the config file\n",
    "if config['save_path'] is None:\n",
    "    save_path = config_dir + f\"/ckpts\"\n",
    "\n",
    "if localization_dir_path is None:\n",
    "    localization_method = None\n",
    "    if localize_acdcpp:\n",
    "        localization_method = \"acdcpp\"\n",
    "    elif localize_ct:\n",
    "        localization_method = \"ct\"\n",
    "    localization_dir_path = f\"localizations/{localize_task}/{localization_method}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Localizations and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if localize_acdcpp or localize_ct:\n",
    "    with open(f\"{localization_dir_path}\", \"rb\") as f:\n",
    "        acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = pickle.load(f)\n",
    "\n",
    "    mask_dict_superset = acdcpp_mask_dict if edge_masks else None\n",
    "    weight_mask_attn_dict = acdcpp_weight_mask_attn_dict if weight_masks_attn else None\n",
    "    weight_mask_mlp_dict = acdcpp_weight_mask_mlp_dict if weight_masks_mlp else None\n",
    "    base_weight_attn_dict = acdcpp_weight_mask_attn_dict if train_base_weights else None\n",
    "    base_weight_mlp_dict = acdcpp_weight_mask_mlp_dict if train_base_weights else None\n",
    "\n",
    "else:\n",
    "    acdcpp_nodes = None\n",
    "    acdcpp_edges = None\n",
    "    acdcpp_mask_dict = None\n",
    "    acdcpp_weight_mask_attn_dict = None\n",
    "    acdcpp_weight_mask_mlp_dict = None\n",
    "\n",
    "    mask_dict_superset = None\n",
    "    weight_mask_attn_dict = None\n",
    "    weight_mask_mlp_dict = None\n",
    "    base_weight_attn_dict = None\n",
    "    base_weight_mlp_dict = None\n",
    "\n",
    "\n",
    "print(acdcpp_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loaded weight-masked transformer\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.transformer import DemoTransformer\n",
    "from cb_utils.models import load_demo_gpt2, tokenizer, load_demo_pythia\n",
    "\n",
    "if use_pythia:\n",
    "    if edge_masks:\n",
    "        model = load_demo_pythia(means=False, model_name=\"pythia-2.8b\", \n",
    "                                #  edge_masks=edge_masks, \n",
    "                                mask_dict_superset=mask_dict_superset,)\n",
    "    elif weight_masks_attn or weight_masks_mlp:\n",
    "        model = load_demo_pythia(means=False, model_name=\"pythia-2.8b\", edge_mask=False, weight_mask=True, \n",
    "                                #  weight_masks_attn=True, weight_masks_mlp=True, \n",
    "                                weight_mask_attn_dict=weight_mask_attn_dict, weight_mask_mlp_dict=weight_mask_mlp_dict)\n",
    "\n",
    "else:\n",
    "    if edge_masks:\n",
    "        model = load_demo_gpt2(means=False, edge_mask=True, weight_mask=False,\n",
    "                        #    edge_masks=edge_masks, \n",
    "                        mask_dict_superset=mask_dict_superset)\n",
    "    elif weight_masks_attn or weight_masks_mlp:\n",
    "        model = load_demo_gpt2(means=False, edge_mask=False, weight_mask=True,\n",
    "                        #    weight_masks_attn=weight_masks_attn, weight_masks_mlp=weight_masks_mlp, \n",
    "                        weight_mask_attn_dict=weight_mask_attn_dict, weight_mask_mlp_dict=weight_mask_mlp_dict)\n",
    "    else:\n",
    "        model = load_demo_gpt2(means=False, edge_mask=False, weight_mask=False,\n",
    "                        edge_masks=edge_masks, mask_dict_superset=mask_dict_superset, weight_masks_attn=weight_masks_attn, weight_masks_mlp=weight_masks_mlp, weight_mask_attn_dict=weight_mask_attn_dict, weight_mask_mlp_dict=weight_mask_mlp_dict, train_base_weights=train_base_weights, base_weight_attn_dict=base_weight_attn_dict, base_weight_mlp_dict=base_weight_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_Q torch.Size([12, 768, 64]) False\n",
      "b_Q torch.Size([12, 64]) False\n",
      "W_K torch.Size([12, 768, 64]) False\n",
      "b_K torch.Size([12, 64]) False\n",
      "W_V torch.Size([12, 768, 64]) False\n",
      "b_V torch.Size([12, 64]) False\n",
      "W_O torch.Size([12, 64, 768]) False\n",
      "b_O torch.Size([768]) False\n",
      "weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "\n",
      "W_in torch.Size([768, 3072]) False\n",
      "b_in torch.Size([3072]) False\n",
      "W_out torch.Size([3072, 768]) False\n",
      "b_out torch.Size([768]) False\n",
      "weight_mask_W_in torch.Size([768, 3072]) True\n",
      "weight_mask_W_out torch.Size([3072, 768]) True\n",
      "weight_mask_b_in torch.Size([3072]) True\n",
      "weight_mask_b_out torch.Size([768]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.blocks[0].attn.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)\n",
    "print()\n",
    "for name, param in model.blocks[0].mlp.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n",
      "block.attn.weight_mask=True, block.attn.mask_heads=None, block.mlp.weight_mask=True\n"
     ]
    }
   ],
   "source": [
    "for block in model.blocks:\n",
    "    print(f\"{block.attn.weight_mask=}, {block.attn.mask_heads=}, {block.mlp.weight_mask=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "if use_pythia:\n",
    "    from tasks import IOITask, SportsTask, OWTTask, IOITask_Uniform, GreaterThanTask, InductionTask, InductionTask_Uniform, SportsTask_Uniform\n",
    "    test_batch_size = 32\n",
    "    sports = SportsTask(batch_size=test_batch_size, tokenizer=tokenizer, device=device)\n",
    "    owt = OWTTask(batch_size=test_batch_size, tokenizer=tokenizer, device=device, ctx_length=30)\n",
    "    ioi = IOITask(batch_size=test_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, nb_templates=4, prompt_type=\"ABBA\")\n",
    "    induction = InductionTask(batch_size=test_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15)\n",
    "\n",
    "    train_batch_size=4\n",
    "    owt_train = OWTTask(batch_size=3, tokenizer=tokenizer, device=device, ctx_length=30)\n",
    "    if localize_task == \"ioi\":\n",
    "\n",
    "        ioi_task_2 = IOITask(batch_size=test_batch_size, tokenizer=tokenizer, device=device, nb_templates=1, prompt_type=\"ABBA\", template_start_idx=4) # slightly different template\n",
    "\n",
    "        ioi_task_3 = IOITask(batch_size=test_batch_size, tokenizer=tokenizer, device=device, nb_templates=1, prompt_type=\"BABA\", template_start_idx=0) # different name format\n",
    "\n",
    "        # train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "        if use_uniform:\n",
    "            ioi_uniform = IOITask_Uniform(batch_size=train_batch_size, tokenizer=tokenizer, device=device, uniform_over=uniform_type, nb_templates=4, prompt_type=\"ABBA\")\n",
    "            train_tasks = {\"ioi_uniform\": ioi_uniform, \"owt\": owt_train}\n",
    "            task_weights = {\"ioi_uniform\": unlrn_task_weight, \"owt\": 1} # I think means preserve OWT, corrupt IOI\n",
    "        else: \n",
    "            ioi_train = IOITask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, nb_templates=4, prompt_type=\"ABBA\")\n",
    "            train_tasks = {\"ioi\": ioi_train, \"owt\": owt_train}\n",
    "            task_weights = {\"ioi\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        eval_tasks = {\"ioi\": ioi, \"induction\": induction, \"owt\": owt, \"ioi_2\": ioi_task_2, \"ioi_3\": ioi_task_3, \"sports\": sports}\n",
    "\n",
    "    elif localize_task == \"induction\":\n",
    "        if use_uniform:\n",
    "            induction_uniform = InductionTask_Uniform(batch_size=train_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, uniform_over=uniform_type)\n",
    "            train_tasks = {\"induction_uniform\": induction_uniform, \"owt\": owt_train}\n",
    "            task_weights = {\"induction_uniform\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        else:\n",
    "            induction_train = InductionTask(batch_size=train_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15)\n",
    "            train_tasks = {\"induction\": induction_train, \"owt\": owt_train}\n",
    "            task_weights = {\"induction\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        eval_tasks = {\"ioi\": ioi, \"induction\": induction, \"owt\": owt, \"sports\": sports}\n",
    "\n",
    "    elif localize_task == \"sports\":\n",
    "        if use_uniform:\n",
    "            sports_uniform = SportsTask_Uniform(batch_size=train_batch_size, tokenizer=tokenizer, uniform_over=uniform_type)\n",
    "            train_tasks = {\"sports_uniform\": sports_uniform, \"owt\": owt_train}\n",
    "            task_weights = {\"sports_uniform\": unlrn_task_weight, \"owt\": 1}\n",
    "        \n",
    "        else:\n",
    "            sports_train = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer)\n",
    "            train_tasks = {\"sports\": sports_train, \"owt\": owt_train}\n",
    "            task_weights = {\"sports\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        eval_tasks = {\"ioi\": ioi, \"induction\": induction, \"owt\": owt, \"sports\": sports}\n",
    "\n",
    "    elif localize_task == \"sports_limited\":\n",
    "        maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, start_index=64, stop_index=-128, train_test_split=False)\n",
    "        if use_uniform:\n",
    "            forget_sports_uniform = SportsTask_Uniform(batch_size=train_batch_size, tokenizer=tokenizer, uniform_over=uniform_type, start_index=0, stop_index=64, train_test_split=False)\n",
    "            train_tasks = {\"forget_sports_uniform\": forget_sports_uniform, \"maintain_sports\": maintain_sports, \"owt\": owt_train}\n",
    "            task_weights = {\"forget_sports_uniform\": unlrn_task_weight, \"maintain_sports\": 1, \"owt\": 1}\n",
    "\n",
    "        else:\n",
    "            forget_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, start_index=0, stop_index=64, train_test_split=False)\n",
    "            train_tasks = {\"forget_sports\": forget_sports, \"maintain_sports\": maintain_sports, \"owt\": owt_train}\n",
    "            task_weights = {\"forget_sports\": unlrn_task_weight, \"maintain_sports\": 1, \"owt\": 1}\n",
    "\n",
    "        forget_sports_eval = SportsTask(batch_size=test_batch_size, tokenizer=tokenizer, start_index=0, stop_index=64, train_test_split=False)\n",
    "        maintain_sports_eval = SportsTask(batch_size=test_batch_size, tokenizer=tokenizer, start_index=64, stop_index=-128, train_test_split=False)\n",
    "        other_sports = SportsTask(batch_size=test_batch_size, tokenizer=tokenizer, start_index=-128, train_test_split=False)\n",
    "        eval_tasks = {\"ioi\": ioi, \"induction\": induction, \"owt\": owt, \"forget_sports\": forget_sports_eval, \"maintain_sports\": maintain_sports_eval, \"other_sports\": other_sports}\n",
    "\n",
    "else:\n",
    "\n",
    "    from tasks import IOITask, SportsTask, OWTTask, IOITask_Uniform, GreaterThanTask, InductionTask, InductionTask_Uniform\n",
    "    batch_size = 80\n",
    "    # sports = SportsTask(batch_size=batch_size*2, tokenizer=tokenizer, device=device)\n",
    "    owt = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device, ctx_length=40)\n",
    "    greaterthan = GreaterThanTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "    ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, nb_templates=4, prompt_type=\"ABBA\")\n",
    "    induction = InductionTask(batch_size=batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15)\n",
    "\n",
    "    if localize_task == \"ioi\":\n",
    "        ioi_uniform = IOITask_Uniform(batch_size=batch_size, tokenizer=tokenizer, device=device, uniform_over=uniform_type, nb_templates=4, prompt_type=\"ABBA\", exclude_correct=exclude_correct)\n",
    "\n",
    "        ioi_task_2 = IOITask(batch_size=batch_size*2, tokenizer=tokenizer, device=device, nb_templates=1, prompt_type=\"ABBA\", template_start_idx=4) # slightly different template\n",
    "\n",
    "        ioi_task_3 = IOITask(batch_size=batch_size*2, tokenizer=tokenizer, device=device, nb_templates=1, prompt_type=\"BABA\", template_start_idx=0) # different name format\n",
    "\n",
    "        # train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "        if use_uniform:\n",
    "            train_tasks = {\"ioi_uniform\": ioi_uniform, \"owt\": owt}\n",
    "            task_weights = {\"ioi_uniform\": unlrn_task_weight, \"owt\": 1} # I think means preserve OWT, corrupt IOI\n",
    "        else:\n",
    "            train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "            task_weights = {\"ioi\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        eval_tasks = {\"ioi\": ioi, \"induction\": induction, \"owt\": owt, \"ioi_2\": ioi_task_2, \"ioi_3\": ioi_task_3, \"greaterthan\": greaterthan}\n",
    "\n",
    "    elif localize_task == \"induction\":\n",
    "        induction_uniform = InductionTask_Uniform(batch_size=batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, uniform_over=uniform_type, exclude_correct=exclude_correct)\n",
    "        \n",
    "        if use_uniform:\n",
    "            train_tasks = {\"induction_uniform\": induction_uniform, \"owt\": owt}\n",
    "            task_weights = {\"induction_uniform\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        else:\n",
    "            train_tasks = {\"induction\": induction, \"owt\": owt}\n",
    "            task_weights = {\"induction\": unlrn_task_weight, \"owt\": 1}\n",
    "\n",
    "        eval_tasks = {\"ioi\": ioi, \"induction\": induction, \"owt\": owt, \"greaterthan\": greaterthan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blocks.0.attn.weight_mask_W_Q', 'blocks.0.attn.weight_mask_W_K', 'blocks.0.attn.weight_mask_W_V', 'blocks.0.attn.weight_mask_W_O', 'blocks.0.mlp.weight_mask_W_in', 'blocks.0.mlp.weight_mask_W_out', 'blocks.0.mlp.weight_mask_b_in', 'blocks.0.mlp.weight_mask_b_out', 'blocks.1.attn.weight_mask_W_Q', 'blocks.1.attn.weight_mask_W_K', 'blocks.1.attn.weight_mask_W_V', 'blocks.1.attn.weight_mask_W_O', 'blocks.1.mlp.weight_mask_W_in', 'blocks.1.mlp.weight_mask_W_out', 'blocks.1.mlp.weight_mask_b_in', 'blocks.1.mlp.weight_mask_b_out', 'blocks.2.attn.weight_mask_W_Q', 'blocks.2.attn.weight_mask_W_K', 'blocks.2.attn.weight_mask_W_V', 'blocks.2.attn.weight_mask_W_O', 'blocks.2.mlp.weight_mask_W_in', 'blocks.2.mlp.weight_mask_W_out', 'blocks.2.mlp.weight_mask_b_in', 'blocks.2.mlp.weight_mask_b_out', 'blocks.3.attn.weight_mask_W_Q', 'blocks.3.attn.weight_mask_W_K', 'blocks.3.attn.weight_mask_W_V', 'blocks.3.attn.weight_mask_W_O', 'blocks.3.mlp.weight_mask_W_in', 'blocks.3.mlp.weight_mask_W_out', 'blocks.3.mlp.weight_mask_b_in', 'blocks.3.mlp.weight_mask_b_out', 'blocks.4.attn.weight_mask_W_Q', 'blocks.4.attn.weight_mask_W_K', 'blocks.4.attn.weight_mask_W_V', 'blocks.4.attn.weight_mask_W_O', 'blocks.4.mlp.weight_mask_W_in', 'blocks.4.mlp.weight_mask_W_out', 'blocks.4.mlp.weight_mask_b_in', 'blocks.4.mlp.weight_mask_b_out', 'blocks.5.attn.weight_mask_W_Q', 'blocks.5.attn.weight_mask_W_K', 'blocks.5.attn.weight_mask_W_V', 'blocks.5.attn.weight_mask_W_O', 'blocks.5.mlp.weight_mask_W_in', 'blocks.5.mlp.weight_mask_W_out', 'blocks.5.mlp.weight_mask_b_in', 'blocks.5.mlp.weight_mask_b_out', 'blocks.6.attn.weight_mask_W_Q', 'blocks.6.attn.weight_mask_W_K', 'blocks.6.attn.weight_mask_W_V', 'blocks.6.attn.weight_mask_W_O', 'blocks.6.mlp.weight_mask_W_in', 'blocks.6.mlp.weight_mask_W_out', 'blocks.6.mlp.weight_mask_b_in', 'blocks.6.mlp.weight_mask_b_out', 'blocks.7.attn.weight_mask_W_Q', 'blocks.7.attn.weight_mask_W_K', 'blocks.7.attn.weight_mask_W_V', 'blocks.7.attn.weight_mask_W_O', 'blocks.7.mlp.weight_mask_W_in', 'blocks.7.mlp.weight_mask_W_out', 'blocks.7.mlp.weight_mask_b_in', 'blocks.7.mlp.weight_mask_b_out', 'blocks.8.attn.weight_mask_W_Q', 'blocks.8.attn.weight_mask_W_K', 'blocks.8.attn.weight_mask_W_V', 'blocks.8.attn.weight_mask_W_O', 'blocks.8.mlp.weight_mask_W_in', 'blocks.8.mlp.weight_mask_W_out', 'blocks.8.mlp.weight_mask_b_in', 'blocks.8.mlp.weight_mask_b_out', 'blocks.9.attn.weight_mask_W_Q', 'blocks.9.attn.weight_mask_W_K', 'blocks.9.attn.weight_mask_W_V', 'blocks.9.attn.weight_mask_W_O', 'blocks.9.mlp.weight_mask_W_in', 'blocks.9.mlp.weight_mask_W_out', 'blocks.9.mlp.weight_mask_b_in', 'blocks.9.mlp.weight_mask_b_out', 'blocks.10.attn.weight_mask_W_Q', 'blocks.10.attn.weight_mask_W_K', 'blocks.10.attn.weight_mask_W_V', 'blocks.10.attn.weight_mask_W_O', 'blocks.10.mlp.weight_mask_W_in', 'blocks.10.mlp.weight_mask_W_out', 'blocks.10.mlp.weight_mask_b_in', 'blocks.10.mlp.weight_mask_b_out', 'blocks.11.attn.weight_mask_W_Q', 'blocks.11.attn.weight_mask_W_K', 'blocks.11.attn.weight_mask_W_V', 'blocks.11.attn.weight_mask_W_O', 'blocks.11.mlp.weight_mask_W_in', 'blocks.11.mlp.weight_mask_W_out', 'blocks.11.mlp.weight_mask_b_in', 'blocks.11.mlp.weight_mask_b_out']\n"
     ]
    }
   ],
   "source": [
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "\n",
    "print(param_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Mask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale_reg_strength:\n",
    "    orig_edge_mask_reg_strength = edge_mask_reg_strength\n",
    "    orig_weight_mask_reg_strength = weight_mask_reg_strength\n",
    "    edge_mask_reg_strength = lambda epoch: orig_edge_mask_reg_strength * (epoch - 1/10*epochs_left)\n",
    "    weight_mask_reg_strength = lambda epoch: orig_weight_mask_reg_strength * (epoch - 1/10*epochs_left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84980736.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84936256.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84905408.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84879112.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84854160.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84830840.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84808896.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84788224.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84768720.0, tot_weight_params: 84980736\n",
      "Epoch 0, step 9\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84750328.0, tot_weight_params: 84980736\n",
      "discretizeing edges and weights\n",
      "Number of ablated edges: 0\n",
      "Number of ablated weights: 0\n",
      "Epoch 0, step 9: train loss 15.369577884674072\n",
      "num_ablated_edges=0, num_ablated_weights=0\n",
      "Evaluating on ioi\n",
      "Loss on ioi: 0.955302357673645\n",
      "Evaluating on induction\n",
      "Loss on induction: 0.607153058052063\n",
      "Evaluating on owt\n",
      "Loss on owt: 3.6509101390838623\n",
      "Evaluating on ioi_2\n",
      "Loss on ioi_2: 0.3115852177143097\n",
      "Evaluating on ioi_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/21 [00:09<03:05,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on ioi_3: 0.7931265234947205\n",
      "Evaluating on greaterthan\n",
      "Loss on greaterthan: 3.911612033843994\n",
      "Epoch 1, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84980736.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84960640.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84941040.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84920872.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84899808.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84878080.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84856280.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84834688.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84813592.0, tot_weight_params: 84980736\n",
      "Epoch 1, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:15<02:27,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84793568.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84774800.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84757136.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84740464.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84724768.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84709936.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84695832.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84682400.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84669592.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84657320.0, tot_weight_params: 84980736\n",
      "Epoch 2, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [00:22<02:12,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84645568.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84634320.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84623608.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84613480.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84603952.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84594928.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84586416.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84578368.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84570800.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84563640.0, tot_weight_params: 84980736\n",
      "Epoch 3, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4/21 [00:29<02:00,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84556880.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84550512.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84544624.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84539160.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84534128.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84529464.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84525112.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84521008.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84517184.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84513608.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 9\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84510264.0, tot_weight_params: 84980736\n",
      "Epoch 4, step 9: train loss -37.46662998199463\n",
      "num_ablated_edges=0, num_ablated_weights=0\n",
      "Evaluating on ioi\n",
      "Loss on ioi: 0.9509758949279785\n",
      "Evaluating on induction\n",
      "Loss on induction: 0.6960806250572205\n",
      "Evaluating on owt\n",
      "Loss on owt: 3.8793866634368896\n",
      "Evaluating on ioi_2\n",
      "Loss on ioi_2: 0.3487468957901001\n",
      "Evaluating on ioi_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [00:37<01:56,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on ioi_3: 0.7243724465370178\n",
      "Evaluating on greaterthan\n",
      "Loss on greaterthan: 3.911612033843994\n",
      "Epoch 5, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84507120.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84504216.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84501544.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84499040.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84496688.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84494488.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84492432.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84490480.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84488632.0, tot_weight_params: 84980736\n",
      "Epoch 5, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [00:43<01:44,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84486896.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84485264.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84483752.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84482496.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84481296.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84480208.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84479232.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84478304.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84477440.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84476752.0, tot_weight_params: 84980736\n",
      "Epoch 6, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7/21 [00:49<01:34,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84475984.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84475424.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84474888.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84474536.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84474208.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473992.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473672.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473616.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473552.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473568.0, tot_weight_params: 84980736\n",
      "Epoch 7, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8/21 [00:56<01:26,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473632.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473656.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84473824.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84474048.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84474352.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84474704.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84475088.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84475528.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84476032.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84476528.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 9\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84477064.0, tot_weight_params: 84980736\n",
      "Epoch 8, step 9: train loss -79.94897842407227\n",
      "num_ablated_edges=0, num_ablated_weights=19\n",
      "Evaluating on ioi\n",
      "Loss on ioi: 0.915966808795929\n",
      "Evaluating on induction\n",
      "Loss on induction: 0.6163302659988403\n",
      "Evaluating on owt\n",
      "Loss on owt: 3.8838658332824707\n",
      "Evaluating on ioi_2\n",
      "Loss on ioi_2: 0.33299458026885986\n",
      "Evaluating on ioi_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9/21 [01:03<01:23,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on ioi_3: 0.7887482047080994\n",
      "Evaluating on greaterthan\n",
      "Loss on greaterthan: 3.911648988723755\n",
      "Epoch 9, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84477608.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84478208.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84478864.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84479552.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84480248.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84481000.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84481824.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84482648.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84483496.0, tot_weight_params: 84980736\n",
      "Epoch 9, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10/21 [01:10<01:14,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84484344.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84485208.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84486104.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84487032.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84488032.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84489040.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84490048.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84491104.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84492192.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84493288.0, tot_weight_params: 84980736\n",
      "Epoch 10, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [01:16<01:06,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84494344.0, tot_weight_params: 84980736\n",
      "discretizeing edges and weights\n",
      "Number of ablated edges: 0\n",
      "Number of ablated weights: 2243\n",
      "Epoch 11, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84978488.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84976120.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84972896.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84968480.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84962576.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84955424.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84947424.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84938904.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84930064.0, tot_weight_params: 84980736\n",
      "Epoch 11, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12/21 [01:23<00:59,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84921040.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84911944.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84902840.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84893696.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84884408.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84875064.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84865656.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84856072.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84846040.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84835808.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 9\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84825360.0, tot_weight_params: 84980736\n",
      "Epoch 12, step 9: train loss -103.35669374465942\n",
      "num_ablated_edges=0, num_ablated_weights=2243\n",
      "Evaluating on ioi\n",
      "Loss on ioi: 1.0047001838684082\n",
      "Evaluating on induction\n",
      "Loss on induction: 0.42489805817604065\n",
      "Evaluating on owt\n",
      "Loss on owt: 3.879051446914673\n",
      "Evaluating on ioi_2\n",
      "Loss on ioi_2: 0.33924025297164917\n",
      "Evaluating on ioi_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 13/21 [01:30<00:55,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on ioi_3: 0.8537848591804504\n",
      "Evaluating on greaterthan\n",
      "Loss on greaterthan: 3.911863088607788\n",
      "Epoch 13, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84814864.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84804640.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84794760.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84784864.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84775040.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84765344.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84755600.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84745840.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84736488.0, tot_weight_params: 84980736\n",
      "Epoch 13, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [01:36<00:46,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84727424.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84718664.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84710336.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84702480.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84695056.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84688120.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84681672.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84675712.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84670192.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84665112.0, tot_weight_params: 84980736\n",
      "Epoch 14, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 15/21 [01:43<00:39,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84660472.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84656176.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84652224.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84648632.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84645448.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84642608.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84640064.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84637848.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84635864.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84634096.0, tot_weight_params: 84980736\n",
      "Epoch 15, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 16/21 [01:49<00:32,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84632536.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84631208.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84630088.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84629192.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84628456.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627928.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627472.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627288.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627232.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627272.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 9\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627344.0, tot_weight_params: 84980736\n",
      "Epoch 16, step 9: train loss -156.8045961856842\n",
      "num_ablated_edges=0, num_ablated_weights=2250\n",
      "Evaluating on ioi\n",
      "Loss on ioi: 0.9737231135368347\n",
      "Evaluating on induction\n",
      "Loss on induction: 0.6956977248191833\n",
      "Evaluating on owt\n",
      "Loss on owt: 3.7059144973754883\n",
      "Evaluating on ioi_2\n",
      "Loss on ioi_2: 0.33285820484161377\n",
      "Evaluating on ioi_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17/21 [01:57<00:27,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on ioi_3: 0.7934633493423462\n",
      "Evaluating on greaterthan\n",
      "Loss on greaterthan: 3.9118459224700928\n",
      "Epoch 17, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627552.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84627832.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84628224.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84628768.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84629344.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84630008.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84630712.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84631512.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84632328.0, tot_weight_params: 84980736\n",
      "Epoch 17, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 18/21 [02:04<00:20,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84633224.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84634120.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84635112.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84636136.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84637168.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84638256.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84639384.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84640544.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84641696.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84642856.0, tot_weight_params: 84980736\n",
      "Epoch 18, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19/21 [02:11<00:13,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84644032.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84645248.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84646472.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84647704.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84648936.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84650184.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84651416.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84652648.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84653872.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84655088.0, tot_weight_params: 84980736\n",
      "Epoch 19, step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [02:17<00:06,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84656288.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 0\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84657472.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 1\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84658688.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 2\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84659896.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 3\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84661088.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 4\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84662256.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 5\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84663416.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 6\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84664592.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 7\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84665768.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 8\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84666912.0, tot_weight_params: 84980736\n",
      "Epoch 20, step 9\n",
      "Calculating weight_reg\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "self.attn.weight_mask=True, self.attn.mask_heads=None, self.mlp.weight_mask=True\n",
      "weight_reg_term: 84668080.0, tot_weight_params: 84980736\n",
      "discretizeing edges and weights\n",
      "Number of ablated edges: 0\n",
      "Number of ablated weights: 5016\n",
      "Epoch 20, step 9: train loss -197.30638122558594\n",
      "num_ablated_edges=0, num_ablated_weights=5016\n",
      "Evaluating on ioi\n",
      "Loss on ioi: 0.948996365070343\n",
      "Evaluating on induction\n",
      "Loss on induction: 0.6241248846054077\n",
      "Evaluating on owt\n",
      "Loss on owt: 3.6976571083068848\n",
      "Evaluating on ioi_2\n",
      "Loss on ioi_2: 0.3342812657356262\n",
      "Evaluating on ioi_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [02:25<00:00,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on ioi_3: 0.85821932554245\n",
      "Evaluating on greaterthan\n",
      "Loss on greaterthan: 3.9120635986328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.learn_mask import *\n",
    "wandb_config = config\n",
    "use_wandb = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "train_losses, test_losses = train_masks(model, \n",
    "                                        tasks=train_tasks, \n",
    "                                        optimizer=optimizer, \n",
    "                                        num_epochs=epochs_left, \n",
    "\n",
    "                                        steps_per_epoch=steps_per_epoch, \n",
    "                                        accum_grad_steps=accum_grad_steps,\n",
    "\n",
    "                                        task_weights=task_weights, \n",
    "                                        eval_tasks=eval_tasks, \n",
    "\n",
    "                                        evaluate_every=evaluate_every, \n",
    "                                        discretize_every=discretize_every, \n",
    "                                        save_every=save_every,\n",
    "\n",
    "                                        threshold=threshold, \n",
    "                                        mask_k=mask_k,\n",
    "                                        edge_mask_reg_strength=edge_mask_reg_strength, \n",
    "                                        weight_mask_reg_strength=weight_mask_reg_strength, \n",
    "\n",
    "                                        verbose=True, \n",
    "                                        use_wandb=use_wandb, \n",
    "                                        wandb_config=wandb_config, \n",
    "                                        save_dir=save_path, \n",
    "\n",
    "                                        save_efficient=save_efficient, \n",
    "                                        refresh_memory=use_pythia) # only refresh memory is pythia is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ioi': [(0, 0, 0.8509278297424316),\n",
       "              (0, 1, 5.451411247253418),\n",
       "              (0, 2, 11.070261001586914),\n",
       "              (0, 3, 15.22369384765625),\n",
       "              (0, 4, 20.594860076904297),\n",
       "              (0, 5, 26.3444766998291),\n",
       "              (0, 6, 29.318954467773438),\n",
       "              (0, 7, 35.21002197265625),\n",
       "              (0, 8, 37.39036560058594),\n",
       "              (0, 9, 41.46675109863281),\n",
       "              (1, 0, 0.9649065136909485),\n",
       "              (1, 1, 1.9460601806640625),\n",
       "              (1, 2, 5.629096508026123),\n",
       "              (1, 3, 10.17425537109375),\n",
       "              (1, 4, 17.89014434814453),\n",
       "              (1, 5, 28.62545394897461),\n",
       "              (1, 6, 36.456703186035156),\n",
       "              (1, 7, 43.86286926269531),\n",
       "              (1, 8, 48.4648323059082),\n",
       "              (1, 9, 51.050193786621094),\n",
       "              (2, 0, 53.51465606689453),\n",
       "              (2, 1, 55.42345428466797),\n",
       "              (2, 2, 60.24665069580078),\n",
       "              (2, 3, 65.27533721923828),\n",
       "              (2, 4, 67.26568603515625),\n",
       "              (2, 5, 70.9899673461914),\n",
       "              (2, 6, 73.35135650634766),\n",
       "              (2, 7, 75.95211029052734),\n",
       "              (2, 8, 79.0306625366211),\n",
       "              (2, 9, 81.30239868164062),\n",
       "              (3, 0, 82.98399353027344),\n",
       "              (3, 1, 85.52350616455078),\n",
       "              (3, 2, 87.54450225830078),\n",
       "              (3, 3, 89.4703369140625),\n",
       "              (3, 4, 90.63444519042969),\n",
       "              (3, 5, 92.08424377441406),\n",
       "              (3, 6, 92.65105438232422),\n",
       "              (3, 7, 97.90644073486328),\n",
       "              (3, 8, 93.22685241699219),\n",
       "              (3, 9, 98.41725158691406),\n",
       "              (4, 0, 99.61339569091797),\n",
       "              (4, 1, 102.21554565429688),\n",
       "              (4, 2, 99.0682601928711),\n",
       "              (4, 3, 102.89107513427734),\n",
       "              (4, 4, 102.50770568847656),\n",
       "              (4, 5, 100.7364501953125),\n",
       "              (4, 6, 104.05220794677734),\n",
       "              (4, 7, 106.8254623413086),\n",
       "              (4, 8, 107.16009521484375),\n",
       "              (4, 9, 105.94816589355469),\n",
       "              (5, 0, 107.88082122802734),\n",
       "              (5, 1, 107.97380065917969),\n",
       "              (5, 2, 109.83780670166016),\n",
       "              (5, 3, 110.87091064453125),\n",
       "              (5, 4, 110.88761138916016),\n",
       "              (5, 5, 110.63819885253906),\n",
       "              (5, 6, 112.10821533203125),\n",
       "              (5, 7, 111.51377868652344),\n",
       "              (5, 8, 112.2493667602539),\n",
       "              (5, 9, 110.57008361816406),\n",
       "              (6, 0, 114.45462799072266),\n",
       "              (6, 1, 112.32527923583984),\n",
       "              (6, 2, 114.0209732055664),\n",
       "              (6, 3, 114.6478271484375),\n",
       "              (6, 4, 111.31800842285156),\n",
       "              (6, 5, 117.45277404785156),\n",
       "              (6, 6, 114.13340759277344),\n",
       "              (6, 7, 115.46708679199219),\n",
       "              (6, 8, 115.95054626464844),\n",
       "              (6, 9, 115.69950103759766),\n",
       "              (7, 0, 115.07121276855469),\n",
       "              (7, 1, 115.4170150756836),\n",
       "              (7, 2, 116.26762390136719),\n",
       "              (7, 3, 115.92362976074219),\n",
       "              (7, 4, 117.28338623046875),\n",
       "              (7, 5, 118.2503662109375),\n",
       "              (7, 6, 119.0845947265625),\n",
       "              (7, 7, 119.76316833496094),\n",
       "              (7, 8, 117.57331848144531),\n",
       "              (7, 9, 119.10115814208984),\n",
       "              (8, 0, 118.23348236083984),\n",
       "              (8, 1, 119.0155258178711),\n",
       "              (8, 2, 118.74958801269531),\n",
       "              (8, 3, 119.9427261352539),\n",
       "              (8, 4, 119.77598571777344),\n",
       "              (8, 5, 120.3324203491211),\n",
       "              (8, 6, 120.66304779052734),\n",
       "              (8, 7, 120.60591125488281),\n",
       "              (8, 8, 119.92315673828125),\n",
       "              (8, 9, 118.94883728027344),\n",
       "              (9, 0, 119.31341552734375),\n",
       "              (9, 1, 121.5128173828125),\n",
       "              (9, 2, 122.06390380859375),\n",
       "              (9, 3, 120.20710754394531),\n",
       "              (9, 4, 122.71183013916016),\n",
       "              (9, 5, 119.9947280883789),\n",
       "              (9, 6, 122.51168060302734),\n",
       "              (9, 7, 122.26268005371094),\n",
       "              (9, 8, 119.65196228027344),\n",
       "              (9, 9, 119.5231704711914),\n",
       "              (10, 0, 117.83646392822266),\n",
       "              (10, 1, 123.1507339477539),\n",
       "              (10, 2, 124.61741638183594),\n",
       "              (10, 3, 121.020751953125),\n",
       "              (10, 4, 122.2694320678711),\n",
       "              (10, 5, 122.79640197753906),\n",
       "              (10, 6, 120.03006744384766),\n",
       "              (10, 7, 123.3009033203125),\n",
       "              (10, 8, 121.39021301269531),\n",
       "              (10, 9, 122.0833511352539),\n",
       "              (11, 0, 1.172997236251831),\n",
       "              (11, 1, 1.2933926582336426),\n",
       "              (11, 2, 2.0015578269958496),\n",
       "              (11, 3, 3.1826388835906982),\n",
       "              (11, 4, 5.091202259063721),\n",
       "              (11, 5, 6.696618556976318),\n",
       "              (11, 6, 8.306882858276367),\n",
       "              (11, 7, 10.158394813537598),\n",
       "              (11, 8, 11.966938018798828),\n",
       "              (11, 9, 13.06689167022705),\n",
       "              (12, 0, 14.308778762817383),\n",
       "              (12, 1, 16.072879791259766),\n",
       "              (12, 2, 18.52598762512207),\n",
       "              (12, 3, 19.144914627075195),\n",
       "              (12, 4, 22.449951171875),\n",
       "              (12, 5, 24.22350311279297),\n",
       "              (12, 6, 26.39823341369629),\n",
       "              (12, 7, 32.11668014526367),\n",
       "              (12, 8, 34.361053466796875),\n",
       "              (12, 9, 36.204750061035156),\n",
       "              (13, 0, 39.89756774902344),\n",
       "              (13, 1, 43.22533416748047),\n",
       "              (13, 2, 46.64060974121094),\n",
       "              (13, 3, 51.53446578979492),\n",
       "              (13, 4, 54.21555709838867),\n",
       "              (13, 5, 59.33457565307617),\n",
       "              (13, 6, 64.37145233154297),\n",
       "              (13, 7, 69.08670806884766),\n",
       "              (13, 8, 72.44017028808594),\n",
       "              (13, 9, 75.18675994873047),\n",
       "              (14, 0, 77.76467895507812),\n",
       "              (14, 1, 79.65169525146484),\n",
       "              (14, 2, 81.67472839355469),\n",
       "              (14, 3, 83.85801696777344),\n",
       "              (14, 4, 85.38859558105469),\n",
       "              (14, 5, 87.07441711425781),\n",
       "              (14, 6, 88.95232391357422),\n",
       "              (14, 7, 91.79021453857422),\n",
       "              (14, 8, 91.05232238769531),\n",
       "              (14, 9, 94.40955352783203),\n",
       "              (15, 0, 94.5870361328125),\n",
       "              (15, 1, 93.49977111816406),\n",
       "              (15, 2, 96.38920593261719),\n",
       "              (15, 3, 95.74772644042969),\n",
       "              (15, 4, 99.46577453613281),\n",
       "              (15, 5, 98.16822052001953),\n",
       "              (15, 6, 99.49678039550781),\n",
       "              (15, 7, 98.6025619506836),\n",
       "              (15, 8, 98.91712188720703),\n",
       "              (15, 9, 100.23519897460938),\n",
       "              (16, 0, 102.9022445678711),\n",
       "              (16, 1, 101.09593200683594),\n",
       "              (16, 2, 101.83915710449219),\n",
       "              (16, 3, 101.77568054199219),\n",
       "              (16, 4, 101.4686279296875),\n",
       "              (16, 5, 100.44989013671875),\n",
       "              (16, 6, 101.4730224609375),\n",
       "              (16, 7, 105.99066162109375),\n",
       "              (16, 8, 103.89642333984375),\n",
       "              (16, 9, 105.1005859375),\n",
       "              (17, 0, 103.47611999511719),\n",
       "              (17, 1, 105.31400299072266),\n",
       "              (17, 2, 103.0230941772461),\n",
       "              (17, 3, 104.0268325805664),\n",
       "              (17, 4, 105.27476501464844),\n",
       "              (17, 5, 105.39555358886719),\n",
       "              (17, 6, 107.1229248046875),\n",
       "              (17, 7, 106.2790298461914),\n",
       "              (17, 8, 105.84029388427734),\n",
       "              (17, 9, 106.0697021484375),\n",
       "              (18, 0, 106.3194808959961),\n",
       "              (18, 1, 105.94526672363281),\n",
       "              (18, 2, 107.70753479003906),\n",
       "              (18, 3, 107.3497543334961),\n",
       "              (18, 4, 104.46507263183594),\n",
       "              (18, 5, 106.09232330322266),\n",
       "              (18, 6, 104.57041931152344),\n",
       "              (18, 7, 107.79191589355469),\n",
       "              (18, 8, 108.38093566894531),\n",
       "              (18, 9, 106.97172546386719),\n",
       "              (19, 0, 108.87569427490234),\n",
       "              (19, 1, 107.69639587402344),\n",
       "              (19, 2, 104.50251770019531),\n",
       "              (19, 3, 105.3896713256836),\n",
       "              (19, 4, 108.67369079589844),\n",
       "              (19, 5, 108.67926025390625),\n",
       "              (19, 6, 107.82823181152344),\n",
       "              (19, 7, 106.35392761230469),\n",
       "              (19, 8, 107.9680404663086),\n",
       "              (19, 9, 108.81718444824219),\n",
       "              (20, 0, 109.2010498046875),\n",
       "              (20, 1, 107.06793212890625),\n",
       "              (20, 2, 106.21900939941406),\n",
       "              (20, 3, 108.26541900634766),\n",
       "              (20, 4, 108.57466888427734),\n",
       "              (20, 5, 108.54178619384766),\n",
       "              (20, 6, 108.3316650390625),\n",
       "              (20, 7, 110.31486511230469),\n",
       "              (20, 8, 108.4168930053711),\n",
       "              (20, 9, 107.68211364746094)],\n",
       "             'owt': [(0, 0, 3.6509106159210205),\n",
       "              (0, 1, 3.8514912128448486),\n",
       "              (0, 2, 3.85174560546875),\n",
       "              (0, 3, 3.835482358932495),\n",
       "              (0, 4, 3.666090488433838),\n",
       "              (0, 5, 3.64482045173645),\n",
       "              (0, 6, 3.7072997093200684),\n",
       "              (0, 7, 3.6794016361236572),\n",
       "              (0, 8, 3.8356902599334717),\n",
       "              (0, 9, 3.717154026031494),\n",
       "              (1, 0, 3.8014752864837646),\n",
       "              (1, 1, 3.6997792720794678),\n",
       "              (1, 2, 3.567599058151245),\n",
       "              (1, 3, 3.7733054161071777),\n",
       "              (1, 4, 3.7525453567504883),\n",
       "              (1, 5, 3.603437662124634),\n",
       "              (1, 6, 3.700308084487915),\n",
       "              (1, 7, 3.652238368988037),\n",
       "              (1, 8, 3.778165102005005),\n",
       "              (1, 9, 3.7222039699554443),\n",
       "              (2, 0, 3.7554738521575928),\n",
       "              (2, 1, 3.813161611557007),\n",
       "              (2, 2, 3.5536048412323),\n",
       "              (2, 3, 3.7179248332977295),\n",
       "              (2, 4, 3.7942817211151123),\n",
       "              (2, 5, 3.7117581367492676),\n",
       "              (2, 6, 3.7135350704193115),\n",
       "              (2, 7, 3.6928699016571045),\n",
       "              (2, 8, 3.5715293884277344),\n",
       "              (2, 9, 3.662344455718994),\n",
       "              (3, 0, 3.7211756706237793),\n",
       "              (3, 1, 3.601947784423828),\n",
       "              (3, 2, 3.661665439605713),\n",
       "              (3, 3, 3.5909180641174316),\n",
       "              (3, 4, 3.722132682800293),\n",
       "              (3, 5, 3.534585952758789),\n",
       "              (3, 6, 3.6211817264556885),\n",
       "              (3, 7, 3.5204918384552),\n",
       "              (3, 8, 3.654445171356201),\n",
       "              (3, 9, 3.5836615562438965),\n",
       "              (4, 0, 3.604792356491089),\n",
       "              (4, 1, 3.6732304096221924),\n",
       "              (4, 2, 3.547605276107788),\n",
       "              (4, 3, 3.594543218612671),\n",
       "              (4, 4, 3.7003276348114014),\n",
       "              (4, 5, 3.622776508331299),\n",
       "              (4, 6, 3.539886713027954),\n",
       "              (4, 7, 3.592108726501465),\n",
       "              (4, 8, 3.641725778579712),\n",
       "              (4, 9, 3.61228084564209),\n",
       "              (5, 0, 3.567301034927368),\n",
       "              (5, 1, 3.624953031539917),\n",
       "              (5, 2, 3.643548011779785),\n",
       "              (5, 3, 3.6261985301971436),\n",
       "              (5, 4, 3.625943422317505),\n",
       "              (5, 5, 3.678250312805176),\n",
       "              (5, 6, 3.5874249935150146),\n",
       "              (5, 7, 3.488041877746582),\n",
       "              (5, 8, 3.616021156311035),\n",
       "              (5, 9, 3.610114097595215),\n",
       "              (6, 0, 3.5446619987487793),\n",
       "              (6, 1, 3.628986120223999),\n",
       "              (6, 2, 3.4626505374908447),\n",
       "              (6, 3, 3.761946678161621),\n",
       "              (6, 4, 3.5981202125549316),\n",
       "              (6, 5, 3.5541932582855225),\n",
       "              (6, 6, 3.7011828422546387),\n",
       "              (6, 7, 3.688443422317505),\n",
       "              (6, 8, 3.4105334281921387),\n",
       "              (6, 9, 3.5734736919403076),\n",
       "              (7, 0, 3.545351028442383),\n",
       "              (7, 1, 3.5301005840301514),\n",
       "              (7, 2, 3.5816502571105957),\n",
       "              (7, 3, 3.5430378913879395),\n",
       "              (7, 4, 3.507951498031616),\n",
       "              (7, 5, 3.4405367374420166),\n",
       "              (7, 6, 3.5728132724761963),\n",
       "              (7, 7, 3.5873844623565674),\n",
       "              (7, 8, 3.618359327316284),\n",
       "              (7, 9, 3.464031457901001),\n",
       "              (8, 0, 3.5371792316436768),\n",
       "              (8, 1, 3.56896710395813),\n",
       "              (8, 2, 3.5737128257751465),\n",
       "              (8, 3, 3.480283498764038),\n",
       "              (8, 4, 3.4953393936157227),\n",
       "              (8, 5, 3.5750505924224854),\n",
       "              (8, 6, 3.5009255409240723),\n",
       "              (8, 7, 3.563366413116455),\n",
       "              (8, 8, 3.7660093307495117),\n",
       "              (8, 9, 3.4851760864257812),\n",
       "              (9, 0, 3.6358790397644043),\n",
       "              (9, 1, 3.5447165966033936),\n",
       "              (9, 2, 3.54494571685791),\n",
       "              (9, 3, 3.414578437805176),\n",
       "              (9, 4, 3.3842482566833496),\n",
       "              (9, 5, 3.5101242065429688),\n",
       "              (9, 6, 3.412728786468506),\n",
       "              (9, 7, 3.421416759490967),\n",
       "              (9, 8, 3.6607956886291504),\n",
       "              (9, 9, 3.454819917678833),\n",
       "              (10, 0, 3.4734904766082764),\n",
       "              (10, 1, 3.6084964275360107),\n",
       "              (10, 2, 3.6147842407226562),\n",
       "              (10, 3, 3.520490884780884),\n",
       "              (10, 4, 3.4762446880340576),\n",
       "              (10, 5, 3.4688029289245605),\n",
       "              (10, 6, 3.6934163570404053),\n",
       "              (10, 7, 3.487165927886963),\n",
       "              (10, 8, 3.4649088382720947),\n",
       "              (10, 9, 3.4923014640808105),\n",
       "              (11, 0, 3.6169257164001465),\n",
       "              (11, 1, 3.755563497543335),\n",
       "              (11, 2, 3.7424495220184326),\n",
       "              (11, 3, 3.817634105682373),\n",
       "              (11, 4, 3.7425780296325684),\n",
       "              (11, 5, 3.7284114360809326),\n",
       "              (11, 6, 3.6937649250030518),\n",
       "              (11, 7, 3.703867197036743),\n",
       "              (11, 8, 3.677205801010132),\n",
       "              (11, 9, 3.6345977783203125),\n",
       "              (12, 0, 3.8234710693359375),\n",
       "              (12, 1, 3.47298526763916),\n",
       "              (12, 2, 3.65769362449646),\n",
       "              (12, 3, 3.7177574634552),\n",
       "              (12, 4, 3.7140719890594482),\n",
       "              (12, 5, 3.706446886062622),\n",
       "              (12, 6, 3.724496603012085),\n",
       "              (12, 7, 3.6845521926879883),\n",
       "              (12, 8, 3.641753673553467),\n",
       "              (12, 9, 3.701417922973633),\n",
       "              (13, 0, 3.6701526641845703),\n",
       "              (13, 1, 3.7426488399505615),\n",
       "              (13, 2, 3.8052775859832764),\n",
       "              (13, 3, 3.7827510833740234),\n",
       "              (13, 4, 3.735633611679077),\n",
       "              (13, 5, 3.9227404594421387),\n",
       "              (13, 6, 3.7647805213928223),\n",
       "              (13, 7, 3.710714340209961),\n",
       "              (13, 8, 3.7188501358032227),\n",
       "              (13, 9, 3.6859641075134277),\n",
       "              (14, 0, 3.607316017150879),\n",
       "              (14, 1, 3.6814117431640625),\n",
       "              (14, 2, 3.855365753173828),\n",
       "              (14, 3, 3.8109307289123535),\n",
       "              (14, 4, 3.514910936355591),\n",
       "              (14, 5, 3.600137710571289),\n",
       "              (14, 6, 3.701460361480713),\n",
       "              (14, 7, 3.6030657291412354),\n",
       "              (14, 8, 3.5492305755615234),\n",
       "              (14, 9, 3.7477524280548096),\n",
       "              (15, 0, 3.473666191101074),\n",
       "              (15, 1, 3.458649158477783),\n",
       "              (15, 2, 3.733227014541626),\n",
       "              (15, 3, 3.6608047485351562),\n",
       "              (15, 4, 3.6639211177825928),\n",
       "              (15, 5, 3.5781850814819336),\n",
       "              (15, 6, 3.6116883754730225),\n",
       "              (15, 7, 3.5565695762634277),\n",
       "              (15, 8, 3.8472018241882324),\n",
       "              (15, 9, 3.629319429397583),\n",
       "              (16, 0, 3.721383571624756),\n",
       "              (16, 1, 3.5530924797058105),\n",
       "              (16, 2, 3.488434076309204),\n",
       "              (16, 3, 3.666757345199585),\n",
       "              (16, 4, 3.586801767349243),\n",
       "              (16, 5, 3.6132915019989014),\n",
       "              (16, 6, 3.733018398284912),\n",
       "              (16, 7, 3.4506657123565674),\n",
       "              (16, 8, 3.4943346977233887),\n",
       "              (16, 9, 3.6333367824554443),\n",
       "              (17, 0, 3.671893358230591),\n",
       "              (17, 1, 3.523721694946289),\n",
       "              (17, 2, 3.6483967304229736),\n",
       "              (17, 3, 3.5094125270843506),\n",
       "              (17, 4, 3.4730963706970215),\n",
       "              (17, 5, 3.553290843963623),\n",
       "              (17, 6, 3.60674786567688),\n",
       "              (17, 7, 3.595402717590332),\n",
       "              (17, 8, 3.3881564140319824),\n",
       "              (17, 9, 3.497856855392456),\n",
       "              (18, 0, 3.4774692058563232),\n",
       "              (18, 1, 3.4833600521087646),\n",
       "              (18, 2, 3.3483643531799316),\n",
       "              (18, 3, 3.5042521953582764),\n",
       "              (18, 4, 3.586439847946167),\n",
       "              (18, 5, 3.630742311477661),\n",
       "              (18, 6, 3.4611687660217285),\n",
       "              (18, 7, 3.414340019226074),\n",
       "              (18, 8, 3.3980631828308105),\n",
       "              (18, 9, 3.660595417022705),\n",
       "              (19, 0, 3.683945655822754),\n",
       "              (19, 1, 3.542941093444824),\n",
       "              (19, 2, 3.5102789402008057),\n",
       "              (19, 3, 3.4239442348480225),\n",
       "              (19, 4, 3.535161256790161),\n",
       "              (19, 5, 3.482250690460205),\n",
       "              (19, 6, 3.5085620880126953),\n",
       "              (19, 7, 3.5346691608428955),\n",
       "              (19, 8, 3.567312479019165),\n",
       "              (19, 9, 3.571443796157837),\n",
       "              (20, 0, 3.5657155513763428),\n",
       "              (20, 1, 3.446763277053833),\n",
       "              (20, 2, 3.417144298553467),\n",
       "              (20, 3, 3.5141735076904297),\n",
       "              (20, 4, 3.457935094833374),\n",
       "              (20, 5, 3.444899082183838),\n",
       "              (20, 6, 3.3945212364196777),\n",
       "              (20, 7, 3.511418342590332),\n",
       "              (20, 8, 3.347546100616455),\n",
       "              (20, 9, 3.5677947998046875)],\n",
       "             'edge_reg_term': [(0, 0, 0),\n",
       "              (0, 1, 0),\n",
       "              (0, 2, 0),\n",
       "              (0, 3, 0),\n",
       "              (0, 4, 0),\n",
       "              (0, 5, 0),\n",
       "              (0, 6, 0),\n",
       "              (0, 7, 0),\n",
       "              (0, 8, 0),\n",
       "              (0, 9, 0),\n",
       "              (1, 0, 0),\n",
       "              (1, 1, 0),\n",
       "              (1, 2, 0),\n",
       "              (1, 3, 0),\n",
       "              (1, 4, 0),\n",
       "              (1, 5, 0),\n",
       "              (1, 6, 0),\n",
       "              (1, 7, 0),\n",
       "              (1, 8, 0),\n",
       "              (1, 9, 0),\n",
       "              (2, 0, 0),\n",
       "              (2, 1, 0),\n",
       "              (2, 2, 0),\n",
       "              (2, 3, 0),\n",
       "              (2, 4, 0),\n",
       "              (2, 5, 0),\n",
       "              (2, 6, 0),\n",
       "              (2, 7, 0),\n",
       "              (2, 8, 0),\n",
       "              (2, 9, 0),\n",
       "              (3, 0, 0),\n",
       "              (3, 1, 0),\n",
       "              (3, 2, 0),\n",
       "              (3, 3, 0),\n",
       "              (3, 4, 0),\n",
       "              (3, 5, 0),\n",
       "              (3, 6, 0),\n",
       "              (3, 7, 0),\n",
       "              (3, 8, 0),\n",
       "              (3, 9, 0),\n",
       "              (4, 0, 0),\n",
       "              (4, 1, 0),\n",
       "              (4, 2, 0),\n",
       "              (4, 3, 0),\n",
       "              (4, 4, 0),\n",
       "              (4, 5, 0),\n",
       "              (4, 6, 0),\n",
       "              (4, 7, 0),\n",
       "              (4, 8, 0),\n",
       "              (4, 9, 0),\n",
       "              (5, 0, 0),\n",
       "              (5, 1, 0),\n",
       "              (5, 2, 0),\n",
       "              (5, 3, 0),\n",
       "              (5, 4, 0),\n",
       "              (5, 5, 0),\n",
       "              (5, 6, 0),\n",
       "              (5, 7, 0),\n",
       "              (5, 8, 0),\n",
       "              (5, 9, 0),\n",
       "              (6, 0, 0),\n",
       "              (6, 1, 0),\n",
       "              (6, 2, 0),\n",
       "              (6, 3, 0),\n",
       "              (6, 4, 0),\n",
       "              (6, 5, 0),\n",
       "              (6, 6, 0),\n",
       "              (6, 7, 0),\n",
       "              (6, 8, 0),\n",
       "              (6, 9, 0),\n",
       "              (7, 0, 0),\n",
       "              (7, 1, 0),\n",
       "              (7, 2, 0),\n",
       "              (7, 3, 0),\n",
       "              (7, 4, 0),\n",
       "              (7, 5, 0),\n",
       "              (7, 6, 0),\n",
       "              (7, 7, 0),\n",
       "              (7, 8, 0),\n",
       "              (7, 9, 0),\n",
       "              (8, 0, 0),\n",
       "              (8, 1, 0),\n",
       "              (8, 2, 0),\n",
       "              (8, 3, 0),\n",
       "              (8, 4, 0),\n",
       "              (8, 5, 0),\n",
       "              (8, 6, 0),\n",
       "              (8, 7, 0),\n",
       "              (8, 8, 0),\n",
       "              (8, 9, 0),\n",
       "              (9, 0, 0),\n",
       "              (9, 1, 0),\n",
       "              (9, 2, 0),\n",
       "              (9, 3, 0),\n",
       "              (9, 4, 0),\n",
       "              (9, 5, 0),\n",
       "              (9, 6, 0),\n",
       "              (9, 7, 0),\n",
       "              (9, 8, 0),\n",
       "              (9, 9, 0),\n",
       "              (10, 0, 0),\n",
       "              (10, 1, 0),\n",
       "              (10, 2, 0),\n",
       "              (10, 3, 0),\n",
       "              (10, 4, 0),\n",
       "              (10, 5, 0),\n",
       "              (10, 6, 0),\n",
       "              (10, 7, 0),\n",
       "              (10, 8, 0),\n",
       "              (10, 9, 0),\n",
       "              (11, 0, 0),\n",
       "              (11, 1, 0),\n",
       "              (11, 2, 0),\n",
       "              (11, 3, 0),\n",
       "              (11, 4, 0),\n",
       "              (11, 5, 0),\n",
       "              (11, 6, 0),\n",
       "              (11, 7, 0),\n",
       "              (11, 8, 0),\n",
       "              (11, 9, 0),\n",
       "              (12, 0, 0),\n",
       "              (12, 1, 0),\n",
       "              (12, 2, 0),\n",
       "              (12, 3, 0),\n",
       "              (12, 4, 0),\n",
       "              (12, 5, 0),\n",
       "              (12, 6, 0),\n",
       "              (12, 7, 0),\n",
       "              (12, 8, 0),\n",
       "              (12, 9, 0),\n",
       "              (13, 0, 0),\n",
       "              (13, 1, 0),\n",
       "              (13, 2, 0),\n",
       "              (13, 3, 0),\n",
       "              (13, 4, 0),\n",
       "              (13, 5, 0),\n",
       "              (13, 6, 0),\n",
       "              (13, 7, 0),\n",
       "              (13, 8, 0),\n",
       "              (13, 9, 0),\n",
       "              (14, 0, 0),\n",
       "              (14, 1, 0),\n",
       "              (14, 2, 0),\n",
       "              (14, 3, 0),\n",
       "              (14, 4, 0),\n",
       "              (14, 5, 0),\n",
       "              (14, 6, 0),\n",
       "              (14, 7, 0),\n",
       "              (14, 8, 0),\n",
       "              (14, 9, 0),\n",
       "              (15, 0, 0),\n",
       "              (15, 1, 0),\n",
       "              (15, 2, 0),\n",
       "              (15, 3, 0),\n",
       "              (15, 4, 0),\n",
       "              (15, 5, 0),\n",
       "              (15, 6, 0),\n",
       "              (15, 7, 0),\n",
       "              (15, 8, 0),\n",
       "              (15, 9, 0),\n",
       "              (16, 0, 0),\n",
       "              (16, 1, 0),\n",
       "              (16, 2, 0),\n",
       "              (16, 3, 0),\n",
       "              (16, 4, 0),\n",
       "              (16, 5, 0),\n",
       "              (16, 6, 0),\n",
       "              (16, 7, 0),\n",
       "              (16, 8, 0),\n",
       "              (16, 9, 0),\n",
       "              (17, 0, 0),\n",
       "              (17, 1, 0),\n",
       "              (17, 2, 0),\n",
       "              (17, 3, 0),\n",
       "              (17, 4, 0),\n",
       "              (17, 5, 0),\n",
       "              (17, 6, 0),\n",
       "              (17, 7, 0),\n",
       "              (17, 8, 0),\n",
       "              (17, 9, 0),\n",
       "              (18, 0, 0),\n",
       "              (18, 1, 0),\n",
       "              (18, 2, 0),\n",
       "              (18, 3, 0),\n",
       "              (18, 4, 0),\n",
       "              (18, 5, 0),\n",
       "              (18, 6, 0),\n",
       "              (18, 7, 0),\n",
       "              (18, 8, 0),\n",
       "              (18, 9, 0),\n",
       "              (19, 0, 0),\n",
       "              (19, 1, 0),\n",
       "              (19, 2, 0),\n",
       "              (19, 3, 0),\n",
       "              (19, 4, 0),\n",
       "              (19, 5, 0),\n",
       "              (19, 6, 0),\n",
       "              (19, 7, 0),\n",
       "              (19, 8, 0),\n",
       "              (19, 9, 0),\n",
       "              (20, 0, 0),\n",
       "              (20, 1, 0),\n",
       "              (20, 2, 0),\n",
       "              (20, 3, 0),\n",
       "              (20, 4, 0),\n",
       "              (20, 5, 0),\n",
       "              (20, 6, 0),\n",
       "              (20, 7, 0),\n",
       "              (20, 8, 0),\n",
       "              (20, 9, 0)],\n",
       "             'weight_mask_reg': [(0,\n",
       "               0,\n",
       "               tensor(1., device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 1, tensor(0.9995, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 2, tensor(0.9991, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 3, tensor(0.9988, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 4, tensor(0.9985, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 5, tensor(0.9982, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 6, tensor(0.9980, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 7, tensor(0.9977, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 8, tensor(0.9975, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (0, 9, tensor(0.9973, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 0, tensor(1., device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 1, tensor(0.9998, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 2, tensor(0.9995, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 3, tensor(0.9993, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 4, tensor(0.9990, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 5, tensor(0.9988, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 6, tensor(0.9985, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 7, tensor(0.9983, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 8, tensor(0.9980, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (1, 9, tensor(0.9978, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 0, tensor(0.9976, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 1, tensor(0.9974, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 2, tensor(0.9972, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 3, tensor(0.9970, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 4, tensor(0.9968, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 5, tensor(0.9966, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 6, tensor(0.9965, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 7, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 8, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (2, 9, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 0, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 1, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 2, tensor(0.9957, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 3, tensor(0.9956, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 4, tensor(0.9955, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 5, tensor(0.9954, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 6, tensor(0.9953, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 7, tensor(0.9952, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 8, tensor(0.9951, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (3, 9, tensor(0.9950, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 0, tensor(0.9949, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 1, tensor(0.9949, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 2, tensor(0.9948, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 3, tensor(0.9947, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 4, tensor(0.9947, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 5, tensor(0.9946, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 6, tensor(0.9946, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 7, tensor(0.9945, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 8, tensor(0.9945, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (4, 9, tensor(0.9945, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 0, tensor(0.9944, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 1, tensor(0.9944, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 2, tensor(0.9944, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 3, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 4, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 5, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 6, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 7, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 8, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (5, 9, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 0, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 1, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 2, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 3, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 4, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 5, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 6, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 7, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 8, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (6, 9, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 0, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 1, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 2, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 3, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 4, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 5, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 6, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 7, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 8, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (7, 9, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 0, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 1, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 2, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 3, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 4, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 5, tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 6, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 7, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 8, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (8, 9, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 0, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 1, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 2, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 3, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 4, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 5, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 6, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 7, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 8, tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (9, 9, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 0, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 1, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 2, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 3, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 4, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 5, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 6, tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 7, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 8, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (10, 9, tensor(0.9943, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 0, tensor(1.0000, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 1, tensor(0.9999, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 2, tensor(0.9999, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 3, tensor(0.9999, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 4, tensor(0.9998, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 5, tensor(0.9997, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 6, tensor(0.9996, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 7, tensor(0.9995, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 8, tensor(0.9994, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (11, 9, tensor(0.9993, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 0, tensor(0.9992, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 1, tensor(0.9991, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 2, tensor(0.9990, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 3, tensor(0.9989, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 4, tensor(0.9988, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 5, tensor(0.9986, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 6, tensor(0.9985, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 7, tensor(0.9984, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 8, tensor(0.9983, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (12, 9, tensor(0.9982, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 0, tensor(0.9980, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 1, tensor(0.9979, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 2, tensor(0.9978, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 3, tensor(0.9977, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 4, tensor(0.9976, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 5, tensor(0.9975, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 6, tensor(0.9974, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 7, tensor(0.9972, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 8, tensor(0.9971, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (13, 9, tensor(0.9970, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 0, tensor(0.9969, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 1, tensor(0.9968, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 2, tensor(0.9967, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 3, tensor(0.9966, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 4, tensor(0.9966, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 5, tensor(0.9965, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 6, tensor(0.9964, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 7, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 8, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (14, 9, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 0, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 1, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 2, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 3, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 4, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 5, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 6, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 7, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 8, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (15, 9, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 0, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 1, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 2, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 3, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 4, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 5, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 6, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 7, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 8, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (16, 9, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 0, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 1, tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 2, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 3, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 4, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 5, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 6, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 7, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 8, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (17, 9, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 0, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 1, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 2, tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 3, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 4, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 5, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 6, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 7, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 8, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (18, 9, tensor(0.9960, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 0, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 1, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 2, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 3, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 4, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 5, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 6, tensor(0.9961, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 7, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 8, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (19, 9, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 0, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 1, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 2, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 3, tensor(0.9962, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 4, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 5, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 6, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 7, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20, 8, tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)),\n",
       "              (20,\n",
       "               9,\n",
       "               tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>))]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d137e95747a4530a3985c292c3ab97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>edge_reg_term</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_ioi</td><td>█▆▅▄▄▃▂▂▁</td></tr><tr><td>train_loss_owt</td><td>▄█▅▁▅█▄▃</td></tr><tr><td>weight_mask_reg</td><td>▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>edge_reg_term</td><td>0</td></tr><tr><td>train_loss_ioi</td><td>-10.9533</td></tr><tr><td>train_loss_owt</td><td>3.68572</td></tr><tr><td>weight_mask_reg</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-dust-30</strong> at: <a href='https://wandb.ai/quirky_lats_at_mats/mech_unlearning_debug/runs/2tt4pnoz' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/mech_unlearning_debug/runs/2tt4pnoz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_083653-2tt4pnoz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how many edges masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoTransformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(57824992., device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(57844992., device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weight_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 'embed'),\n",
       " (0, 'a0.1'),\n",
       " (0, 'a0.10'),\n",
       " (0, 'a0.6'),\n",
       " (0, 'a0.9'),\n",
       " (0, 'm0'),\n",
       " (1, 'a1.11'),\n",
       " (1, 'a1.7'),\n",
       " (1, 'm1'),\n",
       " (2, 'm2'),\n",
       " (3, 'a3.0'),\n",
       " (3, 'a3.4'),\n",
       " (3, 'm3'),\n",
       " (4, 'a4.11'),\n",
       " (4, 'm4'),\n",
       " (5, 'a5.5'),\n",
       " (5, 'a5.8'),\n",
       " (5, 'a5.9'),\n",
       " (5, 'm5'),\n",
       " (6, 'a6.0'),\n",
       " (6, 'm6'),\n",
       " (7, 'a7.3'),\n",
       " (7, 'a7.9'),\n",
       " (7, 'm7'),\n",
       " (8, 'a8.10'),\n",
       " (8, 'a8.6'),\n",
       " (8, 'm8'),\n",
       " (9, 'a9.6'),\n",
       " (9, 'a9.7'),\n",
       " (9, 'a9.8'),\n",
       " (9, 'a9.9'),\n",
       " (9, 'm9'),\n",
       " (10, 'a10.0'),\n",
       " (10, 'a10.1'),\n",
       " (10, 'a10.10'),\n",
       " (10, 'a10.2'),\n",
       " (10, 'a10.6'),\n",
       " (10, 'a10.7'),\n",
       " (10, 'm10'),\n",
       " (11, 'a11.10'),\n",
       " (11, 'a11.2'),\n",
       " (11, 'a11.3'),\n",
       " (12, 'output')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdcpp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 3072])\n",
      "tensor(2359000., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(296, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# check mlp 1\n",
    "print(model.blocks[1].mlp.weight_mask_W_in.shape)\n",
    "print(model.blocks[1].mlp.weight_mask_W_in.sum())\n",
    "print((model.blocks[1].mlp.weight_mask_W_in == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19999, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tot_zeros = 0\n",
    "for layer, node in acdcpp_nodes:\n",
    "    if 'embed' in node:\n",
    "        continue\n",
    "    elif 'm' in node:\n",
    "        tot_zeros += (model.blocks[layer].mlp.weight_mask_W_in == 0).sum()\n",
    "        tot_zeros += (model.blocks[layer].mlp.weight_mask_W_out == 0).sum()\n",
    "        tot_zeros += (model.blocks[layer].mlp.weight_mask_b_in == 0).sum()\n",
    "        tot_zeros += (model.blocks[layer].mlp.weight_mask_b_out == 0).sum()\n",
    "    elif 'a' in node:\n",
    "        head = int(node.split('.')[1])\n",
    "        tot_zeros += (model.blocks[layer].attn.weight_mask_W_Q[head] == 0).sum()\n",
    "        tot_zeros += (model.blocks[layer].attn.weight_mask_W_K[head] == 0).sum()\n",
    "        tot_zeros += (model.blocks[layer].attn.weight_mask_W_V[head] == 0).sum()\n",
    "        tot_zeros += (model.blocks[layer].attn.weight_mask_W_O[head] == 0).sum()\n",
    "    \n",
    "print(tot_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DemoTransformer Implementations Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2482e-06, device='cuda:0')\n",
      "tensor(7.2237e-06, device='cuda:0')\n",
      "tensor(6.9957e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.transformers.gpt2.edge_masked_transformer import DemoTransformer as GPT2EdgeDemoTransformer, Config as GPT2Config\n",
    "\n",
    "from cb_utils.models import tl_config_to_demo_config\n",
    "with open(\"models/gpt2_weights.pkl\", \"rb\") as f:\n",
    "    gpt2_weights = pickle.load(f)\n",
    "demo_edge_gpt2 = GPT2EdgeDemoTransformer(GPT2Config(debug=False, n_layers=12, n_heads=12), means=False)\n",
    "demo_edge_gpt2.load_state_dict(gpt2_weights, strict=False)\n",
    "demo_edge_gpt2.cuda()\n",
    "\n",
    "\n",
    "from cb_utils.transformers.gpt2.weight_masked_transformer import DemoTransformer as GPT2WeightDemoTransformer, Config as GPT2Config\n",
    "demo_weight_gpt2 = GPT2WeightDemoTransformer(GPT2Config(debug=False, n_layers=12, n_heads=12))\n",
    "demo_weight_gpt2.load_state_dict(gpt2_weights, strict=False)\n",
    "demo_weight_gpt2.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_input = t.tensor(gpt2_tokenizer.encode(\"The quick brown fox jumps over the lazy\")).unsqueeze(0).cuda()\n",
    "    print((demo_edge_gpt2(test_input)[0][0, -1] - reference_gpt2(test_input)[0, -1]).std())\n",
    "    print((demo_weight_gpt2(test_input)[0][0, -1] - reference_gpt2(test_input)[0, -1]).std())\n",
    "    print((demo_edge_gpt2(test_input)[0][0, -1] - demo_weight_gpt2(test_input)[0][0, -1]).std())\n",
    "    # print((model(test_input)[0][0, -1] - edge_masked_model(test_input)[0][0, -1]).std())\n",
    "    # print((reference_pythia(test_input)[0, -1] - edge_masked_model(test_input)[0][0, -1]).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ACDCPP edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found, will not be able to run evaluations on HPSAQ Task\n",
      "OpenAI API key not found, will not be able to run evaluations on HPSAQ Task\n",
      "Clean logit diff: 3.040117025375366, Corrupted logit diff: 1.2651995420455933\n",
      "Clean logit diff: 3.040, Corrupt logit diff: 1.265\n"
     ]
    }
   ],
   "source": [
    "from tasks.ioi.IOITask import IOITask_old, IOITask\n",
    "# ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25)\n",
    "ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"ABBA\")\n",
    "ioi_task.set_logit_diffs(model)\n",
    "\n",
    "ioi_metric = ioi_task.get_acdcpp_metric()\n",
    "def negative_abs_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ioi_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ioi_task.clean_data.toks)\n",
    "    corrupt_logits = model(ioi_task.corr_data.toks)\n",
    "    clean_logit_diff = ioi_task.ave_logit_diff(clean_logits, ioi_task.clean_data).item()\n",
    "    corrupt_logit_diff = ioi_task.ave_logit_diff(corrupt_logits, ioi_task.corr_data).item()\n",
    "    print(f'Clean logit diff: {clean_logit_diff:.3f}, Corrupt logit diff: {corrupt_logit_diff:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_metric(clean_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_metric(corrupt_logits, ioi_dataset=ioi_task.corr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.040117025375366, 1.2651995420455933)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logit_diff, corrupt_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PLACE]': ['station',\n",
       "  'restaurant',\n",
       "  'restaurant',\n",
       "  'restaurant',\n",
       "  'restaurant'],\n",
       " '[OBJECT]': ['ring', 'computer', 'necklace', 'bone', 'computer'],\n",
       " 'text': ['Then, William and Richard went to the station. William gave a ring to',\n",
       "  'Then, Charles and Jeremy went to the restaurant. Charles gave a computer to',\n",
       "  'Then, Simon and Clark went to the restaurant. Simon gave a necklace to',\n",
       "  'Then, Jacob and Scott went to the restaurant. Jacob gave a bone to',\n",
       "  'Then, Steven and Sullivan went to the restaurant. Steven gave a computer to'],\n",
       " 'IO': ['Richard', 'Jeremy', 'Clark', 'Scott', 'Sullivan'],\n",
       " 'S': ['William', 'Charles', 'Simon', 'Jacob', 'Steven'],\n",
       " 'TEMPLATE_IDX': tensor([0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_task_2 = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"BABA\", template_start_idx=0)\n",
    "ioi_task_2.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15299.74it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 258.14it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 281818.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:08<00:08,  8.11s/it]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15224.87it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 253.97it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 306343.88it/s]\n",
      "100%|██████████| 2/2 [00:15<00:00,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.08, .15]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ioi_task.clean_data.toks,\n",
    "    corr_data=ioi_task.corr_data.toks,\n",
    "    acdc_metric=negative_abs_ioi_metric,\n",
    "    acdcpp_metric=ioi_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 'a0.10'), (-1, 'embed')),\n",
       " ((0, 'a0.9'), (-1, 'embed')),\n",
       " ((0, 'm0'), (-1, 'embed')),\n",
       " ((0, 'm0'), (0, 'a0.10')),\n",
       " ((1, 'm1'), (0, 'm0')),\n",
       " ((3, 'a3.0'), (0, 'm0')),\n",
       " ((3, 'm3'), (0, 'm0')),\n",
       " ((3, 'm3'), (1, 'm1')),\n",
       " ((3, 'm3'), (3, 'a3.0')),\n",
       " ((4, 'm4'), (0, 'm0')),\n",
       " ((4, 'm4'), (3, 'a3.0')),\n",
       " ((5, 'a5.9'), (0, 'm0')),\n",
       " ((5, 'a5.9'), (1, 'm1')),\n",
       " ((5, 'a5.9'), (3, 'a3.0')),\n",
       " ((5, 'a5.9'), (3, 'm3')),\n",
       " ((5, 'm5'), (3, 'a3.0')),\n",
       " ((5, 'm5'), (5, 'a5.5')),\n",
       " ((6, 'm6'), (3, 'a3.0')),\n",
       " ((6, 'm6'), (5, 'm5')),\n",
       " ((7, 'a7.9'), (0, 'm0')),\n",
       " ((7, 'm7'), (6, 'm6')),\n",
       " ((8, 'a8.10'), (4, 'm4')),\n",
       " ((8, 'a8.10'), (5, 'a5.5')),\n",
       " ((8, 'a8.10'), (5, 'a5.9')),\n",
       " ((8, 'a8.6'), (5, 'a5.5')),\n",
       " ((9, 'a9.9'), (8, 'a8.10')),\n",
       " ((10, 'a10.0'), (8, 'a8.10')),\n",
       " ((10, 'a10.7'), (0, 'm0')),\n",
       " ((10, 'a10.7'), (6, 'm6')),\n",
       " ((10, 'a10.7'), (9, 'a9.6')),\n",
       " ((10, 'a10.7'), (9, 'a9.9')),\n",
       " ((11, 'a11.10'), (0, 'm0')),\n",
       " ((11, 'a11.10'), (6, 'm6')),\n",
       " ((11, 'a11.10'), (8, 'a8.10')),\n",
       " ((11, 'a11.10'), (9, 'a9.6')),\n",
       " ((11, 'a11.10'), (9, 'a9.9')),\n",
       " ((11, 'a11.10'), (10, 'a10.0')),\n",
       " ((11, 'a11.10'), (10, 'a10.1')),\n",
       " ((11, 'a11.10'), (10, 'a10.10')),\n",
       " ((11, 'a11.10'), (10, 'a10.7')),\n",
       " ((11, 'a11.2'), (0, 'm0')),\n",
       " ((11, 'a11.2'), (8, 'a8.10')),\n",
       " ((11, 'a11.2'), (9, 'a9.8')),\n",
       " ((11, 'a11.2'), (9, 'a9.9')),\n",
       " ((11, 'a11.2'), (9, 'm9')),\n",
       " ((12, 'output'), (7, 'a7.9')),\n",
       " ((12, 'output'), (8, 'a8.10')),\n",
       " ((12, 'output'), (9, 'a9.6')),\n",
       " ((12, 'output'), (9, 'a9.8')),\n",
       " ((12, 'output'), (9, 'a9.9')),\n",
       " ((12, 'output'), (10, 'a10.0')),\n",
       " ((12, 'output'), (10, 'a10.1')),\n",
       " ((12, 'output'), (10, 'a10.10')),\n",
       " ((12, 'output'), (10, 'a10.2')),\n",
       " ((12, 'output'), (10, 'a10.6')),\n",
       " ((12, 'output'), (10, 'a10.7')),\n",
       " ((12, 'output'), (11, 'a11.10')),\n",
       " ((12, 'output'), (11, 'a11.2'))}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdcpp_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('head.0.2', 'mlp.0', 0.009443754330277443), ('head.0.14', 'mlp.0', 0.006188404746353626), ('mlp.6', 'mlp.8', 0.00573932658880949), ('mlp.0', 'mlp.2', 0.005653967149555683), ('head.16.20', 'mlp.16', -0.005345507059246302), ('head.16.20', 'head.17.30.v', 0.005315450485795736), ('mlp.0', 'head.1.16.k', -0.005109565332531929), ('head.14.14', 'mlp.15', -0.004921694286167622), ('mlp.15', 'head.16.20.k', -0.004847021773457527), ('mlp.6', 'mlp.15', -0.004780464340001345), ('mlp.10', 'head.16.20.k', 0.004766407422721386), ('mlp.6', 'head.16.20.k', 0.004757486749440432), ('mlp.0', 'head.1.15.k', -0.004494336899369955), ('mlp.0', 'mlp.5', -0.004315529949963093), ('mlp.0', 'mlp.4', -0.004094669129699469), ('head.16.20', 'mlp.18', 0.004019735846668482), ('head.0.30', 'mlp.0', 0.0039080469869077206), ('mlp.0', 'mlp.6', -0.0038432518485933542), ('mlp.8', 'mlp.9', 0.00376768596470356), ('head.16.20', 'head.21.9.v', 0.003543522208929062), ('mlp.9', 'mlp.11', 0.003415714716538787), ('mlp.6', 'head.21.9.k', 0.0033131211530417204), ('head.15.4', 'mlp.15', -0.003284941893070936), ('head.15.20', 'head.16.20.q', -0.0032149143517017365), ('mlp.5', 'mlp.6', 0.003205507528036833), ('mlp.9', 'mlp.13', 0.003195255296304822), ('mlp.3', 'mlp.6', 0.0031151920557022095), ('mlp.5', 'mlp.8', 0.003063748823478818), ('mlp.12', 'head.16.20.k', -0.0030587592627853155), ('mlp.0', 'head.1.17.k', -0.0030355819035321474), ('mlp.11', 'head.16.20.v', 0.0030052291695028543), ('head.9.15', 'head.16.20.k', 0.002988740336149931), ('head.15.20', 'mlp.15', 0.002843316178768873), ('mlp.4', 'mlp.8', 0.002790694823488593), ('head.15.4', 'mlp.16', -0.002768619218841195), ('mlp.13', 'head.16.20.v', 0.002720218151807785), ('mlp.11', 'mlp.14', 0.0025788003113120794), ('mlp.11', 'head.16.20.k', -0.0025367813650518656), ('mlp.0', 'mlp.7', -0.002536351792514324), ('mlp.11', 'mlp.15', 0.0024518363643437624), ('mlp.8', 'head.16.20.v', 0.0023244901094585657), ('head.0.27', 'mlp.0', -0.0023238908033818007), ('mlp.6', 'mlp.11', 0.002282592235133052), ('head.17.30', 'mlp.18', 0.0022805416956543922), ('mlp.0', 'head.21.9.k', 0.0022331280633807182), ('head.0.21', 'mlp.0', 0.0022144378162920475), ('mlp.3', 'mlp.15', -0.00221416843123734), ('head.15.20', 'head.17.30.q', -0.002205037511885166), ('mlp.10', 'mlp.15', -0.0022013422567397356), ('mlp.4', 'head.16.20.k', 0.0021811285987496376), ('mlp.2', 'mlp.3', 0.0021742351818829775), ('mlp.30', 'mlp.31', -0.002144202124327421), ('mlp.5', 'head.16.20.k', 0.0021362670231610537), ('head.16.17', 'head.17.30.q', 0.002123921876773238), ('mlp.29', 'mlp.31', -0.00211007590405643), ('mlp.14', 'head.16.20.v', 0.0021046672482043505), ('mlp.10', 'mlp.11', -0.0020922652911394835), ('head.12.17', 'mlp.15', -0.0020885460544377565), ('mlp.0', 'mlp.13', -0.0020814668387174606), ('mlp.5', 'head.16.20.v', 0.0020730250980705023), ('mlp.28', 'mlp.31', -0.0020570242777466774), ('mlp.7', 'mlp.8', 0.00204459554515779), ('head.9.15', 'head.11.21.q', -0.002041686326265335), ('mlp.0', 'mlp.14', -0.002014364581555128), ('mlp.0', 'head.15.5.k', -0.002007126808166504), ('mlp.15', 'mlp.16', 0.001995496451854706), ('head.14.14', 'mlp.16', -0.001992548583075404), ('mlp.6', 'mlp.14', -0.001976989908143878), ('mlp.15', 'head.17.30.k', -0.0019326637266203761), ('head.16.20', 'head.22.15.v', 0.001931019127368927), ('mlp.6', 'mlp.12', 0.0019294897792860866), ('head.13.8', 'head.21.9.q', 0.0019150624284520745), ('mlp.0', 'mlp.11', -0.0019050340633839369), ('head.0.21', 'mlp.1', 0.0018889722414314747), ('head.7.8', 'mlp.13', -0.0018879767740145326), ('head.1.25', 'mlp.2', 0.0018849290208891034), ('head.15.4', 'head.16.20.q', 0.0018740722443908453), ('mlp.12', 'mlp.13', 0.001870404346846044), ('head.16.20', 'mlp.17', -0.0018696865299716592), ('head.10.1', 'mlp.15', -0.0018582759657874703), ('mlp.1', 'mlp.2', 0.0018570123938843608), ('mlp.2', 'mlp.5', 0.0018493117531761527), ('mlp.0', 'mlp.1', -0.001831269939430058), ('head.0.6', 'mlp.0', 0.00180960597936064), ('mlp.10', 'head.21.9.k', 0.001803320599719882), ('head.0.31', 'mlp.0', -0.0017993211513385177), ('mlp.12', 'head.15.20.k', 0.0017812863225117326), ('mlp.14', 'head.21.9.k', 0.0017715865978971124), ('head.7.8', 'head.16.20.k', 0.0017514253268018365), ('mlp.10', 'mlp.12', -0.001751400763168931), ('head.16.20', 'head.21.9.k', 0.0017360210185870528), ('mlp.5', 'mlp.15', -0.0017260626191273332), ('head.9.15', 'head.17.30.k', 0.0017186870099976659), ('head.15.4', 'head.21.9.k', 0.00171701202634722), ('mlp.5', 'mlp.9', 0.0017162136500701308), ('mlp.6', 'mlp.9', 0.001710148761048913), ('mlp.3', 'head.16.20.k', 0.0016996695194393396), ('mlp.3', 'mlp.8', 0.001693469937890768), ('mlp.0', 'head.1.11.v', 0.0016771022928878665), ('mlp.0', 'head.1.18.k', 0.001676780404523015), ('head.15.20', 'mlp.16', 0.0016647606389597058), ('mlp.2', 'mlp.4', 0.0016523718368262053), ('head.15.4', 'head.16.10.v', 0.0016409446252509952), ('mlp.4', 'mlp.5', 0.0015915663680061698), ('head.7.8', 'mlp.15', 0.001587998354807496), ('head.10.11', 'mlp.11', -0.0015877102268859744), ('head.16.20', 'head.25.1.k', -0.0015855319797992706), ('mlp.8', 'mlp.14', 0.0015662244986742735), ('head.14.14', 'head.16.20.q', 0.0015645340317860246), ('mlp.18', 'head.21.9.v', 0.0015612227143719792), ('head.17.30', 'mlp.17', -0.0015610615955665708), ('mlp.7', 'mlp.10', -0.0015545483911409974), ('head.7.14', 'mlp.15', 0.0015438116388395429), ('mlp.3', 'mlp.5', 0.0015396539820358157), ('head.9.15', 'mlp.12', -0.001538568758405745), ('head.9.15', 'mlp.14', -0.0015276921913027763), ('head.17.30', 'head.21.9.v', 0.0015151547268033028), ('head.16.17', 'mlp.16', -0.0015022397274151444), ('mlp.24', 'head.25.1.k', -0.0014950997428968549), ('mlp.8', 'mlp.13', 0.0014932940248399973), ('mlp.4', 'head.21.9.k', 0.0014929039170965552), ('mlp.6', 'mlp.13', 0.0014775244053453207), ('head.9.15', 'mlp.13', -0.0014682551845908165), ('mlp.7', 'head.16.20.v', 0.0014557429822161794), ('mlp.0', 'head.21.31.k', -0.0014490863541141152), ('head.12.17', 'head.15.4.v', 0.0014430660521611571), ('mlp.6', 'head.7.20.v', 0.0014389472780749202), ('mlp.0', 'head.4.13.q', 0.0014347839169204235), ('mlp.0', 'head.1.16.q', 0.0014134803786873817), ('mlp.8', 'mlp.10', 0.0014106555609032512), ('mlp.0', 'head.21.19.k', -0.001405258197337389), ('head.17.30', 'head.21.9.k', 0.0014030528254806995), ('mlp.9', 'mlp.10', 0.0013922336511313915), ('mlp.0', 'mlp.3', -0.0013902803184464574), ('mlp.6', 'head.16.20.v', 0.0013750517973676324), ('head.9.3', 'mlp.9', -0.0013734701788052917), ('head.13.8', 'head.17.30.q', 0.0013706028694286942), ('head.16.20', 'head.22.17.v', 0.0013674175133928657), ('head.13.8', 'head.15.20.q', -0.0013636015355587006), ('mlp.8', 'head.14.14.q', -0.001362730166874826), ('mlp.0', 'head.16.21.k', 0.0013574105687439442), ('head.7.8', 'head.17.30.k', 0.0013550223084166646), ('head.19.24', 'head.21.9.v', 0.00135287013836205), ('head.0.28', 'mlp.0', -0.001351020997390151), ('head.14.14', 'head.15.20.q', -0.0013473035069182515), ('mlp.0', 'head.1.10.k', 0.0013392082182690501), ('mlp.10', 'head.15.20.q', 0.0013274125522002578), ('mlp.7', 'mlp.14', -0.0013219445245340466), ('head.12.17', 'head.15.20.v', -0.0013092361623421311), ('mlp.11', 'head.17.30.k', -0.0013053971342742443), ('mlp.7', 'head.21.9.k', 0.001304464996792376), ('mlp.0', 'head.1.4.v', 0.0012995852157473564), ('head.13.8', 'head.14.31.q', -0.0012961990432813764), ('mlp.21', 'mlp.31', 0.0012819045223295689), ('mlp.15', 'head.16.20.v', 0.001273989793844521), ('head.13.8', 'head.16.20.q', 0.001270874636247754), ('mlp.0', 'head.1.25.k', -0.001269094762392342), ('mlp.20', 'mlp.31', 0.0012679877690970898), ('mlp.7', 'mlp.11', 0.0012615789892151952), ('mlp.9', 'head.11.21.q', 0.0012575825676321983), ('head.0.30', 'mlp.3', 0.001251908834092319), ('mlp.0', 'head.24.21.k', 0.0012514435220509768), ('mlp.8', 'head.16.20.k', 0.0012460232246667147), ('head.0.16', 'mlp.0', 0.0012409038608893752), ('mlp.0', 'head.15.11.k', -0.001235301955603063), ('head.0.17', 'mlp.0', 0.0012327723670750856), ('mlp.23', 'head.25.1.k', -0.0012299207737669349), ('head.12.17', 'head.16.20.v', 0.0012290324084460735), ('mlp.12', 'mlp.15', 0.0012218646006658673), ('head.15.4', 'head.16.20.v', 0.001221155864186585), ('head.0.4', 'mlp.0', 0.001221106736920774), ('head.12.17', 'head.21.9.k', 0.001220965525135398), ('mlp.12', 'head.16.20.v', 0.0012155117001384497), ('head.16.20', 'head.21.19.k', -0.001211330178193748), ('mlp.3', 'head.21.9.k', 0.0011980809504166245), ('head.16.20', 'head.17.30.q', -0.0011938184034079313), ('mlp.15', 'head.16.17.k', -0.001190812443383038), ('head.13.30', 'mlp.14', 0.0011839853832498193), ('head.8.11', 'mlp.15', -0.0011744748335331678), ('mlp.8', 'head.15.20.v', -0.0011725453659892082), ('head.14.14', 'head.21.9.k', 0.0011706198565661907), ('mlp.6', 'head.15.20.k', -0.0011685850331559777), ('mlp.25', 'mlp.26', -0.0011683834018185735), ('mlp.4', 'head.16.20.v', 0.0011622239835560322), ('mlp.8', 'mlp.11', 0.0011603647144511342), ('mlp.2', 'mlp.15', -0.001156176207587123), ('head.9.3', 'mlp.14', -0.001154008787125349), ('mlp.15', 'head.16.20.q', -0.001148901996202767), ('head.17.30', 'head.22.15.v', 0.0011446698335930705), ('head.13.30', 'mlp.15', 0.0011397490743547678), ('mlp.18', 'head.25.1.k', -0.0011324080405756831), ('head.13.20', 'head.15.4.v', 0.001122756046243012), ('mlp.21', 'head.25.1.k', -0.0011200009612366557), ('head.0.7', 'mlp.0', 0.0011196527630090714), ('mlp.13', 'head.15.20.k', 0.0011141891591250896), ('head.7.9', 'mlp.15', -0.001111720921471715), ('mlp.2', 'head.21.9.k', 0.001110269222408533), ('head.1.25', 'mlp.3', 0.0011059733806177974), ('head.9.15', 'head.21.9.k', 0.0011041940888389945), ('mlp.0', 'head.1.4.k', -0.001102518173865974), ('mlp.23', 'head.26.25.k', 0.001102395704947412), ('mlp.15', 'head.16.17.v', -0.0011007385328412056), ('head.13.30', 'head.16.20.q', -0.001094624400138855), ('mlp.18', 'head.21.9.k', 0.0010888677788898349), ('head.8.14', 'mlp.9', -0.0010858307359740138), ('head.15.4', 'head.17.30.q', 0.0010818063747137785), ('mlp.6', 'head.15.20.v', -0.001078712404705584), ('head.9.15', 'head.10.26.v', -0.0010778617579489946), ('mlp.0', 'mlp.8', -0.0010757920099422336), ('head.15.27', 'head.16.20.q', -0.0010755263501778245), ('mlp.7', 'head.14.14.v', -0.001074428204447031), ('head.15.4', 'head.21.9.q', 0.0010719193378463387), ('head.14.14', 'head.16.17.v', 0.0010705209570005536), ('mlp.4', 'mlp.12', -0.0010661009000614285), ('head.7.8', 'head.15.20.k', 0.0010634062346071005), ('mlp.23', 'mlp.28', -0.001054505119100213), ('head.7.8', 'head.14.8.v', 0.0010530278086662292), ('mlp.13', 'head.15.4.v', 0.0010519307106733322), ('mlp.11', 'head.16.17.k', -0.0010491471039131284), ('mlp.9', 'mlp.18', -0.001046185614541173), ('head.5.17', 'mlp.7', 0.001044679433107376), ('mlp.8', 'head.13.8.v', 0.0010418110759928823), ('head.7.15', 'mlp.9', -0.0010394830023869872), ('mlp.26', 'mlp.30', -0.0010386754292994738), ('mlp.6', 'head.16.21.k', 0.0010373503901064396), ('head.15.20', 'head.16.17.v', -0.0010349199874326587), ('mlp.0', 'head.1.12.v', 0.0010330087970942259), ('head.12.26', 'mlp.14', -0.0010282696457579732), ('mlp.0', 'head.1.6.v', 0.0010257066460326314), ('head.13.30', 'mlp.13', 0.001021881471388042), ('mlp.6', 'head.20.2.k', -0.0010205552680417895), ('head.13.30', 'head.16.20.k', -0.0010205348953604698), ('head.15.4', 'head.21.19.k', -0.0010198857635259628), ('mlp.8', 'head.13.30.v', -0.0010180402314290404), ('head.15.4', 'head.25.1.k', -0.0010171744506806135), ('mlp.0', 'head.1.17.v', 0.0010166249703615904), ('head.14.14', 'head.17.30.q', 0.0010091098956763744), ('head.14.14', 'head.15.20.k', -0.001008716062642634), ('mlp.0', 'head.22.17.k', 0.0010071411961689591), ('head.17.30', 'head.25.1.k', -0.0010067331604659557), ('head.9.3', 'head.15.4.v', 0.001002408331260085), ('mlp.13', 'head.16.20.k', 0.0010006306692957878)]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.001\n",
    "import pickle\n",
    "with open('localizations/eap/eap_sports/1000_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "\n",
    "eap_edges = graph.top_edges(n=1000, threshold=threshold)\n",
    "# eap_edges = set()\n",
    "# for i in range(eap_scores.shape[0]):\n",
    "#     for j in range(eap_scores.shape[1]):\n",
    "#         if eap_scores[i, j] > threshold:\n",
    "            # eap_edges.add((get_node_name(graph.node_names[i], show_full_index=False), get_node_name(graph.node_names[j, show_full_index=False))))\n",
    "print(eap_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.eap_scores.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert format:\n",
    "want: {((3, 'm3'), (3, 'a3.0')),\n",
    " ((4, 'm4'), (0, 'm0')),\n",
    " ((4, 'm4'), (3, 'a3.0')),\n",
    " ((5, 'a5.9'), (0, 'm0')),}\n",
    "\n",
    "have:\n",
    "[('mlp.0', 'mlp.2', 0.005653967149555683),\n",
    "('head.0.14', 'mlp.0', 0.006188404746353626),]\n",
    "...\n",
    "\"\"\"\n",
    "from cb_utils.mask_utils import get_formatted_edges_from_eap, get_masks_from_eap_exp\n",
    "# formatted_eap_edges = get_formatted_edges_from_eap(eap_edges)\n",
    "# formatted_eap_edges\n",
    "with open('localizations/eap/eap_sports/1000_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_eap_exp(graph, threshold=0.001, num_layers=32, num_heads=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acdcpp_mask_dict['m31'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.292320251464844 1.4027974605560303\n"
     ]
    }
   ],
   "source": [
    "from tasks import InductionTask\n",
    "ind_task = InductionTask(batch_size=16, tokenizer=model.tokenizer, prep_acdcpp=True, seq_len=10, acdcpp_metric=\"ave_logit_diff\")\n",
    "ind_task.set_logit_diffs(model)\n",
    "print(ind_task.clean_logit_diff, ind_task.corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_metric = ind_task.get_acdcpp_metric()\n",
    "def negative_abs_ind_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ind_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ind_task.clean_data.cuda())\n",
    "    corrupt_logits = model(ind_task.corr_data.cuda())\n",
    "    clean_logit_diff = ind_task.ave_logit_diff(clean_logits, ind_task.clean_data).item()\n",
    "    corrupt_logit_diff = ind_task.ave_logit_diff(corrupt_logits, ind_task.corr_data).item()\n",
    "    \n",
    "print(ind_metric(clean_logits))\n",
    "print(ind_metric(corrupt_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15171.34it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 252.85it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 304067.19it/s]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 5, 0, 1, 2, 3, 4, 6, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.05]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ind_task.clean_data,\n",
    "    corr_data=ind_task.corr_data,\n",
    "    acdc_metric=negative_abs_ind_metric,\n",
    "    acdcpp_metric=ind_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=THRESHOLDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'acdcpp_mask_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m localize_acdcpp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# if edge_masks is True, then have mask_dict_superset be acdcpp_mask_dict\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m mask_dict_superset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m edge_masks \u001b[38;5;28;01melse\u001b[39;00m \u001b[43macdcpp_mask_dict\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model = load_demo_gpt2(means=means, mask_dict_superset=acdcpp_mask_dict)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m localize_acdcpp:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acdcpp_mask_dict' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cb_utils.transformer import DemoTransformer\n",
    "from cb_utils.models import load_demo_gpt2, tokenizer\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "edge_masks = True\n",
    "weight_masks_attn = True\n",
    "weight_masks_mlp = True\n",
    "train_base_weights = True\n",
    "localize_acdcpp = True\n",
    "\n",
    "# if edge_masks is True, then have mask_dict_superset be acdcpp_mask_dict\n",
    "mask_dict_superset = None if not edge_masks else acdcpp_mask_dict\n",
    "# model = load_demo_gpt2(means=means, mask_dict_superset=acdcpp_mask_dict)\n",
    "if localize_acdcpp:\n",
    "    weight_mask_attn_dict = acdcpp_weight_mask_attn_dict if weight_masks_attn else None\n",
    "    weight_mask_mlp_dict = acdcpp_weight_mask_mlp_dict if weight_masks_mlp else None\n",
    "    base_weight_attn_dict = acdcpp_weight_mask_attn_dict if train_base_weights else None\n",
    "    base_weight_mlp_dict = acdcpp_weight_mask_mlp_dict if train_base_weights else None\n",
    "\n",
    "else:\n",
    "    weight_mask_attn_dict = None\n",
    "    weight_mask_mlp_dict = None\n",
    "    base_weight_attn_dict = None\n",
    "    base_weight_mlp_dict = None\n",
    "\n",
    "# model = load_demo_gpt2(means=False, edge_masks=edge_masks, mask_dict_superset=mask_dict_superset, weight_masks_attn=weight_masks_attn, weight_masks_mlp=weight_masks_mlp, weight_mask_attn_dict=weight_mask_attn_dict, weight_mask_mlp_dict=weight_mask_mlp_dict, train_base_weights=train_base_weights, base_weight_attn_dict=base_weight_attn_dict, base_weight_mlp_dict=base_weight_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.models import load_demo_pythia\n",
    "threshold = 0.0005\n",
    "with open(f\"localizations/eap/eap_sports/pythia-2.8b_{threshold=}.pkl\", \"rb\") as f:\n",
    "    acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = pickle.load(f)\n",
    "\n",
    "# model = load_demo_pythia(means=False, model_name=\"pythia-2.8b\", edge_masks=True, mask_dict_superset=acdcpp_mask_dict)\n",
    "model = load_demo_pythia(means=False, model_name=\"pythia-2.8b\", edge_mask=False, weight_mask=True, weight_masks_attn=True, weight_masks_mlp=True, weight_mask_attn_dict=acdcpp_weight_mask_attn_dict, weight_mask_mlp_dict=acdcpp_weight_mask_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_pythia = HookedTransformer.from_pretrained(\n",
    "        'pythia-2.8b',\n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        # default_padding_side=\"left\",\n",
    "        # device='cuda'\n",
    "        device='cpu'\n",
    "    )\n",
    "tokenizer = reference_pythia.tokenizer\n",
    "pythia_tokenizer = reference_pythia.tokenizer\n",
    "# reference_pythia.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two models\n",
    "# compare model outputs\n",
    "with torch.no_grad():\n",
    "    test_input = t.tensor(pythia_tokenizer.encode(\"The quick brown fox jumps over the lazy\")).unsqueeze(0).cuda()\n",
    "    # print((model(test_input)[0][0, -1] - reference_pythia(test_input)[0, -1]).std())\n",
    "    # print((model(test_input)[0][0, -1] - edge_masked_model(test_input)[0][0, -1]).std())\n",
    "    # print((reference_pythia(test_input)[0, -1] - edge_masked_model(test_input)[0][0, -1]).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "tensor(12.0880, device='cuda:0')\n",
      "tensor(2.8271, device='cuda:0')\n",
      "tensor(3.4641, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from tasks import SportsTask, InductionTask, IOITask, InductionTask_Uniform, OWTTask#, SportsTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask_Uniform\n",
    "sports_task = SportsTask(batch_size=32, tokenizer=tokenizer, prep_acdcpp=False)\n",
    "ioi_task = IOITask(batch_size=32, tokenizer=tokenizer, prep_acdcpp=False)\n",
    "ind_task = InductionTask(batch_size=32, tokenizer=tokenizer, prep_acdcpp=False)\n",
    "ind_uniform_task = InductionTask_Uniform(batch_size=16, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, uniform_over=\"rep_tokens\")\n",
    "owt_task = OWTTask(batch_size=32, tokenizer=tokenizer, device=device, ctx_length=30)\n",
    "sports_uniform_task = SportsTask_Uniform(batch_size=32, tokenizer=tokenizer, uniform_over=\"sports_tokens\")\n",
    "print(sports_task.get_test_accuracy(model))#, sports_task.get_test_accuracy(reference_pythia))\n",
    "print(ioi_task.get_test_accuracy(model))#, ioi_task.get_test_accuracy(reference_pythia))\n",
    "print(ind_task.get_test_accuracy(model))#, ind_task.get_test_accuracy(reference_pythia))\n",
    "print(ind_uniform_task.get_test_loss(model))#, ind_uniform_task.get_test_loss(reference_pythia))\n",
    "print(owt_task.get_test_loss(model))#, owt_task.get_test_accuracy(reference_pythia))\n",
    "print(sports_uniform_task.get_test_loss(model))#, sports_uniform_task.get_test_accuracy(reference_pythia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.2142, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sports_uniform_task = SportsTask_Uniform(batch_size=32, tokenizer=tokenizer, uniform_over=\"all_tokens\")\n",
    "print(sports_uniform_task.get_test_loss(model))#, sports_uniform_task.get_test_accuracy(reference_pythia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tasks import LimitedSportsTask\n",
    "forget_task = LimitedSportsTask(batch_size=32, tokenizer=tokenizer, start_index=0, stop_index=64, make_complementary_task=True)\n",
    "remember_task = forget_task.complementary_task\n",
    "forget_task.get_test_accuracy(model), remember_task.get_test_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.968525312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# test max batch size for sports, owt\n",
    "owt_task = OWTTask(batch_size=1, tokenizer=tokenizer, ctx_length=30)\n",
    "sports_task = SportsTask(batch_size=1, tokenizer=tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.304587776\n",
      "41.535725568\n",
      "51.745850368\n",
      "61.98750208\n",
      "72.183995392\n",
      "31.304588288\n"
     ]
    }
   ],
   "source": [
    "sports_task = SportsTask(batch_size=1, tokenizer=tokenizer, shuffle=False)\n",
    "tot_loss = 0\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "for i in range(4):\n",
    "    loss = sports_task.get_train_loss(model)\n",
    "    tot_loss += loss\n",
    "    print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "tot_loss.backward()\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.808385536\n",
      "19.528289792\n",
      "None\n",
      "19.150077952\n",
      "tensor([-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.1926,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0430,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "        -0.0000,  0.0000, -0.0000, -0.0041,  0.0266,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0028], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.1994,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0397,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0101,  0.0224,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0029], device='cuda:0')\n",
      "19.916095488\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.2187,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0516,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0415,  0.0118,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0107], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3756,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0774,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0243, -0.0222,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0840], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3745,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0777,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0035, -0.0313,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0760], device='cuda:0')\n",
      "20.301221888\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.2625,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0764,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.3730, -0.0606,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2777], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.2575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0730,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.4224, -0.0521,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1660], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3422,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0573,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.4138, -0.0537,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1833], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.3454,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0576,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.4142, -0.0529,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1860], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.6088,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0573,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.2618, -0.0234,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1402], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.8482,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0413,  0.0014,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2160], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9013,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0280,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0584,  0.0114,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2556], device='cuda:0')\n",
      "18.75236864\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9123,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0311,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0663,  0.0107,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2653], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9955,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0243,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.1122,  0.0201,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2994], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9868,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0802,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0145, -0.0399,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2010], device='cuda:0')\n",
      "19.916095488\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.9972,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0819,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0032, -0.0410,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1897], device='cuda:0')\n",
      "19.150077952\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -1.3423,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0843,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0399, -0.0116,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2129], device='cuda:0')\n",
      "19.530473472\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -1.4041,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0863,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0732, -0.0122,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.2291], device='cuda:0')\n",
      "20.684939264\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -1.4221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0859,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0132, -0.0242,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.1758], device='cuda:0')\n",
      "11.808385536\n"
     ]
    }
   ],
   "source": [
    "tot_loss = 0\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "model.zero_grad()\n",
    "for i in range(20):\n",
    "    loss = sports_task.get_train_loss(model)\n",
    "    print(torch.cuda.memory_allocated(device=device) / 1e9)\n",
    "    print(model.blocks[2].edge_mask_mlp.grad)\n",
    "    loss.backward()\n",
    "print(torch.cuda.memory_allocated(device=device) / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000, -0.4634,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.1553,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.2797,  0.0654,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.3760], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[2].edge_mask_mlp.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that gradients flow correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.0.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.0.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.0.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.0.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.0.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.0.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.0.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.1.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.1.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.1.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.1.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.1.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.1.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.1.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.1.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.2.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.2.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.2.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.2.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.2.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.2.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.2.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.2.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.3.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.3.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.3.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.3.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.3.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.3.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.3.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.3.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.4.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.4.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.4.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.4.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.4.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.4.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.4.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.4.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.5.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.5.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.5.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.5.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.5.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.5.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.5.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.5.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.6.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.6.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.6.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.6.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.6.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.6.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.6.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.6.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.7.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.7.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.7.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.7.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.7.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.7.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.7.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.7.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.8.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.8.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.8.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.8.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.8.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.8.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.8.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.8.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.9.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.9.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.9.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.9.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.9.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.9.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.9.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.9.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.10.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.10.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.10.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.10.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.10.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.10.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.10.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.10.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.11.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.11.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.11.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.11.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.11.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.11.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.11.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.11.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.12.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.12.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.12.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.12.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.12.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.12.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.12.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.12.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.13.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.13.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.13.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.13.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.13.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.13.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.13.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.13.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.14.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.14.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.14.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.14.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.14.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.14.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.14.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.14.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.15.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.15.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.15.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.15.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.15.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.15.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.15.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.15.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.16.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.16.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.16.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.16.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.16.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.16.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.16.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.16.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.17.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.17.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.17.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.17.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.17.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.17.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.17.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.17.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.18.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.18.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.18.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.18.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.18.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.18.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.18.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.18.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.19.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.19.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.19.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.19.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.19.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.19.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.19.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.19.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.20.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.20.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.20.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.20.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.20.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.20.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.20.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.20.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.21.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.21.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.21.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.21.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.21.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.21.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.21.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.21.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.22.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.22.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.22.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.22.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.22.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.22.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.22.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.22.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.23.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.23.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.23.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.23.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.23.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.23.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.23.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.23.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.24.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.24.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.24.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.24.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.24.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.24.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.24.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.24.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.25.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.25.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.25.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.25.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.25.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.25.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.25.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.25.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.26.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.26.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.26.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.26.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.26.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.26.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.26.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.26.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.27.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.27.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.27.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.27.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.27.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.27.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.27.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.27.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.28.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.28.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.28.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.28.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.28.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.28.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.28.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.28.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.29.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.29.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.29.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.29.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.29.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.29.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.29.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.29.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.30.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.30.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.30.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.30.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.30.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.30.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.30.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.30.mlp.weight_mask_b_out torch.Size([2560]) True\n",
      "blocks.31.attn.weight_mask_W_Q torch.Size([32, 2560, 80]) True\n",
      "blocks.31.attn.weight_mask_W_K torch.Size([32, 2560, 80]) True\n",
      "blocks.31.attn.weight_mask_W_V torch.Size([32, 2560, 80]) True\n",
      "blocks.31.attn.weight_mask_W_O torch.Size([32, 80, 2560]) True\n",
      "blocks.31.mlp.weight_mask_W_in torch.Size([2560, 10240]) True\n",
      "blocks.31.mlp.weight_mask_W_out torch.Size([10240, 2560]) True\n",
      "blocks.31.mlp.weight_mask_b_in torch.Size([10240]) True\n",
      "blocks.31.mlp.weight_mask_b_out torch.Size([2560]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "    # print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "batch_size = 2\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False)\n",
    "loss = ioi.get_train_loss(model)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.17700352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0216, device='cuda:0')\n",
      "blocks.0.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.5815, device='cuda:0')\n",
      "blocks.0.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.3946, device='cuda:0')\n",
      "blocks.0.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.7858, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.0081, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(5.3979, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.5360, device='cuda:0')\n",
      "blocks.0.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.6562, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0523, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(2.6793, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(3.4024, device='cuda:0')\n",
      "blocks.1.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(4.1716, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.0001, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.7603, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1013, device='cuda:0')\n",
      "blocks.1.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.4754, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0153, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4646, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6357, device='cuda:0')\n",
      "blocks.2.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.5915, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(6.0486, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(5.4989, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1658, device='cuda:0')\n",
      "blocks.2.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.4407, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0287, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.1237, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.1898, device='cuda:0')\n",
      "blocks.3.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.3442, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(6.7269, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(4.4302, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1361, device='cuda:0')\n",
      "blocks.3.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.3720, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0340, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.2193, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.1874, device='cuda:0')\n",
      "blocks.4.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.5638, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(5.7872, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(3.8785, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1223, device='cuda:0')\n",
      "blocks.4.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.2525, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0247, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.5181, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.1584, device='cuda:0')\n",
      "blocks.5.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.9633, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(5.6964, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(3.4137, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1316, device='cuda:0')\n",
      "blocks.5.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.2880, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0326, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.3960, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.2927, device='cuda:0')\n",
      "blocks.6.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.3720, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(4.1122, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.8020, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1218, device='cuda:0')\n",
      "blocks.6.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.2403, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0707, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.0487, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.0041, device='cuda:0')\n",
      "blocks.7.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.3239, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.1489, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.4518, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0938, device='cuda:0')\n",
      "blocks.7.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1907, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0243, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.6422, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4250, device='cuda:0')\n",
      "blocks.8.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6018, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(3.1996, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(2.1894, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.1043, device='cuda:0')\n",
      "blocks.8.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1480, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1076, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.6959, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6134, device='cuda:0')\n",
      "blocks.9.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(1.0054, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(2.0523, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.9276, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0588, device='cuda:0')\n",
      "blocks.9.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1275, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1095, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4939, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6077, device='cuda:0')\n",
      "blocks.10.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.8943, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.7116, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.7489, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0526, device='cuda:0')\n",
      "blocks.10.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1074, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1050, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.7140, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4779, device='cuda:0')\n",
      "blocks.11.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.7883, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(2.0600, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.6167, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0715, device='cuda:0')\n",
      "blocks.11.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1083, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0721, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4778, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.7437, device='cuda:0')\n",
      "blocks.12.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6520, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.4094, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.4171, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0439, device='cuda:0')\n",
      "blocks.12.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1021, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1011, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.5514, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4197, device='cuda:0')\n",
      "blocks.13.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6264, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.4831, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.3279, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0461, device='cuda:0')\n",
      "blocks.13.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1136, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1698, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.4920, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4761, device='cuda:0')\n",
      "blocks.14.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.5347, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.3809, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.2714, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0472, device='cuda:0')\n",
      "blocks.14.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1006, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1117, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.8832, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.7689, device='cuda:0')\n",
      "blocks.15.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.6868, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.2130, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(1.1867, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0375, device='cuda:0')\n",
      "blocks.15.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1102, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0881, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.6491, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(1.4239, device='cuda:0')\n",
      "blocks.16.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.5172, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.9002, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.8568, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0284, device='cuda:0')\n",
      "blocks.16.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0876, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0202, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.5240, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.6827, device='cuda:0')\n",
      "blocks.17.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.4406, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8107, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7710, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0251, device='cuda:0')\n",
      "blocks.17.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0848, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0177, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.1040, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1383, device='cuda:0')\n",
      "blocks.18.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.1011, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8756, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7267, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0321, device='cuda:0')\n",
      "blocks.18.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0764, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0253, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.8453, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.0762, device='cuda:0')\n",
      "blocks.19.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0645, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7466, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.6848, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0295, device='cuda:0')\n",
      "blocks.19.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0766, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.1612, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.5107, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1835, device='cuda:0')\n",
      "blocks.20.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.1371, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8223, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7292, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0507, device='cuda:0')\n",
      "blocks.20.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0725, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0411, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.6082, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4147, device='cuda:0')\n",
      "blocks.21.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.2566, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8589, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7254, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0369, device='cuda:0')\n",
      "blocks.21.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0550, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0118, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(2.9092, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.4474, device='cuda:0')\n",
      "blocks.22.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.3173, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7197, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.6838, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0204, device='cuda:0')\n",
      "blocks.22.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0511, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0093, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(2.3585, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1862, device='cuda:0')\n",
      "blocks.23.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0740, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8991, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.8119, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0500, device='cuda:0')\n",
      "blocks.23.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.1179, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0508, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.1172, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1264, device='cuda:0')\n",
      "blocks.24.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0462, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7543, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7233, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0206, device='cuda:0')\n",
      "blocks.24.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0740, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0253, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.7412, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1592, device='cuda:0')\n",
      "blocks.25.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0607, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8118, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7977, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0196, device='cuda:0')\n",
      "blocks.25.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0570, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0965, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(1.4008, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.3048, device='cuda:0')\n",
      "blocks.26.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.1360, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.7854, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7327, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0208, device='cuda:0')\n",
      "blocks.26.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0462, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0133, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.1217, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1298, device='cuda:0')\n",
      "blocks.27.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0378, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8495, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7477, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0203, device='cuda:0')\n",
      "blocks.27.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0382, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0673, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.2781, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.0908, device='cuda:0')\n",
      "blocks.28.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0264, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8755, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7465, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0207, device='cuda:0')\n",
      "blocks.28.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0370, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_Q grad is not all zeros, param.grad.norm()=tensor(0.0097, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_K grad is not all zeros, param.grad.norm()=tensor(0.0849, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_V grad is not all zeros, param.grad.norm()=tensor(0.1065, device='cuda:0')\n",
      "blocks.29.attn.weight_mask_W_O grad is not all zeros, param.grad.norm()=tensor(0.0473, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8123, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.7129, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0166, device='cuda:0')\n",
      "blocks.29.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0395, device='cuda:0')\n",
      "blocks.30.attn.weight_mask_W_Q grad is all zeros\n",
      "blocks.30.attn.weight_mask_W_K grad is all zeros\n",
      "blocks.30.attn.weight_mask_W_V grad is all zeros\n",
      "blocks.30.attn.weight_mask_W_O grad is all zeros\n",
      "blocks.30.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(1.2086, device='cuda:0')\n",
      "blocks.30.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.9111, device='cuda:0')\n",
      "blocks.30.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0195, device='cuda:0')\n",
      "blocks.30.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0239, device='cuda:0')\n",
      "blocks.31.attn.weight_mask_W_Q grad is all zeros\n",
      "blocks.31.attn.weight_mask_W_K grad is all zeros\n",
      "blocks.31.attn.weight_mask_W_V grad is all zeros\n",
      "blocks.31.attn.weight_mask_W_O grad is all zeros\n",
      "blocks.31.mlp.weight_mask_W_in grad is not all zeros, param.grad.norm()=tensor(0.8112, device='cuda:0')\n",
      "blocks.31.mlp.weight_mask_W_out grad is not all zeros, param.grad.norm()=tensor(0.8142, device='cuda:0')\n",
      "blocks.31.mlp.weight_mask_b_in grad is not all zeros, param.grad.norm()=tensor(0.0111, device='cuda:0')\n",
      "blocks.31.mlp.weight_mask_b_out grad is not all zeros, param.grad.norm()=tensor(0.0247, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad: # and \"edge\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'a0.21'),\n",
       " (0, 'a0.30'),\n",
       " (0, 'm0'),\n",
       " (1, 'a1.0'),\n",
       " (1, 'a1.10'),\n",
       " (1, 'a1.11'),\n",
       " (1, 'a1.12'),\n",
       " (1, 'a1.14'),\n",
       " (1, 'a1.15'),\n",
       " (1, 'a1.16'),\n",
       " (1, 'a1.17'),\n",
       " (1, 'a1.18'),\n",
       " (1, 'a1.25'),\n",
       " (1, 'a1.26'),\n",
       " (1, 'a1.29'),\n",
       " (1, 'a1.30'),\n",
       " (1, 'a1.4'),\n",
       " (1, 'a1.6'),\n",
       " (1, 'm1'),\n",
       " (2, 'a2.11'),\n",
       " (2, 'a2.12'),\n",
       " (2, 'm2'),\n",
       " (3, 'a3.23'),\n",
       " (3, 'a3.29'),\n",
       " (3, 'm3'),\n",
       " (4, 'a4.10'),\n",
       " (4, 'a4.13'),\n",
       " (4, 'a4.16'),\n",
       " (4, 'a4.20'),\n",
       " (4, 'a4.25'),\n",
       " (4, 'a4.4'),\n",
       " (4, 'm4'),\n",
       " (5, 'a5.0'),\n",
       " (5, 'a5.17'),\n",
       " (5, 'a5.20'),\n",
       " (5, 'm5'),\n",
       " (6, 'a6.19'),\n",
       " (6, 'a6.6'),\n",
       " (6, 'm6'),\n",
       " (7, 'a7.14'),\n",
       " (7, 'a7.15'),\n",
       " (7, 'a7.20'),\n",
       " (7, 'a7.8'),\n",
       " (7, 'a7.9'),\n",
       " (7, 'm7'),\n",
       " (8, 'a8.11'),\n",
       " (8, 'a8.14'),\n",
       " (8, 'a8.26'),\n",
       " (8, 'a8.6'),\n",
       " (8, 'm8'),\n",
       " (9, 'a9.0'),\n",
       " (9, 'a9.15'),\n",
       " (9, 'a9.19'),\n",
       " (9, 'a9.3'),\n",
       " (9, 'a9.8'),\n",
       " (9, 'a9.9'),\n",
       " (9, 'm9'),\n",
       " (10, 'a10.1'),\n",
       " (10, 'a10.10'),\n",
       " (10, 'a10.11'),\n",
       " (10, 'a10.14'),\n",
       " (10, 'a10.21'),\n",
       " (10, 'a10.26'),\n",
       " (10, 'a10.29'),\n",
       " (10, 'm10'),\n",
       " (11, 'a11.10'),\n",
       " (11, 'a11.12'),\n",
       " (11, 'a11.14'),\n",
       " (11, 'a11.21'),\n",
       " (11, 'a11.24'),\n",
       " (11, 'a11.25'),\n",
       " (11, 'a11.31'),\n",
       " (11, 'a11.8'),\n",
       " (11, 'm11'),\n",
       " (12, 'a12.1'),\n",
       " (12, 'a12.12'),\n",
       " (12, 'a12.15'),\n",
       " (12, 'a12.17'),\n",
       " (12, 'a12.2'),\n",
       " (12, 'a12.23'),\n",
       " (12, 'a12.26'),\n",
       " (12, 'm12'),\n",
       " (13, 'a13.0'),\n",
       " (13, 'a13.12'),\n",
       " (13, 'a13.16'),\n",
       " (13, 'a13.20'),\n",
       " (13, 'a13.25'),\n",
       " (13, 'a13.30'),\n",
       " (13, 'a13.8'),\n",
       " (13, 'm13'),\n",
       " (14, 'a14.12'),\n",
       " (14, 'a14.14'),\n",
       " (14, 'a14.22'),\n",
       " (14, 'a14.23'),\n",
       " (14, 'a14.31'),\n",
       " (14, 'a14.4'),\n",
       " (14, 'a14.8'),\n",
       " (14, 'm14'),\n",
       " (15, 'a15.11'),\n",
       " (15, 'a15.18'),\n",
       " (15, 'a15.20'),\n",
       " (15, 'a15.27'),\n",
       " (15, 'a15.28'),\n",
       " (15, 'a15.4'),\n",
       " (15, 'a15.5'),\n",
       " (15, 'a15.6'),\n",
       " (15, 'm15'),\n",
       " (16, 'a16.10'),\n",
       " (16, 'a16.17'),\n",
       " (16, 'a16.20'),\n",
       " (16, 'a16.21'),\n",
       " (16, 'a16.29'),\n",
       " (16, 'm16'),\n",
       " (17, 'a17.11'),\n",
       " (17, 'a17.21'),\n",
       " (17, 'a17.28'),\n",
       " (17, 'a17.30'),\n",
       " (17, 'a17.6'),\n",
       " (17, 'm17'),\n",
       " (18, 'a18.31'),\n",
       " (18, 'm18'),\n",
       " (19, 'a19.24'),\n",
       " (19, 'a19.30'),\n",
       " (19, 'm19'),\n",
       " (20, 'a20.17'),\n",
       " (20, 'a20.2'),\n",
       " (20, 'a20.8'),\n",
       " (20, 'm20'),\n",
       " (21, 'a21.19'),\n",
       " (21, 'a21.23'),\n",
       " (21, 'a21.31'),\n",
       " (21, 'a21.9'),\n",
       " (21, 'm21'),\n",
       " (22, 'a22.15'),\n",
       " (22, 'a22.17'),\n",
       " (22, 'a22.24'),\n",
       " (22, 'm22'),\n",
       " (23, 'a23.26'),\n",
       " (23, 'm23'),\n",
       " (24, 'a24.21'),\n",
       " (24, 'm24'),\n",
       " (25, 'a25.1'),\n",
       " (25, 'a25.17'),\n",
       " (25, 'm25'),\n",
       " (26, 'a26.0'),\n",
       " (26, 'a26.25'),\n",
       " (26, 'a26.4'),\n",
       " (26, 'm26'),\n",
       " (27, 'a27.13'),\n",
       " (27, 'm27'),\n",
       " (28, 'a28.19'),\n",
       " (28, 'm28'),\n",
       " (29, 'a29.13'),\n",
       " (29, 'm29'),\n",
       " (30, 'm30'),\n",
       " (31, 'm31')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdcpp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='blocks.29.attn.weight_mask_W_O', nonzero indices: (tensor([13, 13, 13,  ..., 13, 13, 13], device='cuda:0'), tensor([ 0,  0,  0,  ..., 79, 79, 79], device='cuda:0'), tensor([   0,    1,    2,  ..., 2557, 2558, 2559], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and \"blocks.29.attn.weight_mask_W_O\" in name:\n",
    "        zero_indices = torch.nonzero(param.grad != 0, as_tuple=True)\n",
    "        print(f\"{name=}, nonzero indices: {zero_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if correct MLPs flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'm2')\n",
      "(10, 'm10')\n",
      "(5, 'm5')\n",
      "(3, 'm3')\n",
      "(7, 'm7')\n",
      "(1, 'm1')\n",
      "(9, 'm9')\n",
      "(8, 'm8')\n",
      "(4, 'm4')\n",
      "(0, 'm0')\n",
      "(-1, 'embed')\n",
      "(6, 'm6')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"m\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.edge_mask_mlp grad is all zeros\n",
      "blocks.0.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.0.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.0.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.5009, device='cuda:0')\n",
      "blocks.0.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3840, device='cuda:0')\n",
      "blocks.0.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9580, device='cuda:0')\n",
      "blocks.0.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2079, device='cuda:0')\n",
      "blocks.1.edge_mask_mlp grad is all zeros\n",
      "blocks.1.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.1.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.1.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1881, device='cuda:0')\n",
      "blocks.1.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4780, device='cuda:0')\n",
      "blocks.1.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.0281, device='cuda:0')\n",
      "blocks.1.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2563, device='cuda:0')\n",
      "blocks.2.edge_mask_mlp grad is all zeros\n",
      "blocks.2.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.2.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.2.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7383, device='cuda:0')\n",
      "blocks.2.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4888, device='cuda:0')\n",
      "blocks.2.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4441, device='cuda:0')\n",
      "blocks.2.mlp.b_out grad is all zeros\n",
      "blocks.3.edge_mask_mlp grad is all zeros\n",
      "blocks.3.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.3.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.3.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1551, device='cuda:0')\n",
      "blocks.3.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3966, device='cuda:0')\n",
      "blocks.3.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.1005, device='cuda:0')\n",
      "blocks.3.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2987, device='cuda:0')\n",
      "blocks.4.edge_mask_mlp grad is all zeros\n",
      "blocks.4.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.4.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.4.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7344, device='cuda:0')\n",
      "blocks.4.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5089, device='cuda:0')\n",
      "blocks.4.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.5127, device='cuda:0')\n",
      "blocks.4.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3237, device='cuda:0')\n",
      "blocks.5.edge_mask_mlp grad is all zeros\n",
      "blocks.5.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.5.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.5.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8992, device='cuda:0')\n",
      "blocks.5.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5541, device='cuda:0')\n",
      "blocks.5.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4629, device='cuda:0')\n",
      "blocks.5.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3042, device='cuda:0')\n",
      "blocks.6.edge_mask_mlp grad is all zeros\n",
      "blocks.6.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.6.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.6.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.0009, device='cuda:0')\n",
      "blocks.6.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5724, device='cuda:0')\n",
      "blocks.6.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4607, device='cuda:0')\n",
      "blocks.6.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2637, device='cuda:0')\n",
      "blocks.7.edge_mask_mlp grad is all zeros\n",
      "blocks.7.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.7.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.7.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.3364, device='cuda:0')\n",
      "blocks.7.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4871, device='cuda:0')\n",
      "blocks.7.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9889, device='cuda:0')\n",
      "blocks.7.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2189, device='cuda:0')\n",
      "blocks.8.edge_mask_mlp grad is all zeros\n",
      "blocks.8.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.8.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.8.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8043, device='cuda:0')\n",
      "blocks.8.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6429, device='cuda:0')\n",
      "blocks.8.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.5990, device='cuda:0')\n",
      "blocks.8.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1610, device='cuda:0')\n",
      "blocks.9.edge_mask_mlp grad is all zeros\n",
      "blocks.9.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.9.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.9.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1774, device='cuda:0')\n",
      "blocks.9.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4569, device='cuda:0')\n",
      "blocks.9.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.3034, device='cuda:0')\n",
      "blocks.9.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1200, device='cuda:0')\n",
      "blocks.10.edge_mask_mlp grad is all zeros\n",
      "blocks.10.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.10.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.10.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.3321, device='cuda:0')\n",
      "blocks.10.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.2768, device='cuda:0')\n",
      "blocks.10.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(0.9347, device='cuda:0')\n",
      "blocks.10.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.0822, device='cuda:0')\n",
      "blocks.11.edge_mask_mlp grad is all zeros\n",
      "blocks.11.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.11.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.11.mlp.W_in grad is all zeros\n",
      "blocks.11.mlp.b_in grad is all zeros\n",
      "blocks.11.mlp.W_out grad is all zeros\n",
      "blocks.11.mlp.b_out grad is all zeros\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"mlp\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if individual attention heads have gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 'a9.7')\n",
      "(9, 'a9.9')\n",
      "(9, 'a9.6')\n",
      "(9, 'a9.8')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"a9\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1051, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0287, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1273, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0335, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0570, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0485, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 64, 768])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0306, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0389, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and \"9.attn.weight\" in name:\n",
    "    # if param.requires_grad and \"9.attn.b\" in name:\n",
    "        print(param.shape)\n",
    "        # if param.grad is not None and param.grad.sum() != 0:\n",
    "        #     print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        # else:\n",
    "        #     print(f\"{name} grad is all zeros\")\n",
    "        print(f\"{param.grad[3].norm()=}\")\n",
    "        print(f\"{param.grad[4].norm()=}\")\n",
    "        print(f\"{param.grad[7].norm()=}\")\n",
    "        print(f\"{param.grad[8].norm()=}\")\n",
    "        print(f\"{param.grad[11].norm()=}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Mask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from tasks import IOITask, SportsTask, OWTTask\n",
    "batch_size = 64\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=0)\n",
    "sports = SportsTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "owt = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "\n",
    "ioi_ood = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=1) # different template\n",
    "\n",
    "train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "task_weights = {\"ioi\": -.2, \"owt\": 1} # I think means preserve OWT, corrupt IOI\n",
    "eval_tasks = {\"ioi\": ioi, \"sports\": sports, \"owt\": owt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilliphguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/phillip_guo/mechanistic-unlearning/wandb/run-20240110_085753-mznr9fva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">vocal-deluge-22</a></strong> to <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 32/501 [10:48<2:38:21, 20.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_93609/3197613862.py\", line 15, in <module>\n",
      "    train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
      "  File \"/data/phillip_guo/mechanistic-unlearning/cb_utils/learn_mask.py\", line 178, in train_masks\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.learn_mask import train_masks\n",
    "\n",
    "epochs_left = 500\n",
    "steps_per_epoch = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "evaluate_every = 1\n",
    "discretize_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "use_wandb = False\n",
    "edge_mask_reg_strength = None\n",
    "weight_mask_reg_strength = 10\n",
    "\n",
    "wandb_config = {\"edge_masks\": edge_masks, \"weight_masks_attn\": weight_masks_attn, \"weight_masks_mlp\": weight_masks_mlp, \"epochs\": epochs_left, \"steps_per_epoch\": steps_per_epoch, \"lr\": lr, \"weight_decay\": weight_decay, \"evaluate_every\": evaluate_every, \"discretize_every\": discretize_every, \"threshold\": threshold, \"edge_mask_reg_strength\": edge_mask_reg_strength, \"weight_mask_reg_strength\": weight_mask_reg_strength}\n",
    "\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
    "            # param_names=param_names, mask_params=mask_params, \n",
    "            task_weights=task_weights, eval_tasks=eval_tasks, evaluate_every=evaluate_every, discretize_every=discretize_every, threshold=threshold, edge_mask_reg_strength=edge_mask_reg_strength, weight_mask_reg_strength=None, verbose=False, use_wandb=use_wandb, wandb_config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"masks/trained_mask_params_{epochs_left=}_{edge_mask_reg_strength=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, p in zip(param_names, mask_params):\n",
    "    if p.requires_grad:\n",
    "        # print(name, p)\n",
    "        # count how many zeros in p\n",
    "        print(torch.sum(p == 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
