{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:46<00:00, 23.12s/it]\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "model_type = \"gemma-2b\"\n",
    "\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "if model_type == \"pythia\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8B\")#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8B\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "elif model_type == \"gemma-7b\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.bfloat16)#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "elif model_type == \"gemma-2b\":\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=torch.bfloat16)#.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    'google/gemma-2b',\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda',\n",
    "    default_padding_side=\"right\",\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    center_writing_weights=False,\n",
    "    dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m0: -0.055645283311605453\n",
      "m1: 0.0552063025534153\n",
      "m2: -0.11303359270095825\n",
      "m3: 0.024208657443523407\n",
      "m4: -0.0113525390625\n",
      "m5: -0.022718576714396477\n",
      "m6: -0.007042518351227045\n",
      "m7: -0.021432731300592422\n",
      "m8: -0.006188026163727045\n",
      "m9: -0.0019231943879276514\n",
      "m10: -0.03130634129047394\n",
      "m11: -0.0708770751953125\n",
      "m12: -0.04879526048898697\n",
      "m13: 0.04687969759106636\n",
      "m14: 0.035638369619846344\n",
      "m15: 0.02344219572842121\n",
      "m16: -0.018733099102973938\n",
      "m17: -0.06601186841726303\n",
      "m18: -0.10868014395236969\n",
      "m19: -0.0049954927526414394\n",
      "m20: -0.08375901728868484\n",
      "m21: -0.25811299681663513\n",
      "m22: -0.11271785199642181\n",
      "m23: -0.23159556090831757\n",
      "m24: -0.09144005924463272\n",
      "m25: -0.08293269574642181\n",
      "m26: 0.18458910286426544\n",
      "m27: 0.5513822436332703\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"models/google_gemma-7b_sports_baseball_ap_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)\n",
    "for component in ap_graph:\n",
    "    if \"m\" in component:\n",
    "        print(f\"{component}: {ap_graph[component]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: apply masks before doing any calculations\n",
    "this succeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_test_weight_mask_dicts(model):\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        # Want bool of length n_head, randomly set to True\n",
    "        weight_mask_attn_dict[layer]['W_Q'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_K'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_V'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_O'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "\n",
    "        # Randomly set to true or false\n",
    "        weight_mask_mlp_dict[layer] = random.randint(0, 1) == 1\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def make_partly_differentiable_mask(W, unfrozen_heads, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    W is Parameter of shape (n_heads, ...). Returns baseline and frozen (both only 1d arrays of (n_heads,)), and forward pass should be W_baseline.float() + W_frozen.float() * W \n",
    "    \"\"\"\n",
    "    W_baseline = torch.nn.Parameter(torch.zeros(W.shape[0], dtype=torch.bool), requires_grad=False).to(device)\n",
    "\n",
    "    # unsqueeze to broadcast efficiently, until W_baseline has same shape as W\n",
    "    while len(W_baseline.shape) < len(W.shape):\n",
    "        W_baseline = W_baseline.unsqueeze(-1)\n",
    "    \n",
    "    W_baseline[unfrozen_heads] = True\n",
    "    # W_baseline = ~W_frozen\n",
    "    W_frozen = torch.nn.Parameter(~W_baseline, requires_grad=False)\n",
    "    # convert into float\n",
    "    return W_frozen.float(), W_baseline.float()\n",
    "\n",
    "class WeightMaskedTransformer(nn.Module):\n",
    "    def __init__(self, tl_transformer, weight_mask_attn_dict=None, weight_mask_mlp_dict=None, torch_dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        weight_mask_attn_dict: {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}} (frozen_heads is shape (n_heads,) of bools). If none, train mask over all heads\n",
    "        weight_mask_mlp_dict: {layer: bool}. If none, train mask over all mlps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.torch_dtype = torch_dtype\n",
    "        # tl_transformer should be a HookedTransformer\n",
    "        self.tl_transformer = tl_transformer\n",
    "        # turn off gradients for tl_transformer\n",
    "        # for param in self.tl_transformer.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.weight_mask_attn_dict = weight_mask_attn_dict\n",
    "        self.weight_mask_mlp_dict = weight_mask_mlp_dict\n",
    "        # store weight masks for every component that is unfrozen\n",
    "        \n",
    "        # need to store reference weights so that you can reset W_Q, etc after a forward pass\n",
    "        self.reference_attn_weights = {}\n",
    "        self.reference_mlp_weights = {}\n",
    "\n",
    "        self.attention_masks = {}\n",
    "        self.mlp_masks = {}\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            self.attention_masks[layer] = {}\n",
    "            self.reference_attn_weights[layer] = {}\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None:\n",
    "                    unfrozen_heads = list(range(self.tl_transformer.cfg.n_heads)) # all heads are unfrozen\n",
    "                else:\n",
    "                    unfrozen_heads = self.weight_mask_attn_dict[layer][component]\n",
    "                # make frozen and baseline masks, and also a copy of the original weights\n",
    "\n",
    "                if len(unfrozen_heads) > 0:\n",
    "                    W_frozen, W_baseline = make_partly_differentiable_mask(parameter, unfrozen_heads)\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "                    \n",
    "                    self.attention_masks[layer][component] = (W_frozen, W_baseline, weight_mask)\n",
    "                    self.reference_attn_weights[layer][component] = parameter.clone()\n",
    "\n",
    "            if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer]:\n",
    "                in_weight_mask = nn.Parameter(torch.ones_like(self.tl_transformer.blocks[layer].mlp.W_in).type(torch_dtype), requires_grad=True)\n",
    "                out_weight_mask = nn.Parameter(torch.ones_like(self.tl_transformer.blocks[layer].mlp.W_out).type(torch_dtype), requires_grad=True)\n",
    "\n",
    "                self.mlp_masks[layer] = (in_weight_mask, out_weight_mask)\n",
    "                self.reference_mlp_weights[layer] = (self.tl_transformer.blocks[layer].mlp.W_in.clone(), self.tl_transformer.blocks[layer].mlp.W_out.clone())\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        masked_weights = {}\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            masked_weights[layer] = {}\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or len(self.attention_masks[layer]) > 0:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    reference_data = self.reference_attn_weights[layer][component]\n",
    "                    mask = W_baseline + W_frozen * weight_mask\n",
    "                    masked_weights[layer][component] = reference_data * mask\n",
    "                    self.tl_transformer.blocks[layer].attn.W_Q.data.copy_(masked_weights[layer][\"W_Q\"])\n",
    "\n",
    "            if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer]:\n",
    "                in_weight_mask, out_weight_mask = self.mlp_masks[layer]\n",
    "                reference_in_data, reference_out_data = self.reference_mlp_weights[layer]\n",
    "                masked_weights[layer][\"W_in\"] = reference_in_data * in_weight_mask\n",
    "                masked_weights[layer][\"W_out\"] = reference_out_data * out_weight_mask\n",
    "\n",
    "        def custom_forward(x):\n",
    "            for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "                if \"W_Q\" in masked_weights[layer]:\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters']['W_Q'] = masked_weights[layer][\"W_Q\"]\n",
    "                if \"W_K\" in masked_weights[layer]:\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters']['W_K'] = masked_weights[layer][\"W_K\"]\n",
    "                if \"W_V\" in masked_weights[layer]:\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters']['W_V'] = masked_weights[layer][\"W_V\"]\n",
    "                if \"W_O\" in masked_weights[layer]:\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters']['W_O'] = masked_weights[layer][\"W_O\"]\n",
    "                if \"W_in\" in masked_weights[layer]:\n",
    "                    self.tl_transformer.blocks[layer].mlp.__dict__['_parameters']['W_in'] = masked_weights[layer][\"W_in\"]\n",
    "                if \"W_out\" in masked_weights[layer]:\n",
    "                    self.tl_transformer.blocks[layer].mlp.__dict__['_parameters']['W_out'] = masked_weights[layer][\"W_out\"]\n",
    "            return self.tl_transformer(x)\n",
    "\n",
    "        return custom_forward(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mask_attns, weight_mask_mlps = create_test_weight_mask_dicts(tl_model)\n",
    "\n",
    "wmt = WeightMaskedTransformer(tl_model, weight_mask_attn_dict=weight_mask_attns, weight_mask_mlp_dict=weight_mask_mlps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (1252, 8), test_df: (314, 8)\n",
      " basketball baseball football basketball football football football baseball football baseball basketball basketball football football baseball football basketball baseball baseball football football basketball football football baseball football basketball basketball basketball football baseball football baseball football basketball basketball football basketball football baseball basketball football basketball football basketball football baseball basketball basketball baseball baseball baseball basketball basketball football football basketball football basketball football football baseball football football\n",
      "tensor(0.3719, device='cuda:0')\n",
      " basketball football football baseball baseball basketball football football basketball baseball basketball football baseball football basketball basketball football football football basketball baseball baseball basketball football football baseball baseball football baseball football football baseball basketball football baseball football basketball basketball basketball baseball baseball football football basketball baseball baseball baseball baseball basketball baseball baseball baseball football baseball baseball baseball football basketball baseball baseball basketball baseball basketball football\n",
      "tensor(0.4538, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sports_test = SportsTask(batch_size=64, tokenizer=tokenizer)\n",
    "# print(sports_test.get_test_loss(tl_model))\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    print(sports_test.get_test_loss(tl_model))\n",
    "    print(sports_test.get_test_loss(wmt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated() // 1024**3)\n",
    "print(torch.cuda.max_memory_allocated() // 1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that gradients flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tl_model.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (1252, 8), test_df: (314, 8)\n",
      "tensor(0.3264, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sports_train = SportsTask(batch_size=3, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(tl_model, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    # loss = sports_train.get_train_loss(wmt, 1)\n",
    "    # print(loss)\n",
    "    # loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([256000, 3072])\n",
      "blocks.0.ln1.w torch.Size([3072])\n",
      "blocks.0.ln2.w torch.Size([3072])\n",
      "blocks.0.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.0.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.0.attn.b_Q torch.Size([16, 256])\n",
      "blocks.0.attn.b_O torch.Size([3072])\n",
      "blocks.0.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.0.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.0.attn._b_K torch.Size([16, 256])\n",
      "blocks.0.attn._b_V torch.Size([16, 256])\n",
      "blocks.0.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.0.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.0.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([24576])\n",
      "blocks.0.mlp.b_out torch.Size([3072])\n",
      "blocks.1.ln1.w torch.Size([3072])\n",
      "blocks.1.ln2.w torch.Size([3072])\n",
      "blocks.1.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.1.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.1.attn.b_Q torch.Size([16, 256])\n",
      "blocks.1.attn.b_O torch.Size([3072])\n",
      "blocks.1.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.1.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.1.attn._b_K torch.Size([16, 256])\n",
      "blocks.1.attn._b_V torch.Size([16, 256])\n",
      "blocks.1.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.1.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.1.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.1.mlp.b_in torch.Size([24576])\n",
      "blocks.1.mlp.b_out torch.Size([3072])\n",
      "blocks.2.ln1.w torch.Size([3072])\n",
      "blocks.2.ln2.w torch.Size([3072])\n",
      "blocks.2.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.2.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.2.attn.b_Q torch.Size([16, 256])\n",
      "blocks.2.attn.b_O torch.Size([3072])\n",
      "blocks.2.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.2.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.2.attn._b_K torch.Size([16, 256])\n",
      "blocks.2.attn._b_V torch.Size([16, 256])\n",
      "blocks.2.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.2.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.2.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.2.mlp.b_in torch.Size([24576])\n",
      "blocks.2.mlp.b_out torch.Size([3072])\n",
      "blocks.3.ln1.w torch.Size([3072])\n",
      "blocks.3.ln2.w torch.Size([3072])\n",
      "blocks.3.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.3.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.3.attn.b_Q torch.Size([16, 256])\n",
      "blocks.3.attn.b_O torch.Size([3072])\n",
      "blocks.3.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.3.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.3.attn._b_K torch.Size([16, 256])\n",
      "blocks.3.attn._b_V torch.Size([16, 256])\n",
      "blocks.3.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.3.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.3.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.3.mlp.b_in torch.Size([24576])\n",
      "blocks.3.mlp.b_out torch.Size([3072])\n",
      "blocks.4.ln1.w torch.Size([3072])\n",
      "blocks.4.ln2.w torch.Size([3072])\n",
      "blocks.4.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.4.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.4.attn.b_Q torch.Size([16, 256])\n",
      "blocks.4.attn.b_O torch.Size([3072])\n",
      "blocks.4.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.4.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.4.attn._b_K torch.Size([16, 256])\n",
      "blocks.4.attn._b_V torch.Size([16, 256])\n",
      "blocks.4.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.4.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.4.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.4.mlp.b_in torch.Size([24576])\n",
      "blocks.4.mlp.b_out torch.Size([3072])\n",
      "blocks.5.ln1.w torch.Size([3072])\n",
      "blocks.5.ln2.w torch.Size([3072])\n",
      "blocks.5.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.5.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.5.attn.b_Q torch.Size([16, 256])\n",
      "blocks.5.attn.b_O torch.Size([3072])\n",
      "blocks.5.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.5.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.5.attn._b_K torch.Size([16, 256])\n",
      "blocks.5.attn._b_V torch.Size([16, 256])\n",
      "blocks.5.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.5.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.5.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.5.mlp.b_in torch.Size([24576])\n",
      "blocks.5.mlp.b_out torch.Size([3072])\n",
      "blocks.6.ln1.w torch.Size([3072])\n",
      "blocks.6.ln2.w torch.Size([3072])\n",
      "blocks.6.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.6.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.6.attn.b_Q torch.Size([16, 256])\n",
      "blocks.6.attn.b_O torch.Size([3072])\n",
      "blocks.6.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.6.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.6.attn._b_K torch.Size([16, 256])\n",
      "blocks.6.attn._b_V torch.Size([16, 256])\n",
      "blocks.6.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.6.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.6.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.6.mlp.b_in torch.Size([24576])\n",
      "blocks.6.mlp.b_out torch.Size([3072])\n",
      "blocks.7.ln1.w torch.Size([3072])\n",
      "blocks.7.ln2.w torch.Size([3072])\n",
      "blocks.7.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.7.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.7.attn.b_Q torch.Size([16, 256])\n",
      "blocks.7.attn.b_O torch.Size([3072])\n",
      "blocks.7.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.7.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.7.attn._b_K torch.Size([16, 256])\n",
      "blocks.7.attn._b_V torch.Size([16, 256])\n",
      "blocks.7.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.7.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.7.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.7.mlp.b_in torch.Size([24576])\n",
      "blocks.7.mlp.b_out torch.Size([3072])\n",
      "blocks.8.ln1.w torch.Size([3072])\n",
      "blocks.8.ln2.w torch.Size([3072])\n",
      "blocks.8.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.8.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.8.attn.b_Q torch.Size([16, 256])\n",
      "blocks.8.attn.b_O torch.Size([3072])\n",
      "blocks.8.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.8.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.8.attn._b_K torch.Size([16, 256])\n",
      "blocks.8.attn._b_V torch.Size([16, 256])\n",
      "blocks.8.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.8.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.8.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.8.mlp.b_in torch.Size([24576])\n",
      "blocks.8.mlp.b_out torch.Size([3072])\n",
      "blocks.9.ln1.w torch.Size([3072])\n",
      "blocks.9.ln2.w torch.Size([3072])\n",
      "blocks.9.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.9.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.9.attn.b_Q torch.Size([16, 256])\n",
      "blocks.9.attn.b_O torch.Size([3072])\n",
      "blocks.9.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.9.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.9.attn._b_K torch.Size([16, 256])\n",
      "blocks.9.attn._b_V torch.Size([16, 256])\n",
      "blocks.9.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.9.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.9.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.9.mlp.b_in torch.Size([24576])\n",
      "blocks.9.mlp.b_out torch.Size([3072])\n",
      "blocks.10.ln1.w torch.Size([3072])\n",
      "blocks.10.ln2.w torch.Size([3072])\n",
      "blocks.10.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.10.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.10.attn.b_Q torch.Size([16, 256])\n",
      "blocks.10.attn.b_O torch.Size([3072])\n",
      "blocks.10.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.10.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.10.attn._b_K torch.Size([16, 256])\n",
      "blocks.10.attn._b_V torch.Size([16, 256])\n",
      "blocks.10.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.10.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.10.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.10.mlp.b_in torch.Size([24576])\n",
      "blocks.10.mlp.b_out torch.Size([3072])\n",
      "blocks.11.ln1.w torch.Size([3072])\n",
      "blocks.11.ln2.w torch.Size([3072])\n",
      "blocks.11.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.11.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.11.attn.b_Q torch.Size([16, 256])\n",
      "blocks.11.attn.b_O torch.Size([3072])\n",
      "blocks.11.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.11.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.11.attn._b_K torch.Size([16, 256])\n",
      "blocks.11.attn._b_V torch.Size([16, 256])\n",
      "blocks.11.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.11.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.11.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.11.mlp.b_in torch.Size([24576])\n",
      "blocks.11.mlp.b_out torch.Size([3072])\n",
      "blocks.12.ln1.w torch.Size([3072])\n",
      "blocks.12.ln2.w torch.Size([3072])\n",
      "blocks.12.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.12.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.12.attn.b_Q torch.Size([16, 256])\n",
      "blocks.12.attn.b_O torch.Size([3072])\n",
      "blocks.12.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.12.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.12.attn._b_K torch.Size([16, 256])\n",
      "blocks.12.attn._b_V torch.Size([16, 256])\n",
      "blocks.12.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.12.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.12.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.12.mlp.b_in torch.Size([24576])\n",
      "blocks.12.mlp.b_out torch.Size([3072])\n",
      "blocks.13.ln1.w torch.Size([3072])\n",
      "blocks.13.ln2.w torch.Size([3072])\n",
      "blocks.13.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.13.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.13.attn.b_Q torch.Size([16, 256])\n",
      "blocks.13.attn.b_O torch.Size([3072])\n",
      "blocks.13.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.13.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.13.attn._b_K torch.Size([16, 256])\n",
      "blocks.13.attn._b_V torch.Size([16, 256])\n",
      "blocks.13.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.13.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.13.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.13.mlp.b_in torch.Size([24576])\n",
      "blocks.13.mlp.b_out torch.Size([3072])\n",
      "blocks.14.ln1.w torch.Size([3072])\n",
      "blocks.14.ln2.w torch.Size([3072])\n",
      "blocks.14.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.14.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.14.attn.b_Q torch.Size([16, 256])\n",
      "blocks.14.attn.b_O torch.Size([3072])\n",
      "blocks.14.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.14.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.14.attn._b_K torch.Size([16, 256])\n",
      "blocks.14.attn._b_V torch.Size([16, 256])\n",
      "blocks.14.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.14.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.14.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.14.mlp.b_in torch.Size([24576])\n",
      "blocks.14.mlp.b_out torch.Size([3072])\n",
      "blocks.15.ln1.w torch.Size([3072])\n",
      "blocks.15.ln2.w torch.Size([3072])\n",
      "blocks.15.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.15.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.15.attn.b_Q torch.Size([16, 256])\n",
      "blocks.15.attn.b_O torch.Size([3072])\n",
      "blocks.15.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.15.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.15.attn._b_K torch.Size([16, 256])\n",
      "blocks.15.attn._b_V torch.Size([16, 256])\n",
      "blocks.15.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.15.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.15.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.15.mlp.b_in torch.Size([24576])\n",
      "blocks.15.mlp.b_out torch.Size([3072])\n",
      "blocks.16.ln1.w torch.Size([3072])\n",
      "blocks.16.ln2.w torch.Size([3072])\n",
      "blocks.16.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.16.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.16.attn.b_Q torch.Size([16, 256])\n",
      "blocks.16.attn.b_O torch.Size([3072])\n",
      "blocks.16.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.16.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.16.attn._b_K torch.Size([16, 256])\n",
      "blocks.16.attn._b_V torch.Size([16, 256])\n",
      "blocks.16.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.16.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.16.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.16.mlp.b_in torch.Size([24576])\n",
      "blocks.16.mlp.b_out torch.Size([3072])\n",
      "blocks.17.ln1.w torch.Size([3072])\n",
      "blocks.17.ln2.w torch.Size([3072])\n",
      "blocks.17.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.17.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.17.attn.b_Q torch.Size([16, 256])\n",
      "blocks.17.attn.b_O torch.Size([3072])\n",
      "blocks.17.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.17.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.17.attn._b_K torch.Size([16, 256])\n",
      "blocks.17.attn._b_V torch.Size([16, 256])\n",
      "blocks.17.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.17.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.17.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.17.mlp.b_in torch.Size([24576])\n",
      "blocks.17.mlp.b_out torch.Size([3072])\n",
      "blocks.18.ln1.w torch.Size([3072])\n",
      "blocks.18.ln2.w torch.Size([3072])\n",
      "blocks.18.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.18.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.18.attn.b_Q torch.Size([16, 256])\n",
      "blocks.18.attn.b_O torch.Size([3072])\n",
      "blocks.18.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.18.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.18.attn._b_K torch.Size([16, 256])\n",
      "blocks.18.attn._b_V torch.Size([16, 256])\n",
      "blocks.18.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.18.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.18.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.18.mlp.b_in torch.Size([24576])\n",
      "blocks.18.mlp.b_out torch.Size([3072])\n",
      "blocks.19.ln1.w torch.Size([3072])\n",
      "blocks.19.ln2.w torch.Size([3072])\n",
      "blocks.19.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.19.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.19.attn.b_Q torch.Size([16, 256])\n",
      "blocks.19.attn.b_O torch.Size([3072])\n",
      "blocks.19.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.19.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.19.attn._b_K torch.Size([16, 256])\n",
      "blocks.19.attn._b_V torch.Size([16, 256])\n",
      "blocks.19.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.19.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.19.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.19.mlp.b_in torch.Size([24576])\n",
      "blocks.19.mlp.b_out torch.Size([3072])\n",
      "blocks.20.ln1.w torch.Size([3072])\n",
      "blocks.20.ln2.w torch.Size([3072])\n",
      "blocks.20.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.20.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.20.attn.b_Q torch.Size([16, 256])\n",
      "blocks.20.attn.b_O torch.Size([3072])\n",
      "blocks.20.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.20.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.20.attn._b_K torch.Size([16, 256])\n",
      "blocks.20.attn._b_V torch.Size([16, 256])\n",
      "blocks.20.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.20.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.20.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.20.mlp.b_in torch.Size([24576])\n",
      "blocks.20.mlp.b_out torch.Size([3072])\n",
      "blocks.21.ln1.w torch.Size([3072])\n",
      "blocks.21.ln2.w torch.Size([3072])\n",
      "blocks.21.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.21.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.21.attn.b_Q torch.Size([16, 256])\n",
      "blocks.21.attn.b_O torch.Size([3072])\n",
      "blocks.21.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.21.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.21.attn._b_K torch.Size([16, 256])\n",
      "blocks.21.attn._b_V torch.Size([16, 256])\n",
      "blocks.21.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.21.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.21.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.21.mlp.b_in torch.Size([24576])\n",
      "blocks.21.mlp.b_out torch.Size([3072])\n",
      "blocks.22.ln1.w torch.Size([3072])\n",
      "blocks.22.ln2.w torch.Size([3072])\n",
      "blocks.22.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.22.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.22.attn.b_Q torch.Size([16, 256])\n",
      "blocks.22.attn.b_O torch.Size([3072])\n",
      "blocks.22.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.22.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.22.attn._b_K torch.Size([16, 256])\n",
      "blocks.22.attn._b_V torch.Size([16, 256])\n",
      "blocks.22.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.22.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.22.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.22.mlp.b_in torch.Size([24576])\n",
      "blocks.22.mlp.b_out torch.Size([3072])\n",
      "blocks.23.ln1.w torch.Size([3072])\n",
      "blocks.23.ln2.w torch.Size([3072])\n",
      "blocks.23.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.23.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.23.attn.b_Q torch.Size([16, 256])\n",
      "blocks.23.attn.b_O torch.Size([3072])\n",
      "blocks.23.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.23.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.23.attn._b_K torch.Size([16, 256])\n",
      "blocks.23.attn._b_V torch.Size([16, 256])\n",
      "blocks.23.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.23.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.23.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.23.mlp.b_in torch.Size([24576])\n",
      "blocks.23.mlp.b_out torch.Size([3072])\n",
      "blocks.24.ln1.w torch.Size([3072])\n",
      "blocks.24.ln2.w torch.Size([3072])\n",
      "blocks.24.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.24.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.24.attn.b_Q torch.Size([16, 256])\n",
      "blocks.24.attn.b_O torch.Size([3072])\n",
      "blocks.24.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.24.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.24.attn._b_K torch.Size([16, 256])\n",
      "blocks.24.attn._b_V torch.Size([16, 256])\n",
      "blocks.24.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.24.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.24.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.24.mlp.b_in torch.Size([24576])\n",
      "blocks.24.mlp.b_out torch.Size([3072])\n",
      "blocks.25.ln1.w torch.Size([3072])\n",
      "blocks.25.ln2.w torch.Size([3072])\n",
      "blocks.25.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.25.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.25.attn.b_Q torch.Size([16, 256])\n",
      "blocks.25.attn.b_O torch.Size([3072])\n",
      "blocks.25.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.25.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.25.attn._b_K torch.Size([16, 256])\n",
      "blocks.25.attn._b_V torch.Size([16, 256])\n",
      "blocks.25.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.25.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.25.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.25.mlp.b_in torch.Size([24576])\n",
      "blocks.25.mlp.b_out torch.Size([3072])\n",
      "blocks.26.ln1.w torch.Size([3072])\n",
      "blocks.26.ln2.w torch.Size([3072])\n",
      "blocks.26.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.26.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.26.attn.b_Q torch.Size([16, 256])\n",
      "blocks.26.attn.b_O torch.Size([3072])\n",
      "blocks.26.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.26.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.26.attn._b_K torch.Size([16, 256])\n",
      "blocks.26.attn._b_V torch.Size([16, 256])\n",
      "blocks.26.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.26.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.26.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.26.mlp.b_in torch.Size([24576])\n",
      "blocks.26.mlp.b_out torch.Size([3072])\n",
      "blocks.27.ln1.w torch.Size([3072])\n",
      "blocks.27.ln2.w torch.Size([3072])\n",
      "blocks.27.attn.W_Q torch.Size([16, 3072, 256])\n",
      "blocks.27.attn.W_O torch.Size([16, 256, 3072])\n",
      "blocks.27.attn.b_Q torch.Size([16, 256])\n",
      "blocks.27.attn.b_O torch.Size([3072])\n",
      "blocks.27.attn._W_K torch.Size([16, 3072, 256])\n",
      "blocks.27.attn._W_V torch.Size([16, 3072, 256])\n",
      "blocks.27.attn._b_K torch.Size([16, 256])\n",
      "blocks.27.attn._b_V torch.Size([16, 256])\n",
      "blocks.27.mlp.W_in torch.Size([3072, 24576])\n",
      "blocks.27.mlp.W_gate torch.Size([3072, 24576])\n",
      "blocks.27.mlp.W_out torch.Size([24576, 3072])\n",
      "blocks.27.mlp.b_in torch.Size([24576])\n",
      "blocks.27.mlp.b_out torch.Size([3072])\n",
      "ln_final.w torch.Size([3072])\n",
      "unembed.W_U torch.Size([3072, 256000])\n",
      "unembed.b_U torch.Size([256000])\n"
     ]
    }
   ],
   "source": [
    "for name, param in tl_model.named_parameters():\n",
    "    print(name, param.grad.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero grad tl_model\n",
    "for param in tl_model.parameters():\n",
    "    param.grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (1252, 8), test_df: (314, 8)\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sports_train = SportsTask(batch_size=8, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(wmt, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4575/1289788497.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  tl_model.blocks[7].attn.W_K.grad\n"
     ]
    }
   ],
   "source": [
    "tl_model.blocks[7].attn.W_K.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 3.2037e-07,  3.6322e-07, -4.2282e-07,  ...,  1.4305e-06,\n",
      "           1.5870e-06,  4.4405e-06],\n",
      "         [-7.4040e-08, -5.5507e-07, -2.7567e-06,  ..., -4.5635e-08,\n",
      "           6.6683e-07,  2.3246e-06],\n",
      "         [ 2.4959e-07, -1.4305e-06,  4.9770e-06,  ...,  2.8871e-07,\n",
      "          -4.8801e-07,  3.5763e-06],\n",
      "         ...,\n",
      "         [-2.3842e-07,  1.2666e-07,  3.5949e-07,  ...,  1.6391e-06,\n",
      "          -4.1444e-08, -2.5630e-06],\n",
      "         [ 4.3539e-08,  3.2224e-07,  2.6822e-07,  ...,  1.2759e-07,\n",
      "          -6.4075e-07, -1.2219e-06],\n",
      "         [-3.5204e-07,  3.2783e-06,  3.3379e-06,  ...,  5.8860e-07,\n",
      "          -2.2445e-07,  3.7998e-06]],\n",
      "\n",
      "        [[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 1.0133e-06, -2.9616e-07,  5.0664e-07,  ..., -5.3346e-06,\n",
      "          -6.1095e-07,  1.8552e-06],\n",
      "         [-5.0962e-06,  9.0897e-07, -4.3027e-07,  ...,  4.3511e-06,\n",
      "           1.6764e-07,  4.7497e-08],\n",
      "         [ 5.8487e-07,  4.2096e-07,  1.6540e-06,  ...,  4.2319e-06,\n",
      "           1.1176e-06,  3.5949e-07],\n",
      "         ...,\n",
      "         [ 7.5306e-10,  3.9674e-07, -2.2817e-07,  ..., -8.4564e-07,\n",
      "           3.6322e-07,  1.2815e-06],\n",
      "         [ 1.9372e-06, -1.8277e-08, -1.2480e-07,  ..., -2.7008e-07,\n",
      "           5.8487e-07,  3.6135e-07],\n",
      "         [ 1.1683e-05,  1.5125e-06, -4.1127e-06,  ...,  5.9903e-06,\n",
      "           1.2070e-06,  9.4175e-06]],\n",
      "\n",
      "        [[-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -0.0000e+00],\n",
      "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(wmt.attention_masks[3]['W_Q'][-1].grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
