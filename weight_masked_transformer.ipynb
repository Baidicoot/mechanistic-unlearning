{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('monology/pile-uncopyrighted', split='train', streaming=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'google/gemma-2b',\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda',\n",
    "    default_padding_side=\"right\",\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    center_writing_weights=False,\n",
    "    # dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found, will not be able to run evaluations on Sports Trivia Task\n"
     ]
    }
   ],
   "source": [
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "from tasks.facts.SportsTaskAdversarial import adversarial_sports_eval\n",
    "from tasks.facts.SportsTaskSideEffects import run_side_effects_evals\n",
    "\n",
    "\n",
    "train_batch_size = 24\n",
    "eval_batch_size=64\n",
    "\n",
    "device = \"cuda\"\n",
    "train_loss_type = \"sports\"\n",
    "forget_sport = \"basketball\"\n",
    "maintain_sport = None\n",
    "# val_sport = \"baseball\"\n",
    "\n",
    "\n",
    "sports_1mp = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"log_1_minus_p\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "\n",
    "if maintain_sport is None:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "else:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "\n",
    "train_pile = PileTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "train_tasks = {\"sports_1mp\": (sports_1mp, .2), \"maintain_sports\": (maintain_sports, 1), \"pile\": (train_pile, 1)}\n",
    "\n",
    "# want to eval on other sports\n",
    "forget_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "test_pile = PileTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "\n",
    "induction_eval = InductionTask(batch_size=eval_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, device=device)\n",
    "if maintain_sport is None:\n",
    "    maintain_sports_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sports_eval}\n",
    "else:\n",
    "    maintain_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "    val_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={val_sport}, is_forget_dataset=True)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sport_eval, \"val_sport\": val_sport_eval}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_random_weight_mask_dicts(model):\n",
    "    # Creates random weight masks for testing\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        # Want bool of length n_head, randomly set to True\n",
    "        weight_mask_attn_dict[layer]['W_Q'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_K'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_V'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_O'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "\n",
    "        # Randomly set to true or false\n",
    "        weight_mask_mlp_dict[layer] = random.randint(0, 1) == 1\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Masking Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def make_partly_differentiable_mask(W, unfrozen_heads, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    W is Parameter of shape (n_heads, ...). Returns baseline and frozen (both only 1d arrays of (n_heads,)), and forward pass should be W_baseline.float() + W_frozen.float() * W \n",
    "    \"\"\"\n",
    "    W_baseline = torch.nn.Parameter(torch.zeros(W.shape[0], dtype=torch.bool), requires_grad=False).to(device)\n",
    "\n",
    "    # unsqueeze to broadcast efficiently, until W_baseline has same shape as W\n",
    "    while len(W_baseline.shape) < len(W.shape):\n",
    "        W_baseline = W_baseline.unsqueeze(-1)\n",
    "    \n",
    "    W_baseline[unfrozen_heads] = True\n",
    "    # W_baseline = ~W_frozen\n",
    "    W_frozen = torch.nn.Parameter(~W_baseline, requires_grad=False)\n",
    "    # convert into float\n",
    "    return W_frozen.float(), W_baseline.float()\n",
    "\n",
    "class WeightMaskedTransformer(nn.Module):\n",
    "    def __init__(self, tl_transformer, weight_mask_attn_dict=None, weight_mask_mlp_dict=None, torch_dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        weight_mask_attn_dict: {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}} (frozen_heads is shape (n_heads,) of bools). If none, train mask over all heads\n",
    "        weight_mask_mlp_dict: {layer: bool}. If none, train mask over all mlps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.torch_dtype = torch_dtype\n",
    "        # tl_transformer should be a HookedTransformer\n",
    "        self.tl_transformer = tl_transformer\n",
    "        # turn off gradients for tl_transformer\n",
    "        # for param in self.tl_transformer.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.weight_mask_attn_dict = weight_mask_attn_dict\n",
    "        self.weight_mask_mlp_dict = weight_mask_mlp_dict\n",
    "        # store weight masks for every component that is unfrozen\n",
    "        \n",
    "        # need to store reference weights so that you can reset W_Q, etc after a forward pass\n",
    "        self.reference_attn_weights = {}\n",
    "        self.reference_mlp_weights = {}\n",
    "\n",
    "        self.attention_masks = {}\n",
    "        self.mlp_masks = {}\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            self.attention_masks[layer] = {}\n",
    "            self.reference_attn_weights[layer] = {}\n",
    "            self.mlp_masks[layer] = {}\n",
    "            self.reference_mlp_weights[layer] = {}\n",
    "            # Attention heads\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None:\n",
    "                    unfrozen_heads = list(range(self.tl_transformer.cfg.n_heads)) # all heads are unfrozen\n",
    "                else:\n",
    "                    unfrozen_heads = self.weight_mask_attn_dict[layer][component]\n",
    "                # make frozen and baseline masks, and also a copy of the original weights\n",
    "\n",
    "                if unfrozen_heads is not None and len(unfrozen_heads) > 0:\n",
    "                    W_frozen, W_baseline = make_partly_differentiable_mask(parameter, unfrozen_heads)\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "                    \n",
    "                    self.attention_masks[layer][component] = (W_frozen, W_baseline, weight_mask)\n",
    "                    self.reference_attn_weights[layer][component] = parameter.clone()\n",
    "\n",
    "            # MLPs\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "\n",
    "                    self.mlp_masks[layer][component] = weight_mask\n",
    "                    self.reference_mlp_weights[layer][component] = parameter.clone()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    reference_data = self.reference_attn_weights[layer][component]\n",
    "                    mask = W_baseline + W_frozen * weight_mask\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters'][component] = reference_data * mask\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    reference_data = self.reference_mlp_weights[layer][component]\n",
    "                    self.tl_transformer.blocks[layer].mlp.__dict__['_parameters'][component] = reference_data * weight_mask\n",
    "\n",
    "        return self.tl_transformer(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Weight Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_mask_from_ap_graph(model, ap_graph, threshold):\n",
    "    # Attention masks are of form:\n",
    "    # {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}}\n",
    "    # MLP masks are of form:\n",
    "    # {layer: bool}\n",
    "\n",
    "    # Localizations are of form:\n",
    "    # {alayer.head_{q,k,v,result}:int, mlayer_{in,out}: int}\n",
    "\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        weight_mask_mlp_dict[layer] = {}\n",
    "\n",
    "        if 'a0.0_q' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_Q'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_q\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_Q'] = None\n",
    "\n",
    "        if 'a0.0_k' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_K'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_k\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_K'] = None\n",
    "        \n",
    "        if 'a0.0_v' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_V'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_v\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_V'] = None\n",
    "        \n",
    "        if 'a0.0_result' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_O'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_result\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_O'] = None\n",
    "            \n",
    "        if 'm0_in' in ap_graph:\n",
    "            weight_mask_mlp_dict[layer]['W_in'] = abs(ap_graph[f\"m{layer}_in\"]) > threshold\n",
    "        else:\n",
    "            weight_mask_mlp_dict[layer]['W_in'] = None\n",
    "        \n",
    "        if 'm0_out' in ap_graph:\n",
    "            weight_mask_mlp_dict[layer]['W_out'] = abs(ap_graph[f\"m{layer}_out\"]) > threshold\n",
    "        else:\n",
    "            weight_mask_mlp_dict[layer]['W_out'] = None\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models/google_gemma-2b_sports_baseball_ap_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)\n",
    "\n",
    "weight_mask_attn_dict, weight_mask_mlp_dict = get_mask_from_ap_graph(model, ap_graph, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[ 1.0803e-07,  2.6892e-08, -2.6193e-08,  ..., -6.7055e-07,\n",
      "         -5.6811e-08,  2.2259e-07],\n",
      "        [ 1.9278e-07,  5.4762e-07, -5.4482e-08,  ..., -8.0466e-07,\n",
      "          1.0384e-07, -5.3346e-06],\n",
      "        [-7.6368e-08,  2.5029e-08, -1.0338e-07,  ..., -1.0105e-07,\n",
      "          4.4145e-07, -5.3318e-08],\n",
      "        ...,\n",
      "        [-1.1828e-07,  1.5087e-07,  5.5879e-08,  ..., -8.4564e-07,\n",
      "         -7.7998e-09, -3.6787e-08],\n",
      "        [-1.9930e-07,  1.8099e-10,  4.4703e-08,  ...,  8.7544e-07,\n",
      "         -7.2177e-08, -5.5181e-08],\n",
      "        [-5.7509e-08, -2.6822e-07,  7.4040e-08,  ..., -1.3039e-06,\n",
      "          1.6093e-06,  8.8811e-06]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "mask = WeightMaskedTransformer(\n",
    "    model, \n",
    "    weight_mask_attn_dict=weight_mask_attn_dict, \n",
    "    weight_mask_mlp_dict=weight_mask_mlp_dict\n",
    ")\n",
    "sports_train = SportsTask(batch_size=8, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(mask, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "print(mask.attention_masks[3]['W_Q'][-1].grad[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Compression type zstd not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/task.py:42\u001b[0m, in \u001b[0;36mTask.get_batch\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_iter)\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:39\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_last \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(possibly_batched_index)\n\u001b[1;32m     38\u001b[0m     ):\n\u001b[0;32m---> 39\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m task_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(grad_accum_steps):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     loss \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39;49mget_train_loss(model) \u001b[39m/\u001b[39m grad_accum_steps\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     task_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     loss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m task_weight\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/task.py:60\u001b[0m, in \u001b[0;36mTask.get_train_loss\u001b[0;34m(self, model, n_iters)\u001b[0m\n\u001b[1;32m     58\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iters):\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_batch(train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     61\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(model, batch)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m total_loss \u001b[39m/\u001b[39m n_iters\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/task.py:45\u001b[0m, in \u001b[0;36mTask.get_batch\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader)\n\u001b[0;32m---> 45\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_iter)\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/iterable_dataset.py:1388\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[39myield\u001b[39;00m formatter\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1386\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m \u001b[39mfor\u001b[39;00m key, example \u001b[39min\u001b[39;00m ex_iterable:\n\u001b[1;32m   1389\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures:\n\u001b[1;32m   1390\u001b[0m         \u001b[39m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m         \u001b[39m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m         example \u001b[39m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   1393\u001b[0m             example, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, token_per_repo_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   1394\u001b[0m         )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/iterable_dataset.py:987\u001b[0m, in \u001b[0;36mBufferShuffledExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39m# this is the shuffle buffer that we keep in memory\u001b[39;00m\n\u001b[1;32m    986\u001b[0m mem_buffer \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 987\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mex_iterable:\n\u001b[1;32m    988\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mem_buffer) \u001b[39m==\u001b[39m buffer_size:  \u001b[39m# if the buffer is full, pick and example from it\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         i \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(indices_iterator)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/iterable_dataset.py:321\u001b[0m, in \u001b[0;36mShuffledDataSourcesArrowExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m kwargs_with_shuffled_shards \u001b[39m=\u001b[39m _shuffle_gen_kwargs(rng, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m formatter \u001b[39m=\u001b[39m PythonFormatter()\n\u001b[0;32m--> 321\u001b[0m \u001b[39mfor\u001b[39;00m key, pa_table \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_tables_fn(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_with_shuffled_shards):\n\u001b[1;32m    322\u001b[0m     \u001b[39mfor\u001b[39;00m pa_subtable \u001b[39min\u001b[39;00m pa_table\u001b[39m.\u001b[39mto_reader(max_chunksize\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mARROW_READER_BATCH_SIZE_IN_DATASET_ITER):\n\u001b[1;32m    323\u001b[0m         formatted_batch \u001b[39m=\u001b[39m formatter\u001b[39m.\u001b[39mformat_batch(pa_subtable)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py:78\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_tables\u001b[39m(\u001b[39mself\u001b[39m, files):\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mfor\u001b[39;00m file_idx, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(files)):\n\u001b[1;32m     79\u001b[0m         \u001b[39m# If the file is one json object and if we need to look at the list of items in one specific field\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mfield \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file, encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mencoding, errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mencoding_errors) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:869\u001b[0m, in \u001b[0;36m_IterableFromGenerator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 869\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:947\u001b[0m, in \u001b[0;36mFilesIterable._iter_from_urlpaths\u001b[0;34m(cls, urlpaths, download_config)\u001b[0m\n\u001b[1;32m    945\u001b[0m     urlpaths \u001b[39m=\u001b[39m [urlpaths]\n\u001b[1;32m    946\u001b[0m \u001b[39mfor\u001b[39;00m urlpath \u001b[39min\u001b[39;00m urlpaths:\n\u001b[0;32m--> 947\u001b[0m     \u001b[39mif\u001b[39;00m xisfile(urlpath, download_config\u001b[39m=\u001b[39;49mdownload_config):\n\u001b[1;32m    948\u001b[0m         \u001b[39myield\u001b[39;00m urlpath\n\u001b[1;32m    949\u001b[0m     \u001b[39melif\u001b[39;00m xisdir(urlpath, download_config\u001b[39m=\u001b[39mdownload_config):\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:262\u001b[0m, in \u001b[0;36mxisfile\u001b[0;34m(path, download_config)\u001b[0m\n\u001b[1;32m    260\u001b[0m path, storage_options \u001b[39m=\u001b[39m _prepare_path_and_storage_options(path, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    261\u001b[0m main_hop, \u001b[39m*\u001b[39mrest_hops \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m::\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 262\u001b[0m fs, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m fsspec\u001b[39m.\u001b[39;49mget_fs_token_paths(path, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m fs\u001b[39m.\u001b[39misfile(main_hop)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/core.py:635\u001b[0m, in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    633\u001b[0m     inkwargs[\u001b[39m\"\u001b[39m\u001b[39mfo\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m urls\n\u001b[1;32m    634\u001b[0m paths, protocol, _ \u001b[39m=\u001b[39m chain[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 635\u001b[0m fs \u001b[39m=\u001b[39m filesystem(protocol, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minkwargs)\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(urlpath, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m)):\n\u001b[1;32m    637\u001b[0m     pchains \u001b[39m=\u001b[39m [\n\u001b[1;32m    638\u001b[0m         _un_chain(stringify_path(u), storage_options \u001b[39mor\u001b[39;00m {})[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m urlpath\n\u001b[1;32m    639\u001b[0m     ]\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/registry.py:291\u001b[0m, in \u001b[0;36mfilesystem\u001b[0;34m(protocol, **storage_options)\u001b[0m\n\u001b[1;32m    284\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39marrow_hdfs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m protocol has been deprecated and will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mremoved in the future. Specify it as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhdfs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    287\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m get_filesystem_class(protocol)\n\u001b[0;32m--> 291\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/spec.py:80\u001b[0m, in \u001b[0;36m_Cached.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache[token]\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     81\u001b[0m     \u001b[39m# Setting _fs_token here causes some static linters to complain.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     obj\u001b[39m.\u001b[39m_fs_token_ \u001b[39m=\u001b[39m token\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/filesystems/compression.py:138\u001b[0m, in \u001b[0;36mZstdFileSystem.__init__\u001b[0;34m(self, fo, mode, target_protocol, target_options, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     fo: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    139\u001b[0m         fo\u001b[39m=\u001b[39;49mfo,\n\u001b[1;32m    140\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    141\u001b[0m         target_protocol\u001b[39m=\u001b[39;49mtarget_protocol,\n\u001b[1;32m    142\u001b[0m         target_options\u001b[39m=\u001b[39;49mtarget_options,\n\u001b[1;32m    143\u001b[0m         block_size\u001b[39m=\u001b[39;49mblock_size,\n\u001b[1;32m    144\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     \u001b[39m# We need to wrap the zstd decompressor to avoid this error in fsspec==2021.7.0 and zstandard==0.15.2:\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[39m# File \"/Users/user/.virtualenvs/hf-datasets/lib/python3.7/site-packages/fsspec/core.py\", line 145, in open\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[39m# see https://github.com/intake/filesystem_spec/issues/725\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     _enter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/datasets/filesystems/compression.py:37\u001b[0m, in \u001b[0;36mBaseCompressedFileFileSystem.__init__\u001b[0;34m(self, fo, target_protocol, target_options, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m \u001b[39m# always open as \"rb\" since fsspec can then use the TextIOWrapper to make it work for \"r\" mode\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile \u001b[39m=\u001b[39m fsspec\u001b[39m.\u001b[39;49mopen(\n\u001b[1;32m     38\u001b[0m     fo,\n\u001b[1;32m     39\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m     protocol\u001b[39m=\u001b[39;49mtarget_protocol,\n\u001b[1;32m     41\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m     42\u001b[0m     client_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     43\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mrequote_redirect_url\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# see https://github.com/huggingface/datasets/pull/5459\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrust_env\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Enable reading proxy env variables.\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(target_options \u001b[39mor\u001b[39;49;00m {})\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mclient_kwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m, {}),  \u001b[39m# To avoid issues if it was already passed.\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m     },\n\u001b[1;32m     47\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(target_options \u001b[39mor\u001b[39;49;00m {}),\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m::\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muncompressed_name \u001b[39m=\u001b[39m (\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_name[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_name\u001b[39m.\u001b[39mrindex(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_name\n\u001b[1;32m     53\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_name\n\u001b[1;32m     54\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/core.py:459\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(urlpath, mode, compression, encoding, errors, protocol, newline, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    400\u001b[0m     urlpath,\n\u001b[1;32m    401\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    408\u001b[0m ):\n\u001b[1;32m    409\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a path or paths, return one ``OpenFile`` object.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m      https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     out \u001b[39m=\u001b[39m open_files(\n\u001b[1;32m    460\u001b[0m         urlpath\u001b[39m=\u001b[39;49m[urlpath],\n\u001b[1;32m    461\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    462\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    463\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    464\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    465\u001b[0m         protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m    466\u001b[0m         newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    467\u001b[0m         expand\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    468\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out:\n\u001b[1;32m    471\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(urlpath)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/core.py:302\u001b[0m, in \u001b[0;36mopen_files\u001b[0;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mPermissionError\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m OpenFiles(\n\u001b[0;32m--> 302\u001b[0m     [\n\u001b[1;32m    303\u001b[0m         OpenFile(\n\u001b[1;32m    304\u001b[0m             fs,\n\u001b[1;32m    305\u001b[0m             path,\n\u001b[1;32m    306\u001b[0m             mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    307\u001b[0m             compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m    308\u001b[0m             encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    309\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    310\u001b[0m             newline\u001b[39m=\u001b[39mnewline,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m         \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths\n\u001b[1;32m    313\u001b[0m     ],\n\u001b[1;32m    314\u001b[0m     mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    315\u001b[0m     fs\u001b[39m=\u001b[39mfs,\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/core.py:303\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mPermissionError\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m OpenFiles(\n\u001b[1;32m    302\u001b[0m     [\n\u001b[0;32m--> 303\u001b[0m         OpenFile(\n\u001b[1;32m    304\u001b[0m             fs,\n\u001b[1;32m    305\u001b[0m             path,\n\u001b[1;32m    306\u001b[0m             mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    307\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    308\u001b[0m             encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    309\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    310\u001b[0m             newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m         \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths\n\u001b[1;32m    313\u001b[0m     ],\n\u001b[1;32m    314\u001b[0m     mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    315\u001b[0m     fs\u001b[39m=\u001b[39mfs,\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/core.py:77\u001b[0m, in \u001b[0;36mOpenFile.__init__\u001b[0;34m(self, fs, path, mode, compression, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath \u001b[39m=\u001b[39m path\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n\u001b[0;32m---> 77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding \u001b[39m=\u001b[39m encoding\n\u001b[1;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors \u001b[39m=\u001b[39m errors\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/fsspec/core.py:512\u001b[0m, in \u001b[0;36mget_compression\u001b[0;34m(urlpath, compression)\u001b[0m\n\u001b[1;32m    510\u001b[0m     compression \u001b[39m=\u001b[39m infer_compression(urlpath)\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m compression \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m compr:\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompression type \u001b[39m\u001b[39m{\u001b[39;00mcompression\u001b[39m}\u001b[39;00m\u001b[39m not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m compression\n",
      "\u001b[0;31mValueError\u001b[0m: Compression type zstd not supported"
     ]
    }
   ],
   "source": [
    "mask = WeightMaskedTransformer(\n",
    "    model, \n",
    "    weight_mask_attn_dict=weight_mask_attn_dict, \n",
    "    weight_mask_mlp_dict=weight_mask_mlp_dict\n",
    ")\n",
    "# for n, param in mask.tl_transformer.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model_type = 'gemma'\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "grad_accum_steps = 5\n",
    "# max_gpu_batch_size=8\n",
    "alpha = 0.2\n",
    "beta = 1\n",
    "clip_grad = 1\n",
    "\n",
    "evaluate_every = 5\n",
    "n_eval_iters = 5\n",
    "do_adversarial_evals = True\n",
    "do_side_effects_evals = True\n",
    "\n",
    "from collections import defaultdict\n",
    "all_train_losses = defaultdict(list)\n",
    "all_test_losses = defaultdict(list)\n",
    "adversarial_evals = []\n",
    "side_effect_evals = []\n",
    "\n",
    "# Initialize optimizer\n",
    "mask = mask.cuda()\n",
    "mask_params = [\n",
    "    v[-1]\n",
    "    for layer, layer_mask_weights in mask.attention_masks.items()\n",
    "    for k, v in layer_mask_weights.items()\n",
    "] + \\\n",
    "[\n",
    "    v\n",
    "    for layer, layer_mask_weights in mask.mlp_masks.items()\n",
    "    for k, v in layer_mask_weights.items()\n",
    "]\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs)\n",
    "# Cycle dataloaders\n",
    "# Train a sparse mask\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    # Sample batches\n",
    "    # Reset grad\n",
    "    optimizer.zero_grad()\n",
    "    # Compute normal loss over retain\n",
    "    for task_name, (task, task_weight) in train_tasks.items():\n",
    "        task_loss = 0\n",
    "        for i in range(grad_accum_steps):\n",
    "            loss = task.get_train_loss(model) / grad_accum_steps\n",
    "            task_loss += loss.item()\n",
    "            loss *= task_weight\n",
    "            loss.backward()\n",
    "        all_train_losses[task_name].append(task_loss)\n",
    "        \n",
    "    # Add sparsity loss and backprop\n",
    "    loss = beta * mask.regularization_loss()\n",
    "    loss.backward()\n",
    "    all_train_losses[\"reg\"].append(loss.item())\n",
    "    # Step and log\n",
    "    if clip_grad is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(mask_params, clip_grad)\n",
    "    # zero_nan_grads(mask)\n",
    "    optimizer.step()\n",
    "    mask.on_step_end()\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % evaluate_every == 0 or epoch == n_epochs - 1:\n",
    "        for task_name, task in eval_tasks.items():\n",
    "            task_loss = 0\n",
    "            for i in range(n_eval_iters):\n",
    "                task_loss += task.get_test_loss(model).item()\n",
    "            all_test_losses[task_name].append(task_loss / n_eval_iters)\n",
    "        if do_adversarial_evals:\n",
    "            print(\"Running adversarial evals\")\n",
    "            adversarial_evals.append(adversarial_sports_eval(model, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True))\n",
    "        if do_side_effects_evals:\n",
    "            print(\"Running side effects evals\")\n",
    "            side_effect_evals.append(run_side_effects_evals(model, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evals\n",
    "final_adversarial_eval = adversarial_sports_eval(model, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True)\n",
    "print(f\"System Prompt: adversarial evals are {final_adversarial_eval}\")\n",
    "final_adversarial_eval = adversarial_sports_eval(model, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=False)\n",
    "print(f\"No System Prompt: adversarial evals are {final_adversarial_eval}\")\n",
    "\n",
    "final_side_effects = run_side_effects_evals(model, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\", \"Sports Familiarity\", \"Cross Entropy\"], verbose=True)\n",
    "print(final_side_effects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save masks state dict to neuron_cb\n",
    "torch.save(mask.state_dict(), \"masks/neuron_cb/mlps_unlearn_basketball.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
