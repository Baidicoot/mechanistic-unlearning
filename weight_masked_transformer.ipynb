{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('monology/pile-uncopyrighted', split='train', streaming=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = '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'\n",
    "model_name = 'google/gemma-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda',\n",
    "    default_padding_side=\"right\",\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    center_writing_weights=False,\n",
    "    dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found, will not be able to run evaluations on Sports Trivia Task\n"
     ]
    }
   ],
   "source": [
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "from tasks.facts.SportsTaskAdversarial import adversarial_sports_eval\n",
    "from tasks.facts.SportsTaskSideEffects import run_side_effects_evals\n",
    "\n",
    "\n",
    "train_batch_size = 10\n",
    "eval_batch_size = 50\n",
    "\n",
    "device = \"cuda\"\n",
    "train_loss_type = \"sports\"\n",
    "forget_sport = \"basketball\"\n",
    "maintain_sport = None\n",
    "# val_sport = \"baseball\"\n",
    "\n",
    "\n",
    "sports_1mp = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"log_1_minus_p\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "\n",
    "if maintain_sport is None:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "else:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "\n",
    "train_pile = PileTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "train_tasks = {\"sports_1mp\": (sports_1mp, .2), \"maintain_sports\": (maintain_sports, 1), \"pile\": (train_pile, 1)}\n",
    "\n",
    "# want to eval on other sports\n",
    "forget_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "test_pile = PileTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "\n",
    "induction_eval = InductionTask(batch_size=eval_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, device=device)\n",
    "if maintain_sport is None:\n",
    "    maintain_sports_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sports_eval}\n",
    "else:\n",
    "    maintain_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "    val_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={val_sport}, is_forget_dataset=True)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sport_eval, \"val_sport\": val_sport_eval}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "def create_random_weight_mask_dicts(model):\n",
    "    # Creates random weight masks for testing\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        # Want bool of length n_head, randomly set to True\n",
    "        weight_mask_attn_dict[layer]['W_Q'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_K'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_V'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_O'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "\n",
    "        # Randomly set to true or false\n",
    "        weight_mask_mlp_dict[layer] = random.randint(0, 1) == 1\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Masking Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def make_partly_differentiable_mask(W, unfrozen_heads, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    W is Parameter of shape (n_heads, ...). Returns baseline and frozen (both only 1d arrays of (n_heads,)), and forward pass should be W_baseline.float() + W_frozen.float() * W \n",
    "    \"\"\"\n",
    "    W_baseline = torch.nn.Parameter(torch.zeros(W.shape[0], dtype=torch.bool), requires_grad=False).to(device)\n",
    "\n",
    "    # unsqueeze to broadcast efficiently, until W_baseline has same shape as W\n",
    "    while len(W_baseline.shape) < len(W.shape):\n",
    "        W_baseline = W_baseline.unsqueeze(-1)\n",
    "    \n",
    "    W_baseline[unfrozen_heads] = True\n",
    "    # W_baseline = ~W_frozen\n",
    "    W_frozen = torch.nn.Parameter(~W_baseline, requires_grad=False)\n",
    "    # convert into float\n",
    "    return W_frozen.float(), W_baseline.float()\n",
    "\n",
    "class WeightMaskedTransformer(nn.Module):\n",
    "    def __init__(self, tl_transformer, weight_mask_attn_dict=None, weight_mask_mlp_dict=None, torch_dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        weight_mask_attn_dict: {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}} (frozen_heads is shape (n_heads,) of bools). If none, train mask over all heads\n",
    "        weight_mask_mlp_dict: {layer: bool}. If none, train mask over all mlps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.torch_dtype = torch_dtype\n",
    "        # tl_transformer should be a HookedTransformer\n",
    "        self.tl_transformer = tl_transformer\n",
    "        # turn off gradients for tl_transformer\n",
    "        # for param in self.tl_transformer.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.weight_mask_attn_dict = weight_mask_attn_dict\n",
    "        self.weight_mask_mlp_dict = weight_mask_mlp_dict\n",
    "        # store weight masks for every component that is unfrozen\n",
    "        \n",
    "        # need to store reference weights so that you can reset W_Q, etc after a forward pass\n",
    "        self.reference_attn_weights = {}\n",
    "        self.reference_mlp_weights = {}\n",
    "\n",
    "        self.attention_masks = {}\n",
    "        self.mlp_masks = {}\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            self.attention_masks[layer] = {}\n",
    "            self.reference_attn_weights[layer] = {}\n",
    "            self.mlp_masks[layer] = {}\n",
    "            self.reference_mlp_weights[layer] = {}\n",
    "            # Attention heads\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None:\n",
    "                    unfrozen_heads = list(range(self.tl_transformer.cfg.n_heads)) # all heads are unfrozen\n",
    "                else:\n",
    "                    unfrozen_heads = self.weight_mask_attn_dict[layer][component]\n",
    "                # make frozen and baseline masks, and also a copy of the original weights\n",
    "\n",
    "                if unfrozen_heads is not None and len(unfrozen_heads) > 0:\n",
    "                    W_frozen, W_baseline = make_partly_differentiable_mask(parameter, unfrozen_heads)\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "                    \n",
    "                    self.attention_masks[layer][component] = (W_frozen, W_baseline, weight_mask)\n",
    "                    self.reference_attn_weights[layer][component] = parameter.clone()\n",
    "\n",
    "            # MLPs\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "\n",
    "                    self.mlp_masks[layer][component] = weight_mask\n",
    "                    self.reference_mlp_weights[layer][component] = parameter.clone()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    reference_data = self.reference_attn_weights[layer][component]\n",
    "                    mask = W_baseline + W_frozen * weight_mask\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters'][component] = reference_data * mask\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    reference_data = self.reference_mlp_weights[layer][component]\n",
    "                    self.tl_transformer.blocks[layer].mlp.__dict__['_parameters'][component] = reference_data * weight_mask\n",
    "\n",
    "        return self.tl_transformer(*args, **kwargs)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return getattr(self.tl_transformer, name)\n",
    "        except:\n",
    "            raise AttributeError(f\"'CustomTransformerWrapper' object has no attribute '{name}'\")\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        # Compute the L1 sparsity penalty using the masks\n",
    "        loss = 0\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    loss += torch.sum(torch.abs(weight_mask - 1)) # We subtract 1 because we want to encourage the mask to be 1\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    loss += torch.sum(torch.abs(weight_mask - 1))\n",
    "        return loss\n",
    "    \n",
    "    def on_step_end(self):\n",
    "        # Clip all the masks\n",
    "\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    weight_mask.data = torch.clamp(weight_mask.data, 0, 1)\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    weight_mask.data = torch.clamp(weight_mask.data, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Weight Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_mask_from_ap_graph(model, ap_graph, threshold):\n",
    "    # Attention masks are of form:\n",
    "    # {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}}\n",
    "    # MLP masks are of form:\n",
    "    # {layer: bool}\n",
    "\n",
    "    # Localizations are of form:\n",
    "    # {alayer.head_{q,k,v,result}:int, mlayer_{in,out}: int}\n",
    "\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        weight_mask_mlp_dict[layer] = {}\n",
    "\n",
    "        if 'a0.0_q' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_Q'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_q\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_Q'] = None\n",
    "\n",
    "        if 'a0.0_k' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_K'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_k\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_K'] = None\n",
    "        \n",
    "        if 'a0.0_v' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_V'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_v\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_V'] = None\n",
    "        \n",
    "        if 'a0.0_result' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_O'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_result\"]) > threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_O'] = None\n",
    "            \n",
    "        if 'm0_in' in ap_graph:\n",
    "            weight_mask_mlp_dict[layer]['W_in'] = abs(ap_graph[f\"m{layer}_in\"]) > threshold\n",
    "        else:\n",
    "            weight_mask_mlp_dict[layer]['W_in'] = None\n",
    "        \n",
    "        if 'm0_out' in ap_graph:\n",
    "            weight_mask_mlp_dict[layer]['W_out'] = abs(ap_graph[f\"m{layer}_out\"]) > threshold\n",
    "        else:\n",
    "            weight_mask_mlp_dict[layer]['W_out'] = None\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models/google_gemma-2b_sports_baseball_ap_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)\n",
    "\n",
    "weight_mask_attn_dict, weight_mask_mlp_dict = get_mask_from_ap_graph(model, ap_graph, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = WeightMaskedTransformer(\n",
    "    model, \n",
    "    weight_mask_attn_dict=weight_mask_attn_dict, \n",
    "    weight_mask_mlp_dict=weight_mask_mlp_dict\n",
    ")\n",
    "for n, param in mask.tl_transformer.named_parameters():\n",
    "    param.requires_grad = False\n",
    "sports_train = SportsTask(batch_size=8, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(model, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "print(mask.attention_masks[3]['W_Q'][-1].grad[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maaquib111\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/mechanistic-unlearning/wandb/run-20240507_040705-xv4razm5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaquib111/mech-unlearning/runs/xv4razm5' target=\"_blank\">gemma-2b-basketball</a></strong> to <a href='https://wandb.ai/aaquib111/mech-unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aaquib111/mech-unlearning' target=\"_blank\">https://wandb.ai/aaquib111/mech-unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aaquib111/mech-unlearning/runs/xv4razm5' target=\"_blank\">https://wandb.ai/aaquib111/mech-unlearning/runs/xv4razm5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adversarial evals\n",
      "Running side effects evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [01:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WeightMaskedTransformer' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:420\u001b[0m, in \u001b[0;36mtest_prompt_accuracy\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     generation \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(tokenized, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, do_sample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    421\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WeightMaskedTransformer' object has no attribute 'generate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m     \u001b[39mif\u001b[39;00m do_side_effects_evals:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning side effects evals\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m         side_effect_evals\u001b[39m.\u001b[39mappend(run_side_effects_evals(mask, model_type\u001b[39m=\u001b[39;49mmodel_type, batch_size\u001b[39m=\u001b[39;49meval_batch_size, evals_to_run\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mSports Answers\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m log_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m all_train_losses\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:473\u001b[0m, in \u001b[0;36mrun_side_effects_evals\u001b[0;34m(model, evals_to_run, model_type, use_short, eval_model, batch_size, verbose)\u001b[0m\n\u001b[1;32m    471\u001b[0m return_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSports Answers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m evals_to_run:\n\u001b[0;32m--> 473\u001b[0m     sports_accuracies, tot_questions \u001b[39m=\u001b[39m respond_sports_side_effects(model, tokenizer)\n\u001b[1;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    475\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSports Answers:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:448\u001b[0m, in \u001b[0;36mrespond_sports_side_effects\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m    441\u001b[0m tot_questions[sport_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[39m# ref_response = test_prompt_accuracy(reference_model, formatted_question)\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m# unlearned_response = test_prompt_accuracy(unlearned_model, formatted_question)\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# if sport_name in ref_response.lower():\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m#     reference_accuracies[sport_name] += 1\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# if sport_name in unlearned_response.lower():\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m response \u001b[39m=\u001b[39m test_prompt_accuracy(model, tokenizer, formatted_question)\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m sport_name \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mlower():\n\u001b[1;32m    450\u001b[0m     accuracies[sport_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:422\u001b[0m, in \u001b[0;36mtest_prompt_accuracy\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m    420\u001b[0m         generation \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(tokenized, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, do_sample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    421\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m         generation \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(tokenized, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, do_sample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    423\u001b[0m     decoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(generation[\u001b[39m0\u001b[39m, tokens_len:])\n\u001b[1;32m    424\u001b[0m \u001b[39mreturn\u001b[39;00m decoded\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WeightMaskedTransformer' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import wandb\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mask = WeightMaskedTransformer(\n",
    "    model, \n",
    "    weight_mask_attn_dict=weight_mask_attn_dict, \n",
    "    weight_mask_mlp_dict=weight_mask_mlp_dict\n",
    ")\n",
    "for param in mask.tl_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_type = 'gemma'\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "grad_accum_steps = 5\n",
    "# max_gpu_batch_size=8\n",
    "alpha = 0.2\n",
    "beta = 1\n",
    "clip_grad = 1\n",
    "\n",
    "evaluate_every = 5\n",
    "n_eval_iters = 5\n",
    "do_adversarial_evals = True\n",
    "do_side_effects_evals = True\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"mech-unlearning\",\n",
    "    name=f\"{model_name.split('/')[-1]}-{forget_sport}\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model_type\": model_type,\n",
    "        \"model_name\": model_name,\n",
    "        \"forget_sport\": forget_sport,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"grad_accum_steps\": grad_accum_steps,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta,\n",
    "        \"clip_grad\": clip_grad,\n",
    "        \"evaluate_every\": evaluate_every,\n",
    "        \"n_eval_iters\": n_eval_iters,\n",
    "        \"do_adversarial_evals\": do_adversarial_evals,\n",
    "        \"do_side_effects_evals\": do_side_effects_evals,\n",
    "    }\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "all_train_losses = defaultdict(list)\n",
    "all_test_losses = defaultdict(list)\n",
    "adversarial_evals = []\n",
    "side_effect_evals = []\n",
    "\n",
    "# Initialize optimizer\n",
    "mask = mask.cuda()\n",
    "mask_params = [\n",
    "    v[-1]\n",
    "    for layer, layer_mask_weights in mask.attention_masks.items()\n",
    "    for k, v in layer_mask_weights.items()\n",
    "] + \\\n",
    "[\n",
    "    v\n",
    "    for layer, layer_mask_weights in mask.mlp_masks.items()\n",
    "    for k, v in layer_mask_weights.items()\n",
    "]\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs)\n",
    "# Cycle dataloaders\n",
    "# Train a sparse mask\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    # Sample batches\n",
    "    # Reset grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        # Compute normal loss over retain\n",
    "        for task_name, (task, task_weight) in train_tasks.items():\n",
    "            task_loss = 0\n",
    "            # print(task_name)\n",
    "            for i in range(grad_accum_steps):\n",
    "                loss = task.get_train_loss(mask) / grad_accum_steps\n",
    "                task_loss += loss.item()\n",
    "                loss *= task_weight\n",
    "                loss.backward()\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            all_train_losses[task_name].append(task_loss)\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Add sparsity loss and backprop\n",
    "        loss = beta * mask.regularization_loss()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        all_train_losses[\"reg\"].append(loss.item())\n",
    "        # Step and log\n",
    "        if clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(mask_params, clip_grad)\n",
    "        # zero_nan_grads(mask)\n",
    "        optimizer.step()\n",
    "        mask.on_step_end()\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % evaluate_every == 0 or epoch == n_epochs - 1:\n",
    "            for task_name, task in eval_tasks.items():\n",
    "                task_loss = 0\n",
    "                for i in range(n_eval_iters):\n",
    "                    task_loss += task.get_test_loss(mask).item()\n",
    "                all_test_losses[task_name].append(task_loss / n_eval_iters)\n",
    "            if do_adversarial_evals:\n",
    "                print(\"Running adversarial evals\")\n",
    "                adversarial_evals.append(adversarial_sports_eval(mask, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True))\n",
    "            if do_side_effects_evals:\n",
    "                print(\"Running side effects evals\")\n",
    "                side_effect_evals.append(run_side_effects_evals(mask, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\"]))\n",
    "        \n",
    "        log_dict = {}\n",
    "        for k, v in all_train_losses.items():\n",
    "            log_dict[f\"train_loss_{k}\"] = v[-1]\n",
    "        for k, v in all_test_losses.items():\n",
    "            log_dict[f\"test_loss_{k}\"] = v[-1]\n",
    "        for k, v in adversarial_evals[-1].items():\n",
    "            log_dict[f\"adversarial_{k}\"] = v\n",
    "        for k, v in side_effect_evals[-1].items():\n",
    "            log_dict[f\"side_effects_{k}\"] = v\n",
    "        \n",
    "        print((mask.mlp_masks[10]['W_out'] - 1).sum())\n",
    "        wandb.log(log_dict)\n",
    "    \n",
    "wandb.finish()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Final evals\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m final_adversarial_eval \u001b[39m=\u001b[39m adversarial_sports_eval(model, model_type\u001b[39m=\u001b[39;49mmodel_type, batch_size\u001b[39m=\u001b[39;49meval_batch_size, use_system_prompt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSystem Prompt: adversarial evals are \u001b[39m\u001b[39m{\u001b[39;00mfinal_adversarial_eval\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m final_adversarial_eval \u001b[39m=\u001b[39m adversarial_sports_eval(model, model_type\u001b[39m=\u001b[39mmodel_type, batch_size\u001b[39m=\u001b[39meval_batch_size, use_system_prompt\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskAdversarial.py:400\u001b[0m, in \u001b[0;36madversarial_sports_eval\u001b[0;34m(model, model_type, batch_size, n_iters, continuous, test_each_sport, include_evals, use_icl, use_system_prompt)\u001b[0m\n\u001b[1;32m    398\u001b[0m     update_accuracies(\u001b[39m\"\u001b[39m\u001b[39mCapitalized\u001b[39m\u001b[39m\"\u001b[39m, SportsTask_Capitalized)\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDashed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m include_evals:\n\u001b[0;32m--> 400\u001b[0m     update_accuracies(\u001b[39m\"\u001b[39;49m\u001b[39mDashed\u001b[39;49m\u001b[39m\"\u001b[39;49m, SportsTask_Dashed)\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m accuracies\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskAdversarial.py:386\u001b[0m, in \u001b[0;36madversarial_sports_eval.<locals>.update_accuracies\u001b[0;34m(eval_type, eval_constructor)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m                 temp_task \u001b[39m=\u001b[39m eval_constructor(batch_size\u001b[39m=\u001b[39mbatch_size, tokenizer\u001b[39m=\u001b[39mtokenizer, is_forget_dataset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, forget_sport_subset\u001b[39m=\u001b[39m{sport})\n\u001b[0;32m--> 386\u001b[0m             accuracies[eval_type][sport] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m temp_task\u001b[39m.\u001b[39;49mget_test_accuracy(model, continuous\u001b[39m=\u001b[39;49mcontinuous) \u001b[39m/\u001b[39m n_iters\n\u001b[1;32m    387\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     temp_task \u001b[39m=\u001b[39m eval_constructor(batch_size\u001b[39m=\u001b[39mbatch_size, tokenizer\u001b[39m=\u001b[39mtokenizer, use_icl\u001b[39m=\u001b[39muse_icl, use_system_prompt\u001b[39m=\u001b[39muse_system_prompt)\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskAdversarial.py:315\u001b[0m, in \u001b[0;36mSportsTask_Dashed.get_test_accuracy\u001b[0;34m(self, model, use_test_data, continuous)\u001b[0m\n\u001b[1;32m    312\u001b[0m new_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(completed_prompts[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39minput_ids) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(prompts[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39minput_ids)\n\u001b[1;32m    313\u001b[0m \u001b[39m# print(new_len)\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m sports_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(completed_prompts[\u001b[39m0\u001b[39;49m], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49minput_ids[\u001b[39m0\u001b[39;49m, \u001b[39m-\u001b[39;49mnew_len:]\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    316\u001b[0m \u001b[39m# print(f\"{sports_tokens=}\")\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[39m# cross entropy on new_len_tokens\u001b[39;00m\n\u001b[1;32m    319\u001b[0m last_logits \u001b[39m=\u001b[39m get_final_logits(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, completed_prompts, len_final_logits\u001b[39m=\u001b[39mnew_len\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Final evals\n",
    "final_adversarial_eval = adversarial_sports_eval(model, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True)\n",
    "print(f\"System Prompt: adversarial evals are {final_adversarial_eval}\")\n",
    "final_adversarial_eval = adversarial_sports_eval(model, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=False)\n",
    "print(f\"No System Prompt: adversarial evals are {final_adversarial_eval}\")\n",
    "\n",
    "final_side_effects = run_side_effects_evals(model, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\", \"Sports Familiarity\", \"Cross Entropy\"], verbose=True)\n",
    "print(final_side_effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_in': False, 'W_out': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mask_mlp_dict[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask.mlp_masks[5]['W_out'] - 1).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save masks state dict to neuron_cb\n",
    "torch.save(mask.state_dict(), \"masks/neuron_cb/mlps_unlearn_basketball.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
