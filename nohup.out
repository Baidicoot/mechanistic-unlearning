Gemma's activation function should be approximate GeLU and not exact GeLU.
Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.
OpenAI API key not found, will not be able to run evaluations on Sports Trivia Task

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.99s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.95s/it]
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:01,  1.93s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.50s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.67s/it]
/root/mechanistic-unlearning/weight_mask.py:140: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_Q'] = torch.tensor(
/root/mechanistic-unlearning/weight_mask.py:150: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_K'] = torch.tensor(
/root/mechanistic-unlearning/weight_mask.py:160: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_V'] = torch.tensor(
/root/mechanistic-unlearning/weight_mask.py:170: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_O'] = torch.tensor(
wandb: Currently logged in as: aaquib111. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /root/mechanistic-unlearning/wandb/run-20240520_192418-g7t9bhaw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gemma-7b-baseball-ap
wandb: ‚≠êÔ∏è View project at https://wandb.ai/aaquib111/mech-unlearning
wandb: üöÄ View run at https://wandb.ai/aaquib111/mech-unlearning/runs/g7t9bhaw
Loaded pretrained model google/gemma-7b into HookedTransformer
  0%|          | 0/50 [00:00<?, ?it/s]  2%|‚ñè         | 1/50 [01:11<58:21, 71.45s/it]  2%|‚ñè         | 1/50 [01:39<1:21:04, 99.27s/it]
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.2533036470413208
Running sports_1mp
Running maintain_sports
Running pile
Traceback (most recent call last):
  File "/root/mechanistic-unlearning/weight_mask.py", line 702, in <module>
    run()
  File "/root/mechanistic-unlearning/weight_mask.py", line 617, in run
    loss = task.get_train_loss(model) / grad_accum_steps
  File "/root/mechanistic-unlearning/tasks/task.py", line 61, in get_train_loss
    total_loss += self.calculate_loss(model, batch)
  File "/root/mechanistic-unlearning/tasks/general/EveryTokenTask.py", line 60, in calculate_loss
    loss = self.criterion(out.transpose(1, 2), shifted_token_batch)
  File "/root/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/root/venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 561.75 MiB is free. Process 536911 has 78.58 GiB memory in use. Of the allocated memory 77.55 GiB is allocated by PyTorch, and 530.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/mechanistic-unlearning/weight_mask.py", line 702, in <module>
    run()
  File "/root/mechanistic-unlearning/weight_mask.py", line 617, in run
    loss = task.get_train_loss(model) / grad_accum_steps
  File "/root/mechanistic-unlearning/tasks/task.py", line 61, in get_train_loss
    total_loss += self.calculate_loss(model, batch)
  File "/root/mechanistic-unlearning/tasks/general/EveryTokenTask.py", line 60, in calculate_loss
    loss = self.criterion(out.transpose(1, 2), shifted_token_batch)
  File "/root/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/root/venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 561.75 MiB is free. Process 536911 has 78.58 GiB memory in use. Of the allocated memory 77.55 GiB is allocated by PyTorch, and 530.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.009 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: 
wandb: Run history:
wandb:     test_loss_forget_sport ‚ñÅ
wandb:        test_loss_induction ‚ñÅ
wandb:   test_loss_maintain_sport ‚ñÅ
wandb:             test_loss_pile ‚ñÅ
wandb: train_loss_maintain_sports ‚ñÅ
wandb:            train_loss_pile ‚ñÅ
wandb:             train_loss_reg ‚ñÅ
wandb:      train_loss_sports_1mp ‚ñÅ
wandb: 
wandb: Run summary:
wandb:     test_loss_forget_sport 13.91208
wandb:        test_loss_induction 0.62799
wandb:   test_loss_maintain_sport 10.035
wandb:             test_loss_pile 2.82111
wandb: train_loss_maintain_sports 0.21256
wandb:            train_loss_pile 2.78664
wandb:             train_loss_reg 1.2533
wandb:      train_loss_sports_1mp 2.77274
wandb: 
wandb: üöÄ View run gemma-7b-baseball-ap at: https://wandb.ai/aaquib111/mech-unlearning/runs/g7t9bhaw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/aaquib111/mech-unlearning
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_192418-g7t9bhaw/logs
Gemma's activation function should be approximate GeLU and not exact GeLU.
Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.
OpenAI API key not found, will not be able to run evaluations on Sports Trivia Task
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:01,  1.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.40s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.56s/it]
/root/mechanistic-unlearning/weight_mask.py:140: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_Q'] = torch.tensor(
/root/mechanistic-unlearning/weight_mask.py:150: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_K'] = torch.tensor(
/root/mechanistic-unlearning/weight_mask.py:160: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_V'] = torch.tensor(
/root/mechanistic-unlearning/weight_mask.py:170: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  weight_mask_attn_dict[layer]['W_O'] = torch.tensor(
wandb: Currently logged in as: aaquib111. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /root/mechanistic-unlearning/wandb/run-20240520_192751-cn3hrkdc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gemma-7b-baseball-ap
wandb: ‚≠êÔ∏è View project at https://wandb.ai/aaquib111/mech-unlearning
wandb: üöÄ View run at https://wandb.ai/aaquib111/mech-unlearning/runs/cn3hrkdc
Loaded pretrained model google/gemma-7b into HookedTransformer
  0%|          | 0/50 [00:00<?, ?it/s]  2%|‚ñè         | 1/50 [00:59<48:16, 59.12s/it]  4%|‚ñç         | 2/50 [01:48<42:42, 53.39s/it]  6%|‚ñå         | 3/50 [02:42<42:11, 53.85s/it]  8%|‚ñä         | 4/50 [03:32<39:59, 52.15s/it] 10%|‚ñà         | 5/50 [04:25<39:25, 52.56s/it] 12%|‚ñà‚ñè        | 6/50 [05:23<39:47, 54.26s/it] 14%|‚ñà‚ñç        | 7/50 [06:12<37:36, 52.49s/it] 16%|‚ñà‚ñå        | 8/50 [07:00<35:46, 51.12s/it] 18%|‚ñà‚ñä        | 9/50 [07:51<35:00, 51.23s/it] 20%|‚ñà‚ñà        | 10/50 [08:42<33:58, 50.95s/it] 22%|‚ñà‚ñà‚ñè       | 11/50 [09:45<35:34, 54.74s/it] 24%|‚ñà‚ñà‚ñç       | 12/50 [10:44<35:26, 55.97s/it] 26%|‚ñà‚ñà‚ñå       | 13/50 [11:35<33:32, 54.39s/it] 28%|‚ñà‚ñà‚ñä       | 14/50 [12:23<31:35, 52.66s/it] 30%|‚ñà‚ñà‚ñà       | 15/50 [13:13<30:14, 51.85s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [14:11<30:21, 53.59s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [15:00<28:41, 52.16s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [15:57<28:35, 53.60s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [16:54<28:15, 54.69s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [17:53<27:57, 55.93s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [18:58<28:21, 58.69s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [19:50<26:25, 56.64s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [20:42<24:54, 55.34s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [21:32<23:18, 53.81s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [22:21<21:50, 52.42s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [23:19<21:37, 54.07s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [24:15<20:58, 54.70s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [25:10<20:00, 54.59s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [26:02<18:54, 54.01s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [26:51<17:26, 52.33s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [27:49<17:04, 53.94s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [28:39<15:50, 52.83s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [29:32<14:58, 52.83s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [30:29<14:26, 54.17s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [31:21<13:21, 53.43s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [32:28<13:24, 57.49s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [33:19<12:03, 55.66s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [34:12<10:57, 54.76s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [35:04<09:54, 54.05s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [35:54<08:47, 52.72s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [36:52<08:08, 54.28s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [37:41<07:02, 52.76s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [38:30<06:01, 51.62s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [39:19<05:04, 50.80s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [40:07<04:11, 50.24s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [41:07<03:31, 52.93s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [41:57<02:36, 52.04s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [42:46<01:42, 51.13s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [43:34<00:50, 50.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [44:32<00:00, 52.53s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [44:32<00:00, 53.45s/it]
wandb: - 0.008 MB of 0.017 MB uploadedwandb: \ 0.008 MB of 0.017 MB uploadedwandb: | 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:     test_loss_forget_sport ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:        test_loss_induction ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   test_loss_maintain_sport ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             test_loss_pile ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: train_loss_maintain_sports ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            train_loss_pile ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:             train_loss_reg ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      train_loss_sports_1mp ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:     test_loss_forget_sport 10.14441
wandb:        test_loss_induction 1.32956
wandb:   test_loss_maintain_sport 0.30637
wandb:             test_loss_pile 2.63201
wandb: train_loss_maintain_sports 0.00135
wandb:            train_loss_pile 2.55567
wandb:             train_loss_reg 1.92814
wandb:      train_loss_sports_1mp 0.0007
wandb: 
wandb: üöÄ View run gemma-7b-baseball-ap at: https://wandb.ai/aaquib111/mech-unlearning/runs/cn3hrkdc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/aaquib111/mech-unlearning
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_192751-cn3hrkdc/logs
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928159475326538
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281589984893799
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281588792800903
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928158164024353
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281572103500366
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281566143035889
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281562566757202
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281556606292725
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281548261642456
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281542301177979
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.92815363407135
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281527996063232
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281524419784546
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281516075134277
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.92815101146698
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281501770019531
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281489849090576
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281480312347412
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281474351882935
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281468391418457
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281466007232666
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928145408630371
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281450510025024
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281445741653442
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928143858909607
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281433820724487
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281432628631592
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928142786026001
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281426668167114
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281424283981323
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281424283981323
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281418323516846
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281418323516846
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281418323516846
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281415939331055
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281415939331055
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928141474723816
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.928141474723816
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281412363052368
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Running sports_1mp
Running maintain_sports
Running pile
reg loss, 1.9281409978866577
Gemma's activation function should be approximate GeLU and not exact GeLU.
Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.
OpenAI API key not found, will not be able to run evaluations on Sports Trivia Task
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.99s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:04<00:04,  2.08s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:01,  1.98s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.73s/it]
