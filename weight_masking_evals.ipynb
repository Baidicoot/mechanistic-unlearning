{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "from weight_masked_transformer import WeightMaskedTransformer\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('monology/pile-uncopyrighted', split='train', streaming=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4c4c1e1c60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = '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'\n",
    "model_type = \"gemma\"\n",
    "model_name = 'google/gemma-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def load_model(model_name=model_name):\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        device='cuda',\n",
    "        default_padding_side=\"right\",\n",
    "        fold_ln=False,\n",
    "        fold_value_biases=False,\n",
    "        center_writing_weights=False,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_mask(mask, threshold):\n",
    "    for layer in mask.keys():\n",
    "        for name, param in mask[layer].items():\n",
    "            mask[layer][name] = torch.where(param < threshold, torch.zeros_like(param), torch.ones_like(param))\n",
    "\n",
    "def apply_mask(model, mask):\n",
    "    for layer in mask.keys():\n",
    "        for name, mask_weight in mask[layer].items():\n",
    "            if getattr(model.blocks[layer].attn, name, None) is not None:\n",
    "                param = getattr(model.blocks[layer].attn, name)\n",
    "                param.data = param * mask_weight\n",
    "            elif getattr(model.blocks[layer].mlp, name, None) is not None:\n",
    "                param = getattr(model.blocks[layer].mlp, name)\n",
    "                param.data = param * mask_weight\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask name: {name} {layer=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap baseball 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-7b into HookedTransformer\n",
      "eval_name='Adversarial: No System Prompt'\n",
      "Normal {'football': 0.9917290210723876, 'baseball': 0.9974971055984497, 'basketball': 0.9635806322097777}\n",
      "MC {'football': 0.9765451788902282, 'baseball': 0.9347101926803589, 'basketball': 0.8438170909881593}\n",
      "Capitalized {'football': 0.9862990021705628, 'baseball': 0.9985299229621887, 'basketball': 0.9507709264755249}\n",
      "Dashed {'football': 0.4470346748828887, 'baseball': 0.9704322338104248, 'basketball': 0.972851002216339}\n",
      "eval_name='Adversarial: System Prompt'\n",
      "Normal {'football': 0.9916700839996337, 'baseball': 0.9970232963562011, 'basketball': 0.9700718045234681}\n",
      "MC {'football': 0.9763596534729004, 'baseball': 0.9355541110038758, 'basketball': 0.8356130599975585}\n",
      "Capitalized {'football': 0.987703800201416, 'baseball': 0.9986115574836731, 'basketball': 0.9554008960723877}\n",
      "Dashed {'football': 0.44432554244995115, 'baseball': 0.969933032989502, 'basketball': 0.9713426113128663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:56<00:00,  3.56it/s]\n",
      "/root/venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_name='Side Effects'\n",
      "Sports Answers {'football': 1.0, 'baseball': 1.0, 'basketball': 1.0, 'tennis': 1.0}\n",
      "Cross Entropy {'Pile': 2.258193778991699, 'OWT': 2.4734270572662354}\n",
      "ap basketball 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-7b into HookedTransformer\n",
      "eval_name='Adversarial: No System Prompt'\n",
      "Normal {'football': 0.991745150089264, 'baseball': 0.997555184364319, 'basketball': 0.9553537249565124}\n",
      "MC {'football': 0.9753389477729797, 'baseball': 0.9348410964012146, 'basketball': 0.8307579636573792}\n",
      "Capitalized {'football': 0.9865679621696473, 'baseball': 0.9984960198402405, 'basketball': 0.9462555885314942}\n",
      "Dashed {'football': 0.44029967188835145, 'baseball': 0.9696582436561585, 'basketball': 0.9681288480758667}\n",
      "eval_name='Adversarial: System Prompt'\n",
      "Normal {'football': 0.9914636969566345, 'baseball': 0.997358798980713, 'basketball': 0.9659188389778137}\n",
      "MC {'football': 0.9771671772003173, 'baseball': 0.9347681641578673, 'basketball': 0.8394045472145081}\n",
      "Capitalized {'football': 0.9884577870368957, 'baseball': 0.9984808206558227, 'basketball': 0.9542212486267091}\n",
      "Dashed {'football': 0.4554635226726532, 'baseball': 0.9693234920501709, 'basketball': 0.9792730689048768}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:57<00:00,  3.49it/s]\n",
      "/root/venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_name='Side Effects'\n",
      "Sports Answers {'football': 1.0, 'baseball': 1.0, 'basketball': 1.0, 'tennis': 1.0}\n",
      "Cross Entropy {'Pile': 2.2581936359405517, 'OWT': 2.473427104949951}\n",
      "ap football 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-7b into HookedTransformer\n",
      "eval_name='Adversarial: No System Prompt'\n",
      "Normal {'football': 0.9910276651382446, 'baseball': 0.9969815731048584, 'basketball': 0.9580461621284485}\n",
      "MC {'football': 0.9764938592910767, 'baseball': 0.9355010032653808, 'basketball': 0.8460409283638002}\n",
      "Capitalized {'football': 0.9872533679008484, 'baseball': 0.9984369516372682, 'basketball': 0.9493151426315307}\n",
      "Dashed {'football': 0.44449497461318965, 'baseball': 0.9686526179313659, 'basketball': 0.9759507656097411}\n",
      "eval_name='Adversarial: System Prompt'\n",
      "Normal {'football': 0.99196138381958, 'baseball': 0.9976850628852845, 'basketball': 0.9603007793426513}\n",
      "MC {'football': 0.9761391043663026, 'baseball': 0.9343187928199768, 'basketball': 0.8267905950546264}\n",
      "Capitalized {'football': 0.9870429992675782, 'baseball': 0.9985407829284667, 'basketball': 0.9525241255760193}\n",
      "Dashed {'football': 0.44811261892318727, 'baseball': 0.970418643951416, 'basketball': 0.9727262496948241}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:58<00:00,  3.45it/s]\n",
      "/root/venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_name='Side Effects'\n",
      "Sports Answers {'football': 1.0, 'baseball': 1.0, 'basketball': 1.0, 'tennis': 1.0}\n",
      "Cross Entropy {'Pile': 2.258193922042847, 'OWT': 2.4734270095825197}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "from tasks.facts.SportsTaskAdversarial import adversarial_sports_eval\n",
    "from tasks.facts.SportsTaskSideEffects import run_side_effects_evals\n",
    "\n",
    "# Final evals\n",
    "evals = {\n",
    "    \"Adversarial: No System Prompt\": partial(adversarial_sports_eval, use_system_prompt=True),\n",
    "    \"Adversarial: System Prompt\": partial(adversarial_sports_eval, use_system_prompt=True),\n",
    "    \"Side Effects\": partial(run_side_effects_evals, evals_to_run=[\"Cross Entropy\", \"Sports Answers\"], verbose=False), #  \"Sports Familiarity\",\n",
    "}\n",
    "eval_batch_size=50\n",
    "results = {}\n",
    "with torch.autocast(device_type=\"cuda\"), torch.set_grad_enabled(False):\n",
    "    for localization_type in [\"ap\"]:\n",
    "        results[localization_type] = {}\n",
    "        for forget_sport in [\"baseball\", \"basketball\", \"football\"]:\n",
    "            results[localization_type][forget_sport] = {}\n",
    "            for threshold in [0]:#, 0.05, 0.2, 0.5, 0.8, 0.95]:\n",
    "                print(localization_type, forget_sport, threshold)\n",
    "                results[localization_type][forget_sport][threshold] = {}\n",
    "                # Load Model\n",
    "                model = load_model()\n",
    "                mask = torch.load(f\"results/{model_name.replace('/', '_')}-{forget_sport}-{localization_type}.pt\")\n",
    "                threshold_mask(mask, threshold)\n",
    "                apply_mask(model, mask)\n",
    "                del mask\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                for eval_name, eval_func in evals.items():\n",
    "                    results[localization_type][forget_sport][threshold][eval_name] = {}\n",
    "                    eval_result = eval_func(model, model_type=model_type, batch_size=eval_batch_size)\n",
    "                    print(f'{eval_name=}')\n",
    "                    for k, v in eval_result.items():\n",
    "                        results[localization_type][forget_sport][threshold][eval_name][k] = v\n",
    "                        print(k, v)\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                del model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"results/{model_name.replace('/', '_')}-{localization_type}-results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
