{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aaquibsyed/Documents/Python/mechanistic-unlearning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "%cd ../../\n",
    "from dataset.custom_dataset import PairedInstructionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "with open('localizations/eap/eap_sports/sports_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "corr_sub_map = data['corr_sub_map']\n",
    "clean_sub_map = data['clean_sub_map']\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    device='mps:0',\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False\n",
    ")\n",
    "tokenizer=model.tokenizer\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "def tokenize_instructions(tokenizer, instructions):\n",
    "    # Use this to put the text into INST tokens or add a system prompt\n",
    "    return tokenizer(\n",
    "        instructions,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "dataset = PairedInstructionDataset(\n",
    "    N=4,\n",
    "    instruction_templates=data['instruction_templates'],\n",
    "    harmful_substitution_map=corr_sub_map,\n",
    "    harmless_substitution_map=clean_sub_map,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenize_instructions=tokenize_instructions, \n",
    "    device='mps:0'\n",
    ")\n",
    "\n",
    "corr_dataset = dataset.harmful_dataset\n",
    "clean_dataset = dataset.harmless_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fact: Tiger Woods plays the sport of golf\\nFact: Andre Ingram plays the sport of', 'Fact: Tiger Woods plays the sport of golf\\nFact: Miles Austin plays the sport of', 'Fact: Tiger Woods plays the sport of golf\\nFact: Michael Vick plays the sport of', 'Fact: Tiger Woods plays the sport of golf\\nFact: Greg Jennings plays the sport of']\n",
      "['Fact: Tiger Woods plays the sport of golf\\nFact: Lawrence Christian plays the sport of', 'Fact: Tiger Woods plays the sport of golf\\nFact: Karen Walker plays the sport of', 'Fact: Tiger Woods plays the sport of golf\\nFact: Austin Armstrong plays the sport of', 'Fact: Tiger Woods plays the sport of golf\\nFact: Hannah Fleming plays the sport of']\n"
     ]
    }
   ],
   "source": [
    "print(clean_dataset.str_prompts)\n",
    "print(corr_dataset.str_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('localizations/eap/eap_sports/sport_answers.json') as f:\n",
    "    answers = json.load(f)\n",
    "    player_to_sport_tok = {}\n",
    "    set_of_sports = set(['football', 'baseball', 'basketball'])\n",
    "    sport_to_tok = {}\n",
    "    for sport in set_of_sports:\n",
    "        sport_to_tok[sport] = tuple(tokenizer(' ' + sport).input_ids)\n",
    "\n",
    "    for player, sport in zip(answers['players'], answers['sports']):\n",
    "        wrong_sports = set_of_sports - {sport}\n",
    "        wrong_sport_toks = [sport_to_tok[wrong_sport] for wrong_sport in wrong_sports]\n",
    "        player_tuple = tuple(tokenizer(' ' + player).input_ids)\n",
    "        player_to_sport_tok[player_tuple] = (sport_to_tok[sport], wrong_sport_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9669, 4346, 4346, 4346], device='mps:0')\n",
      "tensor([[[4346],\n",
      "         [9283]],\n",
      "\n",
      "        [[9669],\n",
      "         [9283]],\n",
      "\n",
      "        [[9669],\n",
      "         [9283]],\n",
      "\n",
      "        [[9669],\n",
      "         [9283]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "clean_tok_answers = []\n",
    "clean_tok_wrong_answers = []\n",
    "\n",
    "for i in range(len(clean_dataset.deltas)):\n",
    "    start = clean_dataset.deltas[i][\"{player}\"].start\n",
    "    end = clean_dataset.deltas[i][\"{player}\"].stop\n",
    "    correct_sport_tok, wrong_sports_toks = player_to_sport_tok[tuple(clean_dataset.toks[i, start:end].tolist())]\n",
    "    clean_tok_answers.append(\n",
    "        torch.tensor(correct_sport_tok)\n",
    "    )\n",
    "    clean_tok_wrong_answers.append(\n",
    "        torch.tensor(wrong_sports_toks)\n",
    "    )\n",
    "clean_tok_answers = torch.tensor(clean_tok_answers, device='mps:0')\n",
    "clean_tok_wrong_answers = torch.stack(clean_tok_wrong_answers).to(device='mps:0')\n",
    "print(clean_tok_answers)\n",
    "print(clean_tok_wrong_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: -0.10040664672851562, Corrupt direction: -0.1717205047607422\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "from localizations.eap.eap_wrapper import EAP\n",
    "\n",
    "def ave_logit_diff(\n",
    "    logits,\n",
    "    clean_answers,\n",
    "    wrong_answers,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    logits_correct = logits[list(range(logits.size(0))), -1, clean_answers]\n",
    "    logits_incorrect = logits[list(range(logits.size(0))), -1, wrong_answers].mean(dim=1)\n",
    "    logit_diff = logits_correct - logits_incorrect\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_dataset.toks)\n",
    "    corrupt_logits = model(corr_dataset.toks)\n",
    "    clean_logit_diff = ave_logit_diff(\n",
    "        clean_logits, \n",
    "        clean_answers=clean_tok_answers, \n",
    "        wrong_answers=clean_tok_wrong_answers\n",
    "    ).item()\n",
    "    corrupt_logit_diff = ave_logit_diff(\n",
    "        corrupt_logits, \n",
    "        clean_answers=clean_tok_answers, \n",
    "        wrong_answers=clean_tok_wrong_answers\n",
    "    ).item()\n",
    "\n",
    "def refusals_metric(\n",
    "    logits,\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(\n",
    "        logits,\n",
    "        clean_answers=clean_tok_answers,\n",
    "        wrong_answers=clean_tok_wrong_answers,\n",
    "    )\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "# Get clean and corrupt logit differences\n",
    "with torch.no_grad():\n",
    "    clean_metric = refusals_metric(clean_logits, corrupt_logit_diff, clean_logit_diff)\n",
    "    corrupt_metric = refusals_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations requires 0.0004 GB of memory per token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mlp.0', 'head.3.0.k', -1.3594273328781128), ('mlp.0', 'head.3.0.q', -1.2176820039749146), ('head.0.7', 'mlp.0', 0.8883270621299744), ('mlp.0', 'mlp.1', 0.6781187653541565), ('mlp.0', 'head.10.0.v', 0.6231864094734192), ('mlp.2', 'mlp.4', -0.5430629253387451), ('head.0.5', 'mlp.0', 0.4555762708187103), ('head.0.1', 'mlp.0', 0.43618831038475037), ('mlp.0', 'mlp.6', 0.4263656735420227), ('mlp.0', 'head.8.11.k', -0.40294504165649414), ('mlp.3', 'head.5.10.q', 0.39495518803596497), ('mlp.0', 'mlp.4', 0.3860459625720978), ('mlp.2', 'head.3.0.q', -0.3547334671020508), ('mlp.9', 'mlp.10', -0.3467930853366852), ('mlp.0', 'head.5.10.q', -0.33183416724205017), ('mlp.7', 'mlp.8', 0.32305455207824707), ('mlp.0', 'mlp.3', 0.31020641326904297), ('head.0.7', 'mlp.2', 0.30756640434265137), ('head.0.4', 'mlp.0', -0.30755868554115295), ('mlp.4', 'mlp.6', -0.2975465655326843), ('mlp.3', 'mlp.6', -0.2954271137714386), ('mlp.0', 'head.5.0.q', -0.2882046401500702), ('head.0.1', 'head.3.0.q', -0.2831178605556488), ('mlp.0', 'mlp.8', 0.2810089588165283), ('mlp.1', 'mlp.5', -0.27490854263305664), ('mlp.1', 'head.3.0.q', -0.26851242780685425), ('mlp.0', 'head.2.10.v', 0.26832059025764465), ('mlp.6', 'mlp.7', -0.2622745931148529), ('head.0.7', 'head.5.0.q', 0.2611598074436188), ('mlp.7', 'head.10.0.k', -0.26069846749305725), ('mlp.5', 'mlp.8', 0.26000267267227173), ('mlp.0', 'head.9.8.v', 0.257323682308197), ('mlp.3', 'head.8.11.k', 0.2533281743526459), ('head.7.8', 'head.10.0.q', 0.24161449074745178), ('head.9.8', 'mlp.10', 0.2409844696521759), ('head.0.1', 'head.3.0.k', -0.2374599725008011), ('mlp.2', 'head.3.0.k', -0.23200291395187378), ('mlp.9', 'head.10.0.k', 0.22994348406791687), ('mlp.1', 'mlp.8', -0.22847393155097961), ('mlp.8', 'head.10.0.k', 0.22038252651691437), ('head.0.10', 'mlp.0', -0.2203175127506256), ('mlp.2', 'mlp.3', -0.2196151167154312), ('mlp.0', 'head.1.6.q', 0.21674314141273499), ('head.8.11', 'mlp.10', 0.21206916868686676), ('mlp.4', 'head.10.0.k', 0.2096366137266159), ('mlp.1', 'head.10.0.k', 0.20791290700435638), ('head.8.11', 'head.10.0.q', 0.20734646916389465), ('head.8.8', 'mlp.10', -0.20546384155750275), ('mlp.9', 'mlp.11', -0.20379607379436493), ('head.0.3', 'head.8.11.k', -0.20209428668022156), ('mlp.0', 'head.3.0.v', 0.19834382832050323), ('head.2.2', 'mlp.2', 0.19744186103343964), ('mlp.0', 'head.5.8.q', -0.1933024674654007), ('mlp.6', 'mlp.8', 0.19320574402809143), ('head.7.8', 'mlp.11', 0.19154603779315948), ('mlp.0', 'head.5.9.q', -0.1902894675731659), ('mlp.4', 'head.5.10.q', 0.1891692876815796), ('head.0.7', 'head.5.8.q', 0.18854019045829773), ('head.4.11', 'mlp.4', 0.1884017139673233), ('mlp.4', 'mlp.5', 0.18643023073673248), ('mlp.3', 'mlp.7', 0.18584275245666504), ('head.0.7', 'mlp.5', -0.17637783288955688), ('head.0.1', 'head.7.8.k', 0.1760261356830597), ('mlp.1', 'head.8.11.k', 0.17554590106010437), ('head.0.3', 'mlp.0', 0.17346754670143127), ('head.0.1', 'mlp.3', -0.17172370851039886), ('mlp.8', 'mlp.11', 0.17085440456867218), ('mlp.7', 'head.8.11.k', -0.1694697141647339), ('head.0.0', 'mlp.0', -0.16673080623149872), ('mlp.3', 'head.10.0.k', -0.16604316234588623), ('mlp.6', 'head.8.11.k', 0.16564682126045227), ('head.1.6', 'mlp.1', 0.15953834354877472), ('mlp.7', 'mlp.9', 0.15855850279331207), ('mlp.6', 'head.7.8.v', 0.1576947718858719), ('mlp.2', 'mlp.8', -0.15633445978164673), ('mlp.10', 'mlp.11', -0.15623773634433746), ('head.0.4', 'mlp.2', 0.1557534635066986), ('head.0.4', 'head.3.0.k', -0.15372511744499207), ('mlp.3', 'mlp.4', -0.15342625975608826), ('head.0.5', 'mlp.2', -0.1533505767583847), ('head.0.5', 'head.3.0.k', -0.15294459462165833), ('mlp.0', 'head.8.8.v', 0.15265291929244995), ('head.0.7', 'head.3.0.k', -0.15121062099933624), ('head.7.8', 'mlp.8', -0.14979475736618042), ('mlp.0', 'mlp.9', -0.14952898025512695), ('head.0.7', 'mlp.3', -0.14707165956497192), ('mlp.0', 'head.2.11.k', 0.1462951898574829), ('mlp.4', 'head.9.8.k', 0.14555150270462036), ('head.6.2', 'head.10.0.k', -0.14524316787719727), ('head.3.10', 'mlp.4', -0.14132386445999146), ('head.0.5', 'mlp.3', -0.14105021953582764), ('mlp.0', 'head.4.6.v', -0.14034733176231384), ('head.5.8', 'mlp.5', 0.1403225213289261), ('head.8.5', 'mlp.8', -0.13882198929786682), ('head.0.10', 'mlp.2', -0.1385069042444229), ('mlp.2', 'head.5.8.q', 0.13738778233528137), ('mlp.1', 'head.9.8.k', 0.13732214272022247), ('mlp.3', 'mlp.8', 0.13634227216243744), ('mlp.0', 'head.2.11.v', -0.1358168125152588), ('head.0.4', 'mlp.6', 0.13551929593086243)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "model.reset_hooks()\n",
    "\n",
    "#%%\n",
    "from eap_wrapper import EAP\n",
    "\n",
    "graph = EAP(\n",
    "    model,\n",
    "    clean_dataset.toks,\n",
    "    corr_dataset.toks,\n",
    "    refusals_metric,\n",
    "    upstream_nodes=[\"mlp\", \"head\"],\n",
    "    downstream_nodes=[\"mlp\", \"head\"],\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "top_edges = graph.top_edges(n=100, abs_scores=True)\n",
    "print(top_edges)\n",
    "\n",
    "# %%\n",
    "\n",
    "# graph.show(threshold=0.01, abs_scores=True, fname=\"eap_subgraph_bs=100.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
