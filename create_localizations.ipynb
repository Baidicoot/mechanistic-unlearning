{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and save localizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('acdcpp/Automatic-Circuit-Discovery/')\n",
    "sys.path.append('acdcpp/')\n",
    "# from acdc import TLACDCExperiment\n",
    "# from acdcpp.ACDCPPExperiment import ACDCPPExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# import acdc\n",
    "# from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "# from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('mps:0')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACDCPP/EAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaquibsyed/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# set up pipeline from acdcpp to edge mask\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.666549248"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOI Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found, will not be able to run evaluations on HPSAQ Task\n",
      "OpenAI API key not found, will not be able to run evaluations on HPSAQ Task\n",
      "Clean logit diff: 3.040117025375366, Corrupted logit diff: 1.2651995420455933\n",
      "Clean logit diff: 3.040, Corrupt logit diff: 1.265\n"
     ]
    }
   ],
   "source": [
    "from tasks.ioi.IOITask import IOITask_old, IOITask\n",
    "# ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25)\n",
    "ioi_task = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device, prep_acdcpp=True, acdcpp_N=25, nb_templates=1, prompt_type=\"ABBA\")\n",
    "ioi_task.set_logit_diffs(model)\n",
    "\n",
    "ioi_metric = ioi_task.get_acdcpp_metric()\n",
    "def negative_abs_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ioi_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ioi_task.clean_data.toks)\n",
    "    corrupt_logits = model(ioi_task.corr_data.toks)\n",
    "    clean_logit_diff = ioi_task.ave_logit_diff(clean_logits, ioi_task.clean_data).item()\n",
    "    corrupt_logit_diff = ioi_task.ave_logit_diff(corrupt_logits, ioi_task.corr_data).item()\n",
    "    print(f'Clean logit diff: {clean_logit_diff:.3f}, Corrupt logit diff: {corrupt_logit_diff:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15299.74it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 258.14it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 281818.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:08<00:08,  8.11s/it]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15224.87it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 253.97it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 306343.88it/s]\n",
      "100%|██████████| 2/2 [00:15<00:00,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.05]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ioi_task.clean_data.toks,\n",
    "    corr_data=ioi_task.corr_data.toks,\n",
    "    acdc_metric=negative_abs_ioi_metric,\n",
    "    acdcpp_metric=ioi_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=THRESHOLDS[0])\n",
    "\n",
    "with open(f\"localizations/eap/ioi/exp_threshold={THRESHOLDS[0]}\", \"wb\") as f:\n",
    "    pickle.dump(acdcpp_exp, f)\n",
    "\n",
    "with open(f\"localizations/eap/ioi/gpt2_threshold={THRESHOLDS[0]}\", \"wb\") as f:\n",
    "    pickle.dump((acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.induction.InductionTask import InductionTask\n",
    "ind_task = InductionTask(batch_size=5, tokenizer=model.tokenizer, prep_acdcpp=True, seq_len=15, acdcpp_metric=\"ave_logit_diff\")\n",
    "ind_task.set_logit_diffs(model)\n",
    "\n",
    "ind_metric = ind_task.get_acdcpp_metric()\n",
    "def negative_abs_ind_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ind_metric(logits))\n",
    "\n",
    "THRESHOLDS = [0.05]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ind_task.clean_data,\n",
    "    corr_data=ind_task.corr_data,\n",
    "    acdc_metric=negative_abs_ind_metric,\n",
    "    acdcpp_metric=ind_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=THRESHOLDS[0])\n",
    "\n",
    "# with open(f\"localizations/eap/induction/exp_threshold={THRESHOLDS[0]}\", \"wb\") as f:\n",
    "    # pickle.dump(acdcpp_exp, f)\n",
    "\n",
    "with open(f\"localizations/eap/induction/gpt2_threshold={THRESHOLDS[0]}\", \"wb\") as f:\n",
    "    pickle.dump((acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('head.0.2', 'mlp.0', 0.009443754330277443), ('head.0.14', 'mlp.0', 0.006188404746353626), ('mlp.6', 'mlp.8', 0.00573932658880949), ('mlp.0', 'mlp.2', 0.005653967149555683), ('head.16.20', 'mlp.16', -0.005345507059246302), ('head.16.20', 'head.17.30.v', 0.005315450485795736), ('mlp.0', 'head.1.16.k', -0.005109565332531929), ('head.14.14', 'mlp.15', -0.004921694286167622), ('mlp.15', 'head.16.20.k', -0.004847021773457527), ('mlp.6', 'mlp.15', -0.004780464340001345), ('mlp.10', 'head.16.20.k', 0.004766407422721386), ('mlp.6', 'head.16.20.k', 0.004757486749440432), ('mlp.0', 'head.1.15.k', -0.004494336899369955), ('mlp.0', 'mlp.5', -0.004315529949963093), ('mlp.0', 'mlp.4', -0.004094669129699469), ('head.16.20', 'mlp.18', 0.004019735846668482), ('head.0.30', 'mlp.0', 0.0039080469869077206), ('mlp.0', 'mlp.6', -0.0038432518485933542), ('mlp.8', 'mlp.9', 0.00376768596470356), ('head.16.20', 'head.21.9.v', 0.003543522208929062), ('mlp.9', 'mlp.11', 0.003415714716538787), ('mlp.6', 'head.21.9.k', 0.0033131211530417204), ('head.15.4', 'mlp.15', -0.003284941893070936), ('head.15.20', 'head.16.20.q', -0.0032149143517017365), ('mlp.5', 'mlp.6', 0.003205507528036833), ('mlp.9', 'mlp.13', 0.003195255296304822), ('mlp.3', 'mlp.6', 0.0031151920557022095), ('mlp.5', 'mlp.8', 0.003063748823478818), ('mlp.12', 'head.16.20.k', -0.0030587592627853155), ('mlp.0', 'head.1.17.k', -0.0030355819035321474), ('mlp.11', 'head.16.20.v', 0.0030052291695028543), ('head.9.15', 'head.16.20.k', 0.002988740336149931), ('head.15.20', 'mlp.15', 0.002843316178768873), ('mlp.4', 'mlp.8', 0.002790694823488593), ('head.15.4', 'mlp.16', -0.002768619218841195), ('mlp.13', 'head.16.20.v', 0.002720218151807785), ('mlp.11', 'mlp.14', 0.0025788003113120794), ('mlp.11', 'head.16.20.k', -0.0025367813650518656), ('mlp.0', 'mlp.7', -0.002536351792514324), ('mlp.11', 'mlp.15', 0.0024518363643437624), ('mlp.8', 'head.16.20.v', 0.0023244901094585657), ('head.0.27', 'mlp.0', -0.0023238908033818007), ('mlp.6', 'mlp.11', 0.002282592235133052), ('head.17.30', 'mlp.18', 0.0022805416956543922), ('mlp.0', 'head.21.9.k', 0.0022331280633807182), ('head.0.21', 'mlp.0', 0.0022144378162920475), ('mlp.3', 'mlp.15', -0.00221416843123734), ('head.15.20', 'head.17.30.q', -0.002205037511885166), ('mlp.10', 'mlp.15', -0.0022013422567397356), ('mlp.4', 'head.16.20.k', 0.0021811285987496376), ('mlp.2', 'mlp.3', 0.0021742351818829775), ('mlp.30', 'mlp.31', -0.002144202124327421), ('mlp.5', 'head.16.20.k', 0.0021362670231610537), ('head.16.17', 'head.17.30.q', 0.002123921876773238), ('mlp.29', 'mlp.31', -0.00211007590405643), ('mlp.14', 'head.16.20.v', 0.0021046672482043505), ('mlp.10', 'mlp.11', -0.0020922652911394835), ('head.12.17', 'mlp.15', -0.0020885460544377565), ('mlp.0', 'mlp.13', -0.0020814668387174606), ('mlp.5', 'head.16.20.v', 0.0020730250980705023), ('mlp.28', 'mlp.31', -0.0020570242777466774), ('mlp.7', 'mlp.8', 0.00204459554515779), ('head.9.15', 'head.11.21.q', -0.002041686326265335), ('mlp.0', 'mlp.14', -0.002014364581555128), ('mlp.0', 'head.15.5.k', -0.002007126808166504), ('mlp.15', 'mlp.16', 0.001995496451854706), ('head.14.14', 'mlp.16', -0.001992548583075404), ('mlp.6', 'mlp.14', -0.001976989908143878), ('mlp.15', 'head.17.30.k', -0.0019326637266203761), ('head.16.20', 'head.22.15.v', 0.001931019127368927), ('mlp.6', 'mlp.12', 0.0019294897792860866), ('head.13.8', 'head.21.9.q', 0.0019150624284520745), ('mlp.0', 'mlp.11', -0.0019050340633839369), ('head.0.21', 'mlp.1', 0.0018889722414314747), ('head.7.8', 'mlp.13', -0.0018879767740145326), ('head.1.25', 'mlp.2', 0.0018849290208891034), ('head.15.4', 'head.16.20.q', 0.0018740722443908453), ('mlp.12', 'mlp.13', 0.001870404346846044), ('head.16.20', 'mlp.17', -0.0018696865299716592), ('head.10.1', 'mlp.15', -0.0018582759657874703), ('mlp.1', 'mlp.2', 0.0018570123938843608), ('mlp.2', 'mlp.5', 0.0018493117531761527), ('mlp.0', 'mlp.1', -0.001831269939430058), ('head.0.6', 'mlp.0', 0.00180960597936064), ('mlp.10', 'head.21.9.k', 0.001803320599719882), ('head.0.31', 'mlp.0', -0.0017993211513385177), ('mlp.12', 'head.15.20.k', 0.0017812863225117326), ('mlp.14', 'head.21.9.k', 0.0017715865978971124), ('head.7.8', 'head.16.20.k', 0.0017514253268018365), ('mlp.10', 'mlp.12', -0.001751400763168931), ('head.16.20', 'head.21.9.k', 0.0017360210185870528), ('mlp.5', 'mlp.15', -0.0017260626191273332), ('head.9.15', 'head.17.30.k', 0.0017186870099976659), ('head.15.4', 'head.21.9.k', 0.00171701202634722), ('mlp.5', 'mlp.9', 0.0017162136500701308), ('mlp.6', 'mlp.9', 0.001710148761048913), ('mlp.3', 'head.16.20.k', 0.0016996695194393396), ('mlp.3', 'mlp.8', 0.001693469937890768), ('mlp.0', 'head.1.11.v', 0.0016771022928878665), ('mlp.0', 'head.1.18.k', 0.001676780404523015), ('head.15.20', 'mlp.16', 0.0016647606389597058), ('mlp.2', 'mlp.4', 0.0016523718368262053), ('head.15.4', 'head.16.10.v', 0.0016409446252509952), ('mlp.4', 'mlp.5', 0.0015915663680061698), ('head.7.8', 'mlp.15', 0.001587998354807496), ('head.10.11', 'mlp.11', -0.0015877102268859744), ('head.16.20', 'head.25.1.k', -0.0015855319797992706), ('mlp.8', 'mlp.14', 0.0015662244986742735), ('head.14.14', 'head.16.20.q', 0.0015645340317860246), ('mlp.18', 'head.21.9.v', 0.0015612227143719792), ('head.17.30', 'mlp.17', -0.0015610615955665708), ('mlp.7', 'mlp.10', -0.0015545483911409974), ('head.7.14', 'mlp.15', 0.0015438116388395429), ('mlp.3', 'mlp.5', 0.0015396539820358157), ('head.9.15', 'mlp.12', -0.001538568758405745), ('head.9.15', 'mlp.14', -0.0015276921913027763), ('head.17.30', 'head.21.9.v', 0.0015151547268033028), ('head.16.17', 'mlp.16', -0.0015022397274151444), ('mlp.24', 'head.25.1.k', -0.0014950997428968549), ('mlp.8', 'mlp.13', 0.0014932940248399973), ('mlp.4', 'head.21.9.k', 0.0014929039170965552), ('mlp.6', 'mlp.13', 0.0014775244053453207), ('head.9.15', 'mlp.13', -0.0014682551845908165), ('mlp.7', 'head.16.20.v', 0.0014557429822161794), ('mlp.0', 'head.21.31.k', -0.0014490863541141152), ('head.12.17', 'head.15.4.v', 0.0014430660521611571), ('mlp.6', 'head.7.20.v', 0.0014389472780749202), ('mlp.0', 'head.4.13.q', 0.0014347839169204235), ('mlp.0', 'head.1.16.q', 0.0014134803786873817), ('mlp.8', 'mlp.10', 0.0014106555609032512), ('mlp.0', 'head.21.19.k', -0.001405258197337389), ('head.17.30', 'head.21.9.k', 0.0014030528254806995), ('mlp.9', 'mlp.10', 0.0013922336511313915), ('mlp.0', 'mlp.3', -0.0013902803184464574), ('mlp.6', 'head.16.20.v', 0.0013750517973676324), ('head.9.3', 'mlp.9', -0.0013734701788052917), ('head.13.8', 'head.17.30.q', 0.0013706028694286942), ('head.16.20', 'head.22.17.v', 0.0013674175133928657), ('head.13.8', 'head.15.20.q', -0.0013636015355587006), ('mlp.8', 'head.14.14.q', -0.001362730166874826), ('mlp.0', 'head.16.21.k', 0.0013574105687439442), ('head.7.8', 'head.17.30.k', 0.0013550223084166646), ('head.19.24', 'head.21.9.v', 0.00135287013836205), ('head.0.28', 'mlp.0', -0.001351020997390151), ('head.14.14', 'head.15.20.q', -0.0013473035069182515), ('mlp.0', 'head.1.10.k', 0.0013392082182690501), ('mlp.10', 'head.15.20.q', 0.0013274125522002578), ('mlp.7', 'mlp.14', -0.0013219445245340466), ('head.12.17', 'head.15.20.v', -0.0013092361623421311), ('mlp.11', 'head.17.30.k', -0.0013053971342742443), ('mlp.7', 'head.21.9.k', 0.001304464996792376), ('mlp.0', 'head.1.4.v', 0.0012995852157473564), ('head.13.8', 'head.14.31.q', -0.0012961990432813764), ('mlp.21', 'mlp.31', 0.0012819045223295689), ('mlp.15', 'head.16.20.v', 0.001273989793844521), ('head.13.8', 'head.16.20.q', 0.001270874636247754), ('mlp.0', 'head.1.25.k', -0.001269094762392342), ('mlp.20', 'mlp.31', 0.0012679877690970898), ('mlp.7', 'mlp.11', 0.0012615789892151952), ('mlp.9', 'head.11.21.q', 0.0012575825676321983), ('head.0.30', 'mlp.3', 0.001251908834092319), ('mlp.0', 'head.24.21.k', 0.0012514435220509768), ('mlp.8', 'head.16.20.k', 0.0012460232246667147), ('head.0.16', 'mlp.0', 0.0012409038608893752), ('mlp.0', 'head.15.11.k', -0.001235301955603063), ('head.0.17', 'mlp.0', 0.0012327723670750856), ('mlp.23', 'head.25.1.k', -0.0012299207737669349), ('head.12.17', 'head.16.20.v', 0.0012290324084460735), ('mlp.12', 'mlp.15', 0.0012218646006658673), ('head.15.4', 'head.16.20.v', 0.001221155864186585), ('head.0.4', 'mlp.0', 0.001221106736920774), ('head.12.17', 'head.21.9.k', 0.001220965525135398), ('mlp.12', 'head.16.20.v', 0.0012155117001384497), ('head.16.20', 'head.21.19.k', -0.001211330178193748), ('mlp.3', 'head.21.9.k', 0.0011980809504166245), ('head.16.20', 'head.17.30.q', -0.0011938184034079313), ('mlp.15', 'head.16.17.k', -0.001190812443383038), ('head.13.30', 'mlp.14', 0.0011839853832498193), ('head.8.11', 'mlp.15', -0.0011744748335331678), ('mlp.8', 'head.15.20.v', -0.0011725453659892082), ('head.14.14', 'head.21.9.k', 0.0011706198565661907), ('mlp.6', 'head.15.20.k', -0.0011685850331559777), ('mlp.25', 'mlp.26', -0.0011683834018185735), ('mlp.4', 'head.16.20.v', 0.0011622239835560322), ('mlp.8', 'mlp.11', 0.0011603647144511342), ('mlp.2', 'mlp.15', -0.001156176207587123), ('head.9.3', 'mlp.14', -0.001154008787125349), ('mlp.15', 'head.16.20.q', -0.001148901996202767), ('head.17.30', 'head.22.15.v', 0.0011446698335930705), ('head.13.30', 'mlp.15', 0.0011397490743547678), ('mlp.18', 'head.25.1.k', -0.0011324080405756831), ('head.13.20', 'head.15.4.v', 0.001122756046243012), ('mlp.21', 'head.25.1.k', -0.0011200009612366557), ('head.0.7', 'mlp.0', 0.0011196527630090714), ('mlp.13', 'head.15.20.k', 0.0011141891591250896), ('head.7.9', 'mlp.15', -0.001111720921471715), ('mlp.2', 'head.21.9.k', 0.001110269222408533), ('head.1.25', 'mlp.3', 0.0011059733806177974), ('head.9.15', 'head.21.9.k', 0.0011041940888389945), ('mlp.0', 'head.1.4.k', -0.001102518173865974), ('mlp.23', 'head.26.25.k', 0.001102395704947412), ('mlp.15', 'head.16.17.v', -0.0011007385328412056), ('head.13.30', 'head.16.20.q', -0.001094624400138855), ('mlp.18', 'head.21.9.k', 0.0010888677788898349), ('head.8.14', 'mlp.9', -0.0010858307359740138), ('head.15.4', 'head.17.30.q', 0.0010818063747137785), ('mlp.6', 'head.15.20.v', -0.001078712404705584), ('head.9.15', 'head.10.26.v', -0.0010778617579489946), ('mlp.0', 'mlp.8', -0.0010757920099422336), ('head.15.27', 'head.16.20.q', -0.0010755263501778245), ('mlp.7', 'head.14.14.v', -0.001074428204447031), ('head.15.4', 'head.21.9.q', 0.0010719193378463387), ('head.14.14', 'head.16.17.v', 0.0010705209570005536), ('mlp.4', 'mlp.12', -0.0010661009000614285), ('head.7.8', 'head.15.20.k', 0.0010634062346071005), ('mlp.23', 'mlp.28', -0.001054505119100213), ('head.7.8', 'head.14.8.v', 0.0010530278086662292), ('mlp.13', 'head.15.4.v', 0.0010519307106733322), ('mlp.11', 'head.16.17.k', -0.0010491471039131284), ('mlp.9', 'mlp.18', -0.001046185614541173), ('head.5.17', 'mlp.7', 0.001044679433107376), ('mlp.8', 'head.13.8.v', 0.0010418110759928823), ('head.7.15', 'mlp.9', -0.0010394830023869872), ('mlp.26', 'mlp.30', -0.0010386754292994738), ('mlp.6', 'head.16.21.k', 0.0010373503901064396), ('head.15.20', 'head.16.17.v', -0.0010349199874326587), ('mlp.0', 'head.1.12.v', 0.0010330087970942259), ('head.12.26', 'mlp.14', -0.0010282696457579732), ('mlp.0', 'head.1.6.v', 0.0010257066460326314), ('head.13.30', 'mlp.13', 0.001021881471388042), ('mlp.6', 'head.20.2.k', -0.0010205552680417895), ('head.13.30', 'head.16.20.k', -0.0010205348953604698), ('head.15.4', 'head.21.19.k', -0.0010198857635259628), ('mlp.8', 'head.13.30.v', -0.0010180402314290404), ('head.15.4', 'head.25.1.k', -0.0010171744506806135), ('mlp.0', 'head.1.17.v', 0.0010166249703615904), ('head.14.14', 'head.17.30.q', 0.0010091098956763744), ('head.14.14', 'head.15.20.k', -0.001008716062642634), ('mlp.0', 'head.22.17.k', 0.0010071411961689591), ('head.17.30', 'head.25.1.k', -0.0010067331604659557), ('head.9.3', 'head.15.4.v', 0.001002408331260085), ('mlp.13', 'head.16.20.k', 0.0010006306692957878)]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.001\n",
    "import pickle\n",
    "with open('localizations/eap/eap_sports/1000_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "\n",
    "eap_edges = graph.top_edges(n=1000, threshold=threshold)\n",
    "# eap_edges = set()\n",
    "# for i in range(eap_scores.shape[0]):\n",
    "#     for j in range(eap_scores.shape[1]):\n",
    "#         if eap_scores[i, j] > threshold:\n",
    "            # eap_edges.add((get_node_name(graph.node_names[i], show_full_index=False), get_node_name(graph.node_names[j, show_full_index=False))))\n",
    "print(eap_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.eap_scores.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert format:\n",
    "want: {((3, 'm3'), (3, 'a3.0')),\n",
    " ((4, 'm4'), (0, 'm0')),\n",
    " ((4, 'm4'), (3, 'a3.0')),\n",
    " ((5, 'a5.9'), (0, 'm0')),}\n",
    "\n",
    "have:\n",
    "[('mlp.0', 'mlp.2', 0.005653967149555683),\n",
    "('head.0.14', 'mlp.0', 0.006188404746353626),]\n",
    "...\n",
    "\"\"\"\n",
    "from cb_utils.mask_utils import get_formatted_edges_from_eap, get_masks_from_eap_exp\n",
    "# formatted_eap_edges = get_formatted_edges_from_eap(eap_edges)\n",
    "# formatted_eap_edges\n",
    "with open('localizations/eap/eap_sports/1000_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_eap_exp(graph, threshold=0.001, num_layers=32, num_heads=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acdcpp_mask_dict['m31'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.292320251464844 1.4027974605560303\n"
     ]
    }
   ],
   "source": [
    "from tasks import InductionTask\n",
    "ind_task = InductionTask(batch_size=16, tokenizer=model.tokenizer, prep_acdcpp=True, seq_len=10, acdcpp_metric=\"ave_logit_diff\")\n",
    "ind_task.set_logit_diffs(model)\n",
    "print(ind_task.clean_logit_diff, ind_task.corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_metric = ind_task.get_acdcpp_metric()\n",
    "def negative_abs_ind_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ind_metric(logits))\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(ind_task.clean_data.cuda())\n",
    "    corrupt_logits = model(ind_task.corr_data.cuda())\n",
    "    clean_logit_diff = ind_task.ave_logit_diff(clean_logits, ind_task.clean_data).item()\n",
    "    corrupt_logit_diff = ind_task.ave_logit_diff(corrupt_logits, ind_task.corr_data).item()\n",
    "    \n",
    "print(ind_metric(clean_logits))\n",
    "print(ind_metric(corrupt_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 15171.34it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:04<00:00, 252.85it/s]\n",
      "Edge pruning: 100%|██████████| 1034/1034 [00:00<00:00, 304067.19it/s]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 11, 10, 9, 8, 7, 5, 0, 1, 2, 3, 4, 6, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "from cb_utils.mask_utils import get_masks_from_acdcpp_exp\n",
    "THRESHOLDS = [0.05]#np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'abs_edge'\n",
    "\n",
    "acdcpp_exp = ACDCPPExperiment(\n",
    "    model=model,\n",
    "    clean_data=ind_task.clean_data,\n",
    "    corr_data=ind_task.corr_data,\n",
    "    acdc_metric=negative_abs_ind_metric,\n",
    "    acdcpp_metric=ind_metric,\n",
    "    thresholds=THRESHOLDS,\n",
    "    run_name=RUN_NAME,\n",
    "    verbose=False,\n",
    "    attr_absolute_val=True,\n",
    "    save_graphs_after=-100,\n",
    "    pruning_mode='edge',\n",
    "    no_pruned_nodes_attr=1,\n",
    "    run_acdc=False,\n",
    "    run_acdcpp=True,\n",
    ")\n",
    "# e=acdcpp_exp.setup_exp(0.0)\n",
    "\n",
    "# pruned_heads, num_passes, acdcpp_pruned_attrs, acdc_pruned_attrs, edges_after_acdcpp, edges_after_acdc = acdcpp_exp.run()\n",
    "acdcpp_nodes, acdcpp_edges, acdcpp_mask_dict, acdcpp_weight_mask_attn_dict, acdcpp_weight_mask_mlp_dict = get_masks_from_acdcpp_exp(acdcpp_exp, threshold=THRESHOLDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline from acdcpp to edge mask\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from localizations.causal_tracing.causal_tracing import get_causal_tracing_components\n",
    "from tasks.induction import InductionTask\n",
    "\n",
    "ind_task = torch.stack(\n",
    "    InductionTask(batch_size=16, tokenizer=model.tokenizer).train_data,\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "ind_prompts = model.tokenizer.batch_decode(ind_task[:, :-1])\n",
    "ind_answers = model.tokenizer.batch_decode(ind_task[:, -1])\n",
    "\n",
    "data = {\n",
    "    'text': ind_prompts,\n",
    "    'IO': ind_answers\n",
    "}\n",
    "\n",
    "components = get_causal_tracing_components(\n",
    "    model,\n",
    "    data,\n",
    "    n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c539467d3664c7387b01b5afb7c939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942829ae09a24e5489d165a6774f5b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e503b6212684e6d8a180c087ffddf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39266928464d4c178f0a4b6d7dab85fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9754346781c24c1ca0999a2bbdef1ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69455123e03a4845a95802cbce3db379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47c8ab4b76b46f7a8dec732c8ff836f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab70e988ac948f09b0f30d91851bc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ce6f219b6147f8bac77b7f09dc266a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b00dc5eb6ae4dc8b17d88c419ff4eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdf0d279c29448284591da6313c7006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e81cefb4b47491d9661efe9082d3354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2b9ce34c6d4405ab2343ca24064492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9221b9efd3af432eb2baf54f09f90090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0349e777efc141df936d693be2a9f451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cbf62a9b7a4f7d8c59a7a11e94c346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8863c80573e742e997ef243b491cdc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5216951e535248a884dce1be61333816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d18054a45a41c7bac5792bd5f0c8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e484513e7574e22a63621f9e30e0f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099baa9a02204be18a781c150c7c55d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f042f3cd1a048f6a6f9ed2fb0a9efc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a292571df8484c1695f1d9dce825bd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db5db42484b4f438547042da7b3c988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9126c86ea0e2459f86aaffa0a004c6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0822e60bd0429e98f6c458d5b0e606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7f9376338e4670a0729a0970033e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb39db39a0d34c5290b770f390d820d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940d9785b05840a59c1f8674822e940c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282edb44082f485ea099befba9b5aae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0130dba8224efa8a5626e7ae3691e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0077eb01791e46bb8c09e11a51106ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cef321c3c44e1dad0a791a910394f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43c79a9ec74403494b4cadd00ad2200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddabf3a0612e463cb141c8881eeaa19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f7461baa8546c8a70b369ec64e18f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c092e8a2d1a4fc9a9d426af47cb0324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8c76853aed46b3a29f0ed938bd097a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba3f735fd904e53ab5a8dfe36436361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9ac109eb4e4c0d8c2666f1045ce472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290976f41ddc4f17ab304621792b6984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cafbfba8c74f7283d488b9eaf29969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b3ea1f302a44f5abfec9eda2fb4b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ec02073e9a4bb78638fae0c1fda0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bda799ab46c4ae3b134790f94e869ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771a7edbfc5942c0a06775276924d007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4ef2ec78904affb3323edeecd6ffd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eae2fd489ec40ce821ff3bc0407c5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb8ad193fc74f708a42e255ac009b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8fa6fb0fab4f22a10f14f84cb22505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff6d2ce87fa49d0aba2575e1e90f0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc3c1e1c3274696b8dd29091558b899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642299e9a1ee4ec4b376a4466a8a84e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87c688ccc5e44a9b4f7990168dff949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb6c43baba14908aca6c570bd9f6327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677358669b6e44dbbeaa28cf8355ee63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5896f6c0a6c64b62844cce761d162afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab4c84313b94a0f88ed1cd0126ec030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bf1fd71e8b43d6aa7b159271527976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64eafdc38164449bb340875f8e05a03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9b97e2d9434dc0ac4e9e094f3df01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb Cell 25\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtasks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mioi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mIOITask\u001b[39;00m \u001b[39mimport\u001b[39;00m IOITask_old, IOITask\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ioi_data \u001b[39m=\u001b[39m IOITask(batch_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, tokenizer\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mtokenizer, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39mioi_data\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m components \u001b[39m=\u001b[39m get_causal_tracing_components(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ioi_data,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocalizations/causal_tracing/ioi/gpt2_threshold=all\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaquibsyed/Documents/Python/mechanistic-unlearning/create_localizations.ipynb#Y105sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(components)\n",
      "File \u001b[0;32m~/Documents/Python/mechanistic-unlearning/localizations/causal_tracing/causal_tracing.py:317\u001b[0m, in \u001b[0;36mget_causal_tracing_components\u001b[0;34m(model, ioi_data, n)\u001b[0m\n\u001b[1;32m    310\u001b[0m patching_result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m    311\u001b[0m     model\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mn_layers,\n\u001b[1;32m    312\u001b[0m     model\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mn_heads \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    313\u001b[0m     \u001b[39mlen\u001b[39m(ioi_data\u001b[39m.\u001b[39mtoks[\u001b[39m0\u001b[39m]),\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m ioi_data\u001b[39m.\u001b[39mioi_prompts[:n]:\n\u001b[0;32m--> 317\u001b[0m     patching_result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m get_patching_results(\n\u001b[1;32m    318\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    319\u001b[0m         prompt\u001b[39m=\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m# TODO: many prompts\u001b[39;49;00m\n\u001b[1;32m    320\u001b[0m         hook_names\u001b[39m=\u001b[39;49mhook_names,  \u001b[39m# TODO: make this neater\u001b[39;49;00m\n\u001b[1;32m    321\u001b[0m         subject\u001b[39m=\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mIO\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    322\u001b[0m         num_seeds\u001b[39m=\u001b[39;49mNUM_SEEDS,\n\u001b[1;32m    323\u001b[0m         noise_coefficient\u001b[39m=\u001b[39;49mNOISE_COEFFICIENT,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    326\u001b[0m patching_result \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(ioi_data)\n\u001b[1;32m    327\u001b[0m top_k \u001b[39m=\u001b[39m get_top_k_nodes(patching_result, k\u001b[39m=\u001b[39m\u001b[39m145\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Python/mechanistic-unlearning/localizations/causal_tracing/causal_tracing.py:180\u001b[0m, in \u001b[0;36mget_patching_results\u001b[0;34m(model, prompt, subject, hook_names, num_seeds, noise_coefficient)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m hook_name:\n\u001b[1;32m    170\u001b[0m     hooks \u001b[39m=\u001b[39m [\n\u001b[1;32m    171\u001b[0m         (\n\u001b[1;32m    172\u001b[0m             hook_name\u001b[39m.\u001b[39mformat(layer\u001b[39m=\u001b[39mlayer),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m         ),\n\u001b[1;32m    179\u001b[0m     ]\n\u001b[0;32m--> 180\u001b[0m     patched_prob \u001b[39m=\u001b[39m get_corrupted_probs(\n\u001b[1;32m    181\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    182\u001b[0m         tokens\u001b[39m=\u001b[39;49mtokens,\n\u001b[1;32m    183\u001b[0m         corruption_start\u001b[39m=\u001b[39;49mcorruption_start,\n\u001b[1;32m    184\u001b[0m         corruption_end\u001b[39m=\u001b[39;49mcorruption_end,\n\u001b[1;32m    185\u001b[0m         noise_coefficient\u001b[39m=\u001b[39;49mnoise_coefficient,\n\u001b[1;32m    186\u001b[0m         num_seeds\u001b[39m=\u001b[39;49mnum_seeds,\n\u001b[1;32m    187\u001b[0m         hooks\u001b[39m=\u001b[39;49mhooks,\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     patched_score \u001b[39m=\u001b[39m patched_prob \u001b[39m/\u001b[39m prob_diff\n\u001b[1;32m    190\u001b[0m     layer_node \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Python/mechanistic-unlearning/localizations/causal_tracing/causal_tracing.py:118\u001b[0m, in \u001b[0;36mget_corrupted_probs\u001b[0;34m(model, tokens, corruption_start, corruption_end, noise_coefficient, num_seeds, hooks)\u001b[0m\n\u001b[1;32m    108\u001b[0m noise_hook_fn \u001b[39m=\u001b[39m partial(\n\u001b[1;32m    109\u001b[0m     noise_embedding_hook,\n\u001b[1;32m    110\u001b[0m     corruption_start\u001b[39m=\u001b[39mcorruption_start,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m fwd_hooks\u001b[39m.\u001b[39minsert(\n\u001b[1;32m    116\u001b[0m     \u001b[39m0\u001b[39m, (utils\u001b[39m.\u001b[39mget_act_name(\u001b[39m\"\u001b[39m\u001b[39mresid_pre\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m), noise_hook_fn)\n\u001b[1;32m    117\u001b[0m )  \u001b[39m# put it at beginning of list\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m corrupted_logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_hooks(tokens, fwd_hooks\u001b[39m=\u001b[39;49mfwd_hooks)\n\u001b[1;32m    119\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    120\u001b[0m     corrupted_prob, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(\n\u001b[1;32m    121\u001b[0m         torch\u001b[39m.\u001b[39msoftmax(corrupted_logits[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Python/TransformerLens/transformer_lens/hook_points.py:365\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    359\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    363\u001b[0m     fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts\n\u001b[1;32m    364\u001b[0m ) \u001b[39mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mreturn\u001b[39;00m hooked_model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n",
      "File \u001b[0;32m~/Documents/Python/TransformerLens/transformer_lens/HookedTransformer.py:557\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    554\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    555\u001b[0m         )\n\u001b[0;32m--> 557\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    558\u001b[0m         residual,\n\u001b[1;32m    559\u001b[0m         \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    560\u001b[0m         \u001b[39m# block\u001b[39;49;00m\n\u001b[1;32m    561\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    562\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    563\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    564\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    565\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    566\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[39mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Python/TransformerLens/transformer_lens/components.py:1188\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     key_input \u001b[39m=\u001b[39m attn_in\n\u001b[1;32m   1182\u001b[0m     value_input \u001b[39m=\u001b[39m attn_in\n\u001b[1;32m   1184\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1185\u001b[0m     \u001b[39m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m     \u001b[39m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[39m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m-> 1188\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m   1189\u001b[0m         query_input\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(query_input)\n\u001b[1;32m   1190\u001b[0m         \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m   1191\u001b[0m         key_input\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(key_input)\n\u001b[1;32m   1192\u001b[0m         \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m   1193\u001b[0m         value_input\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(value_input),\n\u001b[1;32m   1194\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache_entry,\n\u001b[1;32m   1195\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1196\u001b[0m     )\n\u001b[1;32m   1197\u001b[0m )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1199\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1200\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m   1201\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from localizations.causal_tracing.causal_tracing import get_causal_tracing_components\n",
    "# from tasks.ioi.IOITask import IOITask_old, IOITask\n",
    "\n",
    "# ioi_data = IOITask(batch_size=5, tokenizer=model.tokenizer, device=device).ioi_data\n",
    "\n",
    "# components = get_causal_tracing_components(\n",
    "#     model,\n",
    "#     ioi_data,\n",
    "#     n=100  \n",
    "# )\n",
    "\n",
    "# with open(f\"localizations/causal_tracing/ioi/gpt2_threshold=all\", \"wb\") as f:\n",
    "#     pickle.dump(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cb_utils.transformer import DemoTransformer\n",
    "from cb_utils.models import load_demo_gpt2, tokenizer\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "edge_masks = True\n",
    "weight_masks_attn = True\n",
    "weight_masks_mlp = True\n",
    "train_base_weights = True\n",
    "localize_acdcpp = True\n",
    "\n",
    "# if edge_masks is True, then have mask_dict_superset be acdcpp_mask_dict\n",
    "mask_dict_superset = None if not edge_masks else acdcpp_mask_dict\n",
    "# model = load_demo_gpt2(means=means, mask_dict_superset=acdcpp_mask_dict)\n",
    "if localize_acdcpp:\n",
    "    weight_mask_attn_dict = acdcpp_weight_mask_attn_dict if weight_masks_attn else None\n",
    "    weight_mask_mlp_dict = acdcpp_weight_mask_mlp_dict if weight_masks_mlp else None\n",
    "    base_weight_attn_dict = acdcpp_weight_mask_attn_dict if train_base_weights else None\n",
    "    base_weight_mlp_dict = acdcpp_weight_mask_mlp_dict if train_base_weights else None\n",
    "\n",
    "else:\n",
    "    weight_mask_attn_dict = None\n",
    "    weight_mask_mlp_dict = None\n",
    "    base_weight_attn_dict = None\n",
    "    base_weight_mlp_dict = None\n",
    "\n",
    "# model = load_demo_gpt2(means=False, edge_masks=edge_masks, mask_dict_superset=mask_dict_superset, weight_masks_attn=weight_masks_attn, weight_masks_mlp=weight_masks_mlp, weight_mask_attn_dict=weight_mask_attn_dict, weight_mask_mlp_dict=weight_mask_mlp_dict, train_base_weights=train_base_weights, base_weight_attn_dict=base_weight_attn_dict, base_weight_mlp_dict=base_weight_mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.models import load_demo_pythia\n",
    "model = load_demo_pythia(means=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_pythia =  HookedTransformer.from_pretrained(\n",
    "        'pythia-2.8b',\n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        default_padding_side=\"left\",\n",
    "        device='cuda'\n",
    "    )\n",
    "tokenizer = reference_pythia.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_tokenizer = reference_pythia.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "tensor(1.8776, device='cuda:0', grad_fn=<MaxBackward1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/phillip_guo/mechanistic-unlearning/setup_models.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/mechanistic-unlearning/setup_models.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# compare model outputs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/mechanistic-unlearning/setup_models.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_input \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor(pythia_tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mThe quick brown fox jumps over the lazy\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/mechanistic-unlearning/setup_models.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m (model(test_input)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reference_pythia(test_input)[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m \u001b[39m1e-5\u001b[39m, (model(test_input)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reference_pythia(test_input)[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax()\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(1.8776, device='cuda:0', grad_fn=<MaxBackward1>)"
     ]
    }
   ],
   "source": [
    "# compare model outputs\n",
    "test_input = t.tensor(pythia_tokenizer.encode(\"The quick brown fox jumps over the lazy\")).unsqueeze(0).cuda()\n",
    "assert (model(test_input)[0][0, -1] - reference_pythia(test_input)[0, -1]).abs().max() < 1e-5, (model(test_input)[0][0, -1] - reference_pythia(test_input)[0, -1]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5317, -2.0861,  9.2626,  ..., -2.3424, -2.3985, -2.5804],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_input)[0][0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.6789, -3.9404,  7.4135,  ..., -4.1955, -4.2537, -4.4353],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_pythia(test_input)[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythia_tokenizer.decode(torch.argmax(model(test_input)[0][0, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dog'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythia_tokenizer.decode(torch.argmax(reference_pythia(test_input)[0, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.1681, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tasks.induction.InductionTask import InductionTask, InductionTask_Uniform\n",
    "\n",
    "ind_uniform_task = InductionTask_Uniform(batch_size=16, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, uniform_over=\"rep_tokens\")\n",
    "ind_uniform_task.get_test_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6161, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_task.get_test_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.789679616"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=device) / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that gradients flow correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_mask torch.Size([157]) True\n",
      "blocks.0.edge_mask_attentions torch.Size([1, 12]) True\n",
      "blocks.0.edge_mask_mlp torch.Size([13]) True\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.0.attn.b_O torch.Size([768]) True\n",
      "blocks.0.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.0.mlp.b_in torch.Size([3072]) True\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.0.mlp.b_out torch.Size([768]) True\n",
      "blocks.0.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.0.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.0.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.0.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.1.edge_mask_attentions torch.Size([14, 12]) True\n",
      "blocks.1.edge_mask_mlp torch.Size([26]) True\n",
      "blocks.1.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.1.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.1.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.1.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.1.attn.b_O torch.Size([768]) True\n",
      "blocks.1.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.1.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.1.mlp.b_in torch.Size([3072]) True\n",
      "blocks.1.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.1.mlp.b_out torch.Size([768]) True\n",
      "blocks.1.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.1.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.1.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.1.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.2.edge_mask_attentions torch.Size([27, 12]) True\n",
      "blocks.2.edge_mask_mlp torch.Size([39]) True\n",
      "blocks.2.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.2.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.2.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.2.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.2.attn.b_O torch.Size([768]) True\n",
      "blocks.2.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.2.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.2.mlp.b_in torch.Size([3072]) True\n",
      "blocks.2.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.2.mlp.b_out torch.Size([768]) True\n",
      "blocks.2.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.2.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.2.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.2.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.3.edge_mask_attentions torch.Size([40, 12]) True\n",
      "blocks.3.edge_mask_mlp torch.Size([52]) True\n",
      "blocks.3.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.3.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.3.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.3.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.3.attn.b_O torch.Size([768]) True\n",
      "blocks.3.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.3.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.3.mlp.b_in torch.Size([3072]) True\n",
      "blocks.3.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.3.mlp.b_out torch.Size([768]) True\n",
      "blocks.3.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.3.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.3.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.3.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.4.edge_mask_attentions torch.Size([53, 12]) True\n",
      "blocks.4.edge_mask_mlp torch.Size([65]) True\n",
      "blocks.4.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.4.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.4.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.4.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.4.attn.b_O torch.Size([768]) True\n",
      "blocks.4.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.4.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.4.mlp.b_in torch.Size([3072]) True\n",
      "blocks.4.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.4.mlp.b_out torch.Size([768]) True\n",
      "blocks.4.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.4.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.4.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.4.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.5.edge_mask_attentions torch.Size([66, 12]) True\n",
      "blocks.5.edge_mask_mlp torch.Size([78]) True\n",
      "blocks.5.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.5.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.5.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.5.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.5.attn.b_O torch.Size([768]) True\n",
      "blocks.5.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.5.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.5.mlp.b_in torch.Size([3072]) True\n",
      "blocks.5.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.5.mlp.b_out torch.Size([768]) True\n",
      "blocks.5.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.5.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.5.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.5.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.6.edge_mask_attentions torch.Size([79, 12]) True\n",
      "blocks.6.edge_mask_mlp torch.Size([91]) True\n",
      "blocks.6.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.6.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.6.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.6.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.attn.b_O torch.Size([768]) True\n",
      "blocks.6.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.b_out torch.Size([768]) True\n",
      "blocks.6.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.7.edge_mask_attentions torch.Size([92, 12]) True\n",
      "blocks.7.edge_mask_mlp torch.Size([104]) True\n",
      "blocks.7.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.7.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.7.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.7.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.7.attn.b_O torch.Size([768]) True\n",
      "blocks.7.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.7.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.7.mlp.b_in torch.Size([3072]) True\n",
      "blocks.7.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.7.mlp.b_out torch.Size([768]) True\n",
      "blocks.7.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.7.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.7.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.7.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.8.edge_mask_attentions torch.Size([105, 12]) True\n",
      "blocks.8.edge_mask_mlp torch.Size([117]) True\n",
      "blocks.8.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.8.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.8.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.8.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.8.attn.b_O torch.Size([768]) True\n",
      "blocks.8.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.8.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.8.mlp.b_in torch.Size([3072]) True\n",
      "blocks.8.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.8.mlp.b_out torch.Size([768]) True\n",
      "blocks.8.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.8.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.8.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.8.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.9.edge_mask_attentions torch.Size([118, 12]) True\n",
      "blocks.9.edge_mask_mlp torch.Size([130]) True\n",
      "blocks.9.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.9.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.9.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.9.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.9.attn.b_O torch.Size([768]) True\n",
      "blocks.9.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.9.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.9.mlp.b_in torch.Size([3072]) True\n",
      "blocks.9.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.9.mlp.b_out torch.Size([768]) True\n",
      "blocks.9.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.9.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.9.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.9.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.10.edge_mask_attentions torch.Size([131, 12]) True\n",
      "blocks.10.edge_mask_mlp torch.Size([143]) True\n",
      "blocks.10.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.10.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.10.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.10.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.10.attn.b_O torch.Size([768]) True\n",
      "blocks.10.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n",
      "blocks.10.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.10.mlp.b_in torch.Size([3072]) True\n",
      "blocks.10.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.10.mlp.b_out torch.Size([768]) True\n",
      "blocks.10.mlp.weight_mask_W_in torch.Size([768, 3072]) True\n",
      "blocks.10.mlp.weight_mask_W_out torch.Size([3072, 768]) True\n",
      "blocks.10.mlp.weight_mask_b_in torch.Size([3072]) True\n",
      "blocks.10.mlp.weight_mask_b_out torch.Size([768]) True\n",
      "blocks.11.edge_mask_attentions torch.Size([144, 12]) True\n",
      "blocks.11.edge_mask_mlp torch.Size([156]) True\n",
      "blocks.11.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.11.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.11.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.11.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.11.attn.b_O torch.Size([768]) True\n",
      "blocks.11.attn.weight_mask_W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_K torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_V torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.weight_mask_W_O torch.Size([12, 64, 768]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "    # print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "batch_size = 8\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False)\n",
    "loss = ioi.get_train_loss(model)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.701484032\n",
      "9.50002944\n",
      "11.296608768\n",
      "13.095366144\n",
      "14.886672896\n",
      "16.662592\n",
      "18.435150336\n",
      "20.20689152\n",
      "21.983021568\n",
      "23.762801152\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(10):\n",
    "    losses.append(ioi.get_train_loss(model))\n",
    "    print(torch.cuda.memory_allocated(device=device) / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_mask grad is all zeros\n",
      "blocks.0.edge_mask_attentions grad is all zeros\n",
      "blocks.0.edge_mask_mlp grad is all zeros\n",
      "blocks.0.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.1548, device='cuda:0')\n",
      "blocks.0.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0549, device='cuda:0')\n",
      "blocks.0.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.2216, device='cuda:0')\n",
      "blocks.0.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.4179e-08, device='cuda:0')\n",
      "blocks.0.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.2717, device='cuda:0')\n",
      "blocks.0.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.7394, device='cuda:0')\n",
      "blocks.0.attn.W_O grad is not all zeros, param.grad.norm()=tensor(2.7787, device='cuda:0')\n",
      "blocks.0.attn.b_O grad is not all zeros, param.grad.norm()=tensor(1.2196, device='cuda:0')\n",
      "blocks.0.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.8217, device='cuda:0')\n",
      "blocks.0.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4786, device='cuda:0')\n",
      "blocks.0.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.2637, device='cuda:0')\n",
      "blocks.0.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2465, device='cuda:0')\n",
      "blocks.1.edge_mask_attentions grad is all zeros\n",
      "blocks.1.edge_mask_mlp grad is all zeros\n",
      "blocks.1.attn.W_Q grad is all zeros\n",
      "blocks.1.attn.b_Q grad is all zeros\n",
      "blocks.1.attn.W_K grad is all zeros\n",
      "blocks.1.attn.b_K grad is all zeros\n",
      "blocks.1.attn.W_V grad is all zeros\n",
      "blocks.1.attn.b_V grad is all zeros\n",
      "blocks.1.attn.W_O grad is all zeros\n",
      "blocks.1.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2197, device='cuda:0')\n",
      "blocks.1.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.6849, device='cuda:0')\n",
      "blocks.1.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6041, device='cuda:0')\n",
      "blocks.1.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4991, device='cuda:0')\n",
      "blocks.1.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3097, device='cuda:0')\n",
      "blocks.2.edge_mask_attentions grad is all zeros\n",
      "blocks.2.edge_mask_mlp grad is all zeros\n",
      "blocks.2.attn.W_Q grad is all zeros\n",
      "blocks.2.attn.b_Q grad is all zeros\n",
      "blocks.2.attn.W_K grad is all zeros\n",
      "blocks.2.attn.b_K grad is all zeros\n",
      "blocks.2.attn.W_V grad is all zeros\n",
      "blocks.2.attn.b_V grad is all zeros\n",
      "blocks.2.attn.W_O grad is all zeros\n",
      "blocks.2.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2781, device='cuda:0')\n",
      "blocks.2.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.3378, device='cuda:0')\n",
      "blocks.2.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5941, device='cuda:0')\n",
      "blocks.2.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.9843, device='cuda:0')\n",
      "blocks.2.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3619, device='cuda:0')\n",
      "blocks.3.edge_mask_attentions grad is all zeros\n",
      "blocks.3.edge_mask_mlp grad is all zeros\n",
      "blocks.3.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.3450, device='cuda:0')\n",
      "blocks.3.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0399, device='cuda:0')\n",
      "blocks.3.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.2601, device='cuda:0')\n",
      "blocks.3.attn.b_K grad is not all zeros, param.grad.norm()=tensor(4.4314e-09, device='cuda:0')\n",
      "blocks.3.attn.W_V grad is not all zeros, param.grad.norm()=tensor(0.9300, device='cuda:0')\n",
      "blocks.3.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.3861, device='cuda:0')\n",
      "blocks.3.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.8948, device='cuda:0')\n",
      "blocks.3.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.3325, device='cuda:0')\n",
      "blocks.3.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.6125, device='cuda:0')\n",
      "blocks.3.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4856, device='cuda:0')\n",
      "blocks.3.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.5817, device='cuda:0')\n",
      "blocks.3.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3593, device='cuda:0')\n",
      "blocks.4.edge_mask_attentions grad is all zeros\n",
      "blocks.4.edge_mask_mlp grad is all zeros\n",
      "blocks.4.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.0010, device='cuda:0')\n",
      "blocks.4.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0001, device='cuda:0')\n",
      "blocks.4.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.0007, device='cuda:0')\n",
      "blocks.4.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.4829e-09, device='cuda:0')\n",
      "blocks.4.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.3327, device='cuda:0')\n",
      "blocks.4.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.1678, device='cuda:0')\n",
      "blocks.4.attn.W_O grad is not all zeros, param.grad.norm()=tensor(1.0120, device='cuda:0')\n",
      "blocks.4.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.3405, device='cuda:0')\n",
      "blocks.4.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.3849, device='cuda:0')\n",
      "blocks.4.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6280, device='cuda:0')\n",
      "blocks.4.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(3.0652, device='cuda:0')\n",
      "blocks.4.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3845, device='cuda:0')\n",
      "blocks.5.edge_mask_attentions grad is all zeros\n",
      "blocks.5.edge_mask_mlp grad is all zeros\n",
      "blocks.5.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.0015, device='cuda:0')\n",
      "blocks.5.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.1075, device='cuda:0')\n",
      "blocks.5.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.7923, device='cuda:0')\n",
      "blocks.5.attn.b_K grad is not all zeros, param.grad.norm()=tensor(8.4344e-09, device='cuda:0')\n",
      "blocks.5.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.6556, device='cuda:0')\n",
      "blocks.5.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.5560, device='cuda:0')\n",
      "blocks.5.attn.W_O grad is not all zeros, param.grad.norm()=tensor(1.6231, device='cuda:0')\n",
      "blocks.5.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.3329, device='cuda:0')\n",
      "blocks.5.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.5079, device='cuda:0')\n",
      "blocks.5.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6716, device='cuda:0')\n",
      "blocks.5.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.9281, device='cuda:0')\n",
      "blocks.5.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3576, device='cuda:0')\n",
      "blocks.6.edge_mask_attentions grad is all zeros\n",
      "blocks.6.edge_mask_mlp grad is all zeros\n",
      "blocks.6.attn.W_Q grad is all zeros\n",
      "blocks.6.attn.b_Q grad is all zeros\n",
      "blocks.6.attn.W_K grad is all zeros\n",
      "blocks.6.attn.b_K grad is all zeros\n",
      "blocks.6.attn.W_V grad is all zeros\n",
      "blocks.6.attn.b_V grad is all zeros\n",
      "blocks.6.attn.W_O grad is all zeros\n",
      "blocks.6.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2853, device='cuda:0')\n",
      "blocks.6.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.5282, device='cuda:0')\n",
      "blocks.6.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6701, device='cuda:0')\n",
      "blocks.6.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.9120, device='cuda:0')\n",
      "blocks.6.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3113, device='cuda:0')\n",
      "blocks.7.edge_mask_attentions grad is all zeros\n",
      "blocks.7.edge_mask_mlp grad is all zeros\n",
      "blocks.7.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.3832, device='cuda:0')\n",
      "blocks.7.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.1353, device='cuda:0')\n",
      "blocks.7.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.9461, device='cuda:0')\n",
      "blocks.7.attn.b_K grad is not all zeros, param.grad.norm()=tensor(4.4310e-09, device='cuda:0')\n",
      "blocks.7.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.0588, device='cuda:0')\n",
      "blocks.7.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.2373, device='cuda:0')\n",
      "blocks.7.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.4142, device='cuda:0')\n",
      "blocks.7.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.2509, device='cuda:0')\n",
      "blocks.7.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7239, device='cuda:0')\n",
      "blocks.7.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5642, device='cuda:0')\n",
      "blocks.7.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.3637, device='cuda:0')\n",
      "blocks.7.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2601, device='cuda:0')\n",
      "blocks.8.edge_mask_attentions grad is all zeros\n",
      "blocks.8.edge_mask_mlp grad is all zeros\n",
      "blocks.8.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.5048, device='cuda:0')\n",
      "blocks.8.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.1685, device='cuda:0')\n",
      "blocks.8.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.9461, device='cuda:0')\n",
      "blocks.8.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.2591e-08, device='cuda:0')\n",
      "blocks.8.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.7726, device='cuda:0')\n",
      "blocks.8.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.3488, device='cuda:0')\n",
      "blocks.8.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.8881, device='cuda:0')\n",
      "blocks.8.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.1886, device='cuda:0')\n",
      "blocks.8.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.4894, device='cuda:0')\n",
      "blocks.8.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.7993, device='cuda:0')\n",
      "blocks.8.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9286, device='cuda:0')\n",
      "blocks.8.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1984, device='cuda:0')\n",
      "blocks.9.edge_mask_attentions grad is all zeros\n",
      "blocks.9.edge_mask_mlp grad is all zeros\n",
      "blocks.9.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(0.9216, device='cuda:0')\n",
      "blocks.9.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.0945, device='cuda:0')\n",
      "blocks.9.attn.W_K grad is not all zeros, param.grad.norm()=tensor(0.7821, device='cuda:0')\n",
      "blocks.9.attn.b_K grad is not all zeros, param.grad.norm()=tensor(6.5322e-09, device='cuda:0')\n",
      "blocks.9.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.6014, device='cuda:0')\n",
      "blocks.9.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.3802, device='cuda:0')\n",
      "blocks.9.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.7301, device='cuda:0')\n",
      "blocks.9.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.1410, device='cuda:0')\n",
      "blocks.9.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7937, device='cuda:0')\n",
      "blocks.9.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5903, device='cuda:0')\n",
      "blocks.9.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.6236, device='cuda:0')\n",
      "blocks.9.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1541, device='cuda:0')\n",
      "blocks.10.edge_mask_attentions grad is all zeros\n",
      "blocks.10.edge_mask_mlp grad is all zeros\n",
      "blocks.10.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.9649, device='cuda:0')\n",
      "blocks.10.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.2903, device='cuda:0')\n",
      "blocks.10.attn.W_K grad is not all zeros, param.grad.norm()=tensor(1.4916, device='cuda:0')\n",
      "blocks.10.attn.b_K grad is not all zeros, param.grad.norm()=tensor(1.5129e-08, device='cuda:0')\n",
      "blocks.10.attn.W_V grad is not all zeros, param.grad.norm()=tensor(1.2587, device='cuda:0')\n",
      "blocks.10.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.2849, device='cuda:0')\n",
      "blocks.10.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.6736, device='cuda:0')\n",
      "blocks.10.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.0921, device='cuda:0')\n",
      "blocks.10.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.5749, device='cuda:0')\n",
      "blocks.10.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3302, device='cuda:0')\n",
      "blocks.10.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.1263, device='cuda:0')\n",
      "blocks.10.mlp.b_out grad is all zeros\n",
      "blocks.11.edge_mask_attentions grad is all zeros\n",
      "blocks.11.edge_mask_mlp grad is all zeros\n",
      "blocks.11.attn.W_Q grad is not all zeros, param.grad.norm()=tensor(1.6651, device='cuda:0')\n",
      "blocks.11.attn.b_Q grad is not all zeros, param.grad.norm()=tensor(0.2231, device='cuda:0')\n",
      "blocks.11.attn.W_K grad is not all zeros, param.grad.norm()=tensor(1.6382, device='cuda:0')\n",
      "blocks.11.attn.b_K grad is not all zeros, param.grad.norm()=tensor(3.8128e-08, device='cuda:0')\n",
      "blocks.11.attn.W_V grad is not all zeros, param.grad.norm()=tensor(0.8799, device='cuda:0')\n",
      "blocks.11.attn.b_V grad is not all zeros, param.grad.norm()=tensor(0.1831, device='cuda:0')\n",
      "blocks.11.attn.W_O grad is not all zeros, param.grad.norm()=tensor(0.3651, device='cuda:0')\n",
      "blocks.11.attn.b_O grad is not all zeros, param.grad.norm()=tensor(0.0663, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad: # and \"edge\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if correct MLPs flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'm2')\n",
      "(10, 'm10')\n",
      "(5, 'm5')\n",
      "(3, 'm3')\n",
      "(7, 'm7')\n",
      "(1, 'm1')\n",
      "(9, 'm9')\n",
      "(8, 'm8')\n",
      "(4, 'm4')\n",
      "(0, 'm0')\n",
      "(-1, 'embed')\n",
      "(6, 'm6')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"m\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.edge_mask_mlp grad is all zeros\n",
      "blocks.0.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.0.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.0.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.5009, device='cuda:0')\n",
      "blocks.0.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3840, device='cuda:0')\n",
      "blocks.0.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9580, device='cuda:0')\n",
      "blocks.0.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2079, device='cuda:0')\n",
      "blocks.1.edge_mask_mlp grad is all zeros\n",
      "blocks.1.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.1.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.1.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1881, device='cuda:0')\n",
      "blocks.1.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4780, device='cuda:0')\n",
      "blocks.1.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.0281, device='cuda:0')\n",
      "blocks.1.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2563, device='cuda:0')\n",
      "blocks.2.edge_mask_mlp grad is all zeros\n",
      "blocks.2.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.2.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.2.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7383, device='cuda:0')\n",
      "blocks.2.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4888, device='cuda:0')\n",
      "blocks.2.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4441, device='cuda:0')\n",
      "blocks.2.mlp.b_out grad is all zeros\n",
      "blocks.3.edge_mask_mlp grad is all zeros\n",
      "blocks.3.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.3.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.3.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1551, device='cuda:0')\n",
      "blocks.3.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.3966, device='cuda:0')\n",
      "blocks.3.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.1005, device='cuda:0')\n",
      "blocks.3.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2987, device='cuda:0')\n",
      "blocks.4.edge_mask_mlp grad is all zeros\n",
      "blocks.4.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.4.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.4.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.7344, device='cuda:0')\n",
      "blocks.4.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5089, device='cuda:0')\n",
      "blocks.4.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.5127, device='cuda:0')\n",
      "blocks.4.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3237, device='cuda:0')\n",
      "blocks.5.edge_mask_mlp grad is all zeros\n",
      "blocks.5.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.5.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.5.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8992, device='cuda:0')\n",
      "blocks.5.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5541, device='cuda:0')\n",
      "blocks.5.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4629, device='cuda:0')\n",
      "blocks.5.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.3042, device='cuda:0')\n",
      "blocks.6.edge_mask_mlp grad is all zeros\n",
      "blocks.6.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.6.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.6.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(3.0009, device='cuda:0')\n",
      "blocks.6.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.5724, device='cuda:0')\n",
      "blocks.6.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(2.4607, device='cuda:0')\n",
      "blocks.6.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2637, device='cuda:0')\n",
      "blocks.7.edge_mask_mlp grad is all zeros\n",
      "blocks.7.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.7.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.7.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.3364, device='cuda:0')\n",
      "blocks.7.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4871, device='cuda:0')\n",
      "blocks.7.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.9889, device='cuda:0')\n",
      "blocks.7.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.2189, device='cuda:0')\n",
      "blocks.8.edge_mask_mlp grad is all zeros\n",
      "blocks.8.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.8.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.8.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.8043, device='cuda:0')\n",
      "blocks.8.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.6429, device='cuda:0')\n",
      "blocks.8.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.5990, device='cuda:0')\n",
      "blocks.8.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1610, device='cuda:0')\n",
      "blocks.9.edge_mask_mlp grad is all zeros\n",
      "blocks.9.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.9.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.9.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(2.1774, device='cuda:0')\n",
      "blocks.9.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.4569, device='cuda:0')\n",
      "blocks.9.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(1.3034, device='cuda:0')\n",
      "blocks.9.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.1200, device='cuda:0')\n",
      "blocks.10.edge_mask_mlp grad is all zeros\n",
      "blocks.10.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.10.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.10.mlp.W_in grad is not all zeros, param.grad.norm()=tensor(1.3321, device='cuda:0')\n",
      "blocks.10.mlp.b_in grad is not all zeros, param.grad.norm()=tensor(0.2768, device='cuda:0')\n",
      "blocks.10.mlp.W_out grad is not all zeros, param.grad.norm()=tensor(0.9347, device='cuda:0')\n",
      "blocks.10.mlp.b_out grad is not all zeros, param.grad.norm()=tensor(0.0822, device='cuda:0')\n",
      "blocks.11.edge_mask_mlp grad is all zeros\n",
      "blocks.11.edge_mask_mlp_baseline grad is all zeros\n",
      "blocks.11.edge_mask_mlp_frozen grad is all zeros\n",
      "blocks.11.mlp.W_in grad is all zeros\n",
      "blocks.11.mlp.b_in grad is all zeros\n",
      "blocks.11.mlp.W_out grad is all zeros\n",
      "blocks.11.mlp.b_out grad is all zeros\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"mlp\" in name:\n",
    "        # check if param.grad is all zeros\n",
    "        if param.grad is not None and param.grad.sum() != 0:\n",
    "            print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        else:\n",
    "            print(f\"{name} grad is all zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if individual attention heads have gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 'a9.7')\n",
      "(9, 'a9.9')\n",
      "(9, 'a9.6')\n",
      "(9, 'a9.8')\n"
     ]
    }
   ],
   "source": [
    "for node in acdcpp_nodes:\n",
    "    if \"a9\" in node[1]:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1051, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0287, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.1273, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0335, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 768, 64])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0570, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0485, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n",
      "torch.Size([12, 64, 768])\n",
      "param.grad[3].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[4].norm()=tensor(0., device='cuda:0')\n",
      "param.grad[7].norm()=tensor(0.0306, device='cuda:0')\n",
      "param.grad[8].norm()=tensor(0.0389, device='cuda:0')\n",
      "param.grad[11].norm()=tensor(0., device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_names = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and \"9.attn.weight\" in name:\n",
    "    # if param.requires_grad and \"9.attn.b\" in name:\n",
    "        print(param.shape)\n",
    "        # if param.grad is not None and param.grad.sum() != 0:\n",
    "        #     print(f\"{name} grad is not all zeros, {param.grad.norm()=}\")\n",
    "        # else:\n",
    "        #     print(f\"{name} grad is all zeros\")\n",
    "        print(f\"{param.grad[3].norm()=}\")\n",
    "        print(f\"{param.grad[4].norm()=}\")\n",
    "        print(f\"{param.grad[7].norm()=}\")\n",
    "        print(f\"{param.grad[8].norm()=}\")\n",
    "        print(f\"{param.grad[11].norm()=}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Mask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from tasks import IOITask, SportsTask, OWTTask\n",
    "batch_size = 64\n",
    "ioi = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=0)\n",
    "sports = SportsTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "owt = OWTTask(batch_size=batch_size, tokenizer=tokenizer, device=device)\n",
    "\n",
    "ioi_ood = IOITask(batch_size=batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, prompt_type=\"ABBA\", nb_templates=1, template_start_idx=1) # different template\n",
    "\n",
    "train_tasks = {\"ioi\": ioi, \"owt\": owt}\n",
    "task_weights = {\"ioi\": -.2, \"owt\": 1} # I think means preserve OWT, corrupt IOI\n",
    "eval_tasks = {\"ioi\": ioi, \"sports\": sports, \"owt\": owt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilliphguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/phillip_guo/mechanistic-unlearning/wandb/run-20240110_085753-mznr9fva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">vocal-deluge-22</a></strong> to <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/philliphguo/mech_unlearning' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva' target=\"_blank\">https://wandb.ai/philliphguo/mech_unlearning/runs/mznr9fva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 32/501 [10:48<2:38:21, 20.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_93609/3197613862.py\", line 15, in <module>\n",
      "    train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
      "  File \"/data/phillip_guo/mechanistic-unlearning/cb_utils/learn_mask.py\", line 178, in train_masks\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from cb_utils.learn_mask import train_masks\n",
    "\n",
    "epochs_left = 500\n",
    "steps_per_epoch = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "evaluate_every = 1\n",
    "discretize_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "use_wandb = False\n",
    "edge_mask_reg_strength = None\n",
    "weight_mask_reg_strength = 10\n",
    "\n",
    "wandb_config = {\"edge_masks\": edge_masks, \"weight_masks_attn\": weight_masks_attn, \"weight_masks_mlp\": weight_masks_mlp, \"epochs\": epochs_left, \"steps_per_epoch\": steps_per_epoch, \"lr\": lr, \"weight_decay\": weight_decay, \"evaluate_every\": evaluate_every, \"discretize_every\": discretize_every, \"threshold\": threshold, \"edge_mask_reg_strength\": edge_mask_reg_strength, \"weight_mask_reg_strength\": weight_mask_reg_strength}\n",
    "\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "train_masks(model, tasks=train_tasks, optimizer=optimizer, num_epochs=epochs_left, steps_per_epoch=steps_per_epoch,\n",
    "            # param_names=param_names, mask_params=mask_params, \n",
    "            task_weights=task_weights, eval_tasks=eval_tasks, evaluate_every=evaluate_every, discretize_every=discretize_every, threshold=threshold, edge_mask_reg_strength=edge_mask_reg_strength, weight_mask_reg_strength=None, verbose=False, use_wandb=use_wandb, wandb_config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"masks/trained_mask_params_{epochs_left=}_{edge_mask_reg_strength=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, p in zip(param_names, mask_params):\n",
    "    if p.requires_grad:\n",
    "        # print(name, p)\n",
    "        # count how many zeros in p\n",
    "        print(torch.sum(p == 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
